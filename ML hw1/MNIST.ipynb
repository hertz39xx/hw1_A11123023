{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22ea7fe7-2fea-4a1a-bc8e-2d395fe5b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ded8077-1b23-49b8-8050-211484cbac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8cea32b-6e17-4bb3-a7f6-8e0886ff8442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "plt.imshow(train_images[0])\n",
    "print(train_labels[0])\n",
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6fed8196-bd74-468d-9ffe-c02e48f3f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[1])\n",
    "print(train_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54cfdaf1-a6de-4c1e-98ed-c58aff56b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "# sequential模型->layer間線性序列排序, 輸出會做為下一層的輸入\n",
    "\n",
    "network.add(layers.Input(shape=(28 * 28, )))\n",
    "network.add(layers.Dense(1000, activation='relu'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "# 可以用add的方式把layer加上去, 最後的就是輸出層\n",
    "# dense是一個全連接層, 第一個參數是神經元數量\n",
    "\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efdb4de4-1578-4137-988e-fbf318e876f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels shape: (60000, 10)\n",
      "Test labels shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 做正規化\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79279656-e61a-47f9-9174-d60f4642e610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9992 - loss: 0.0045 - val_accuracy: 0.9830 - val_loss: 0.0723\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9823 - loss: 0.0698\n",
      "test_acc: 0.984499990940094\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "訓練集績效:\n",
      "Precision score: 0.9963509523766846\n",
      "Recall score: 0.99635\n",
      "F1 score: 0.9963496239654532\n"
     ]
    }
   ],
   "source": [
    "history = network.fit(train_images, train_labels, epochs=1, batch_size=128, validation_split=0.2)\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)\n",
    "\n",
    "prediction = network.predict(train_images)\n",
    "prediction_label = argmax(prediction, axis=1)\n",
    "truth_label = argmax(train_labels, axis=1)\n",
    "\n",
    "presicion = precision_score(truth_label, prediction_label, average=\"weighted\")\n",
    "recall = recall_score(truth_label, prediction_label, average=\"weighted\")\n",
    "f1 = f1_score(truth_label, prediction_label, average=\"weighted\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"訓練集績效:\")\n",
    "print(\"Precision score:\", presicion)\n",
    "print(\"Recall score:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a36abfb3-8f72-4791-ac28-39e30850561d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "測試集績效:\n",
      "Precision score: 0.9845356069720105\n",
      "Recall score: 0.9845\n",
      "F1 score: 0.9845032035690847\n"
     ]
    }
   ],
   "source": [
    "prediction = network.predict(test_images)\n",
    "\n",
    "prediction_label = argmax(prediction, axis=1)\n",
    "truth_label = argmax(test_labels, axis=1)\n",
    "\n",
    "presicion = precision_score(truth_label, prediction_label, average=\"weighted\")\n",
    "recall = recall_score(truth_label, prediction_label, average=\"weighted\")\n",
    "f1 = f1_score(truth_label, prediction_label, average=\"weighted\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"測試集績效:\")\n",
    "print(\"Precision score:\", presicion)\n",
    "print(\"Recall score:\", recall)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174104c7-f1c1-43b3-8127-77d4274bae55",
   "metadata": {},
   "source": [
    "**超參數實驗**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "552a3f02-f301-4401-9a92-0dbca8d94277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9128 - loss: 0.3072\n",
      "Epoch 2/5\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9576 - loss: 0.1452\n",
      "Epoch 3/5\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9687 - loss: 0.1063\n",
      "Epoch 4/5\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9748 - loss: 0.0833\n",
      "Epoch 5/5\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9786 - loss: 0.0705\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9648 - loss: 0.1126\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9819863182944305\n",
      "Recall Score: 0.9818166666666667\n",
      "F1 Score 0.9817856884469943\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9694186180968071\n",
      "Recall Score: 0.9691\n",
      "F1 Score 0.969041739798803\n",
      "WARNING:tensorflow:From C:\\Users\\love7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:73: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9164 - loss: 0.2937\n",
      "Epoch 2/5\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9570 - loss: 0.1459\n",
      "Epoch 3/5\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9676 - loss: 0.1096\n",
      "Epoch 4/5\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9729 - loss: 0.0906\n",
      "Epoch 5/5\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9778 - loss: 0.0782\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9707 - loss: 0.1092\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9814885369920594\n",
      "Recall Score: 0.9814166666666667\n",
      "F1 Score 0.9814127958239065\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9738613850870086\n",
      "Recall Score: 0.9738\n",
      "F1 Score 0.973787409548254\n",
      "Epoch 1/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9010 - loss: 0.3548\n",
      "Epoch 2/5\n",
      "938/938 - 1s - 2ms/step - acc: 0.9505 - loss: 0.1744\n",
      "Epoch 3/5\n",
      "938/938 - 1s - 2ms/step - acc: 0.9622 - loss: 0.1275\n",
      "Epoch 4/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9709 - loss: 0.1008\n",
      "Epoch 5/5\n",
      "938/938 - 1s - 1ms/step - acc: 0.9750 - loss: 0.0844\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9672 - loss: 0.1090\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9802833546988728\n",
      "Recall Score: 0.9802333333333333\n",
      "F1 Score 0.9802164271362876\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9715969965996556\n",
      "Recall Score: 0.9715\n",
      "F1 Score 0.9714681808175385\n",
      "Epoch 1/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9074 - loss: 0.3348\n",
      "Epoch 2/5\n",
      "938/938 - 1s - 1ms/step - acc: 0.9503 - loss: 0.1713\n",
      "Epoch 3/5\n",
      "938/938 - 1s - 1ms/step - acc: 0.9619 - loss: 0.1285\n",
      "Epoch 4/5\n",
      "938/938 - 1s - 1ms/step - acc: 0.9693 - loss: 0.1040\n",
      "Epoch 5/5\n",
      "938/938 - 1s - 1ms/step - acc: 0.9741 - loss: 0.0876\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9647 - loss: 0.1154\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9780668278736931\n",
      "Recall Score: 0.9779\n",
      "F1 Score 0.9778964395068115\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.968966825948843\n",
      "Recall Score: 0.9688\n",
      "F1 Score 0.9688115436709736\n",
      "Epoch 1/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.8837 - loss: 0.4234\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9445 - loss: 0.1956\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9579 - loss: 0.1491\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9647 - loss: 0.1216\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9700 - loss: 0.1034\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9633 - loss: 0.1233\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9750746991147236\n",
      "Recall Score: 0.97505\n",
      "F1 Score 0.975022791210627\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.966796883497835\n",
      "Recall Score: 0.9668\n",
      "F1 Score 0.9667682276160479\n",
      "Epoch 1/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.8912 - loss: 0.4051\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9389 - loss: 0.2173\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 1ms/step - acc: 0.9529 - loss: 0.1643\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 1ms/step - acc: 0.9620 - loss: 0.1332\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9676 - loss: 0.1130\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9568 - loss: 0.1393\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9719392987590523\n",
      "Recall Score: 0.9718166666666667\n",
      "F1 Score 0.9718139109385737\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9648303598395424\n",
      "Recall Score: 0.9647\n",
      "F1 Score 0.9646661405713641\n",
      "Epoch 1/5\n",
      "235/235 - 1s - 5ms/step - acc: 0.8608 - loss: 0.5282\n",
      "Epoch 2/5\n",
      "235/235 - 0s - 2ms/step - acc: 0.9313 - loss: 0.2466\n",
      "Epoch 3/5\n",
      "235/235 - 0s - 2ms/step - acc: 0.9448 - loss: 0.1969\n",
      "Epoch 4/5\n",
      "235/235 - 0s - 2ms/step - acc: 0.9543 - loss: 0.1618\n",
      "Epoch 5/5\n",
      "235/235 - 0s - 2ms/step - acc: 0.9613 - loss: 0.1382\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9543 - loss: 0.1558\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9666273390161563\n",
      "Recall Score: 0.9665666666666667\n",
      "F1 Score 0.9665535958248251\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9605512229883035\n",
      "Recall Score: 0.9604\n",
      "F1 Score 0.9603640648209543\n",
      "Epoch 1/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.8767 - loss: 0.4792\n",
      "Epoch 2/5\n",
      "235/235 - 0s - 2ms/step - acc: 0.9326 - loss: 0.2408\n",
      "Epoch 3/5\n",
      "235/235 - 0s - 2ms/step - acc: 0.9465 - loss: 0.1905\n",
      "Epoch 4/5\n",
      "235/235 - 0s - 2ms/step - acc: 0.9539 - loss: 0.1591\n",
      "Epoch 5/5\n",
      "235/235 - 0s - 2ms/step - acc: 0.9609 - loss: 0.1365\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9530 - loss: 0.1555\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9656598679863312\n",
      "Recall Score: 0.9656166666666667\n",
      "F1 Score 0.9655824858255136\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9593938724424341\n",
      "Recall Score: 0.9593\n",
      "F1 Score 0.9592360401258747\n",
      "Epoch 1/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9130 - loss: 0.3121\n",
      "Epoch 2/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9562 - loss: 0.1519\n",
      "Epoch 3/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9671 - loss: 0.1106\n",
      "Epoch 4/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9742 - loss: 0.0863\n",
      "Epoch 5/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9789 - loss: 0.0710\n",
      "Epoch 6/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9818 - loss: 0.0592\n",
      "Epoch 7/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9845 - loss: 0.0505\n",
      "Epoch 8/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9862 - loss: 0.0436\n",
      "Epoch 9/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9882 - loss: 0.0382\n",
      "Epoch 10/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9898 - loss: 0.0329\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9699 - loss: 0.1083\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9937498656176073\n",
      "Recall Score: 0.9937333333333334\n",
      "F1 Score 0.9937328578868719\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9750714120833347\n",
      "Recall Score: 0.975\n",
      "F1 Score 0.9749895332522194\n",
      "Epoch 1/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9165 - loss: 0.2990\n",
      "Epoch 2/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9559 - loss: 0.1547\n",
      "Epoch 3/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9666 - loss: 0.1169\n",
      "Epoch 4/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9728 - loss: 0.0966\n",
      "Epoch 5/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9762 - loss: 0.0839\n",
      "Epoch 6/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9789 - loss: 0.0740\n",
      "Epoch 7/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9808 - loss: 0.0666\n",
      "Epoch 8/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9825 - loss: 0.0602\n",
      "Epoch 9/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9847 - loss: 0.0552\n",
      "Epoch 10/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9857 - loss: 0.0511\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9687 - loss: 0.1126\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9888609154459298\n",
      "Recall Score: 0.9888333333333333\n",
      "F1 Score 0.9888326656122609\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9737851365170043\n",
      "Recall Score: 0.9737\n",
      "F1 Score 0.973697076697502\n",
      "Epoch 1/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9027 - loss: 0.3497\n",
      "Epoch 2/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9490 - loss: 0.1748\n",
      "Epoch 3/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9622 - loss: 0.1283\n",
      "Epoch 4/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9695 - loss: 0.1038\n",
      "Epoch 5/10\n",
      "938/938 - 1s - 2ms/step - acc: 0.9750 - loss: 0.0861\n",
      "Epoch 6/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9779 - loss: 0.0739\n",
      "Epoch 7/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9811 - loss: 0.0628\n",
      "Epoch 8/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9838 - loss: 0.0551\n",
      "Epoch 9/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9855 - loss: 0.0482\n",
      "Epoch 10/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9874 - loss: 0.0426\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9697 - loss: 0.0992\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9902371779533207\n",
      "Recall Score: 0.9901833333333333\n",
      "F1 Score 0.9901747974108638\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9732761522790475\n",
      "Recall Score: 0.9732\n",
      "F1 Score 0.9731914607055558\n",
      "Epoch 1/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9076 - loss: 0.3394\n",
      "Epoch 2/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9498 - loss: 0.1733\n",
      "Epoch 3/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9625 - loss: 0.1290\n",
      "Epoch 4/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9697 - loss: 0.1052\n",
      "Epoch 5/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9746 - loss: 0.0881\n",
      "Epoch 6/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9776 - loss: 0.0766\n",
      "Epoch 7/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9803 - loss: 0.0682\n",
      "Epoch 8/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9827 - loss: 0.0607\n",
      "Epoch 9/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9843 - loss: 0.0551\n",
      "Epoch 10/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9856 - loss: 0.0500\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9713 - loss: 0.0952\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9884026140992835\n",
      "Recall Score: 0.9883833333333333\n",
      "F1 Score 0.9883779083788606\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9746314502033061\n",
      "Recall Score: 0.9746\n",
      "F1 Score 0.9745901838983285\n",
      "Epoch 1/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.8892 - loss: 0.4141\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9430 - loss: 0.2002\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9569 - loss: 0.1515\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9647 - loss: 0.1233\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9706 - loss: 0.1036\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9736 - loss: 0.0902\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9773 - loss: 0.0795\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9794 - loss: 0.0714\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9819 - loss: 0.0631\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9833 - loss: 0.0579\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9730 - loss: 0.0951\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9873972607287889\n",
      "Recall Score: 0.9873666666666666\n",
      "F1 Score 0.9873732948458752\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9765518195857097\n",
      "Recall Score: 0.9765\n",
      "F1 Score 0.9765023088550634\n",
      "Epoch 1/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.8964 - loss: 0.3903\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9453 - loss: 0.1959\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9582 - loss: 0.1469\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 1ms/step - acc: 0.9663 - loss: 0.1191\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 1ms/step - acc: 0.9712 - loss: 0.1005\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9749 - loss: 0.0868\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 1ms/step - acc: 0.9785 - loss: 0.0765\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9803 - loss: 0.0682\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 1ms/step - acc: 0.9821 - loss: 0.0617\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9841 - loss: 0.0558\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9706 - loss: 0.1043\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.985977923872626\n",
      "Recall Score: 0.9859166666666667\n",
      "F1 Score 0.9859115142738238\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9745531853799281\n",
      "Recall Score: 0.9743\n",
      "F1 Score 0.9742976571991985\n",
      "Epoch 1/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.8504 - loss: 0.5441\n",
      "Epoch 2/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9300 - loss: 0.2496\n",
      "Epoch 3/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9445 - loss: 0.1955\n",
      "Epoch 4/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9541 - loss: 0.1618\n",
      "Epoch 5/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9609 - loss: 0.1386\n",
      "Epoch 6/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9653 - loss: 0.1208\n",
      "Epoch 7/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9688 - loss: 0.1078\n",
      "Epoch 8/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9725 - loss: 0.0966\n",
      "Epoch 9/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9755 - loss: 0.0868\n",
      "Epoch 10/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9774 - loss: 0.0795\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9643 - loss: 0.1162\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9806762485195101\n",
      "Recall Score: 0.9806\n",
      "F1 Score 0.9806055814068718\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9683350077369345\n",
      "Recall Score: 0.9683\n",
      "F1 Score 0.9683027189361297\n",
      "Epoch 1/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.8758 - loss: 0.4672\n",
      "Epoch 2/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9338 - loss: 0.2341\n",
      "Epoch 3/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9484 - loss: 0.1823\n",
      "Epoch 4/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9574 - loss: 0.1509\n",
      "Epoch 5/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9632 - loss: 0.1289\n",
      "Epoch 6/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9679 - loss: 0.1116\n",
      "Epoch 7/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9715 - loss: 0.0996\n",
      "Epoch 8/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9746 - loss: 0.0889\n",
      "Epoch 9/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9768 - loss: 0.0799\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9791 - loss: 0.0728\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9678 - loss: 0.1084\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9806965308548371\n",
      "Recall Score: 0.9806\n",
      "F1 Score 0.9806056184356188\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9717957045346078\n",
      "Recall Score: 0.9717\n",
      "F1 Score 0.971697194793637\n",
      "Epoch 1/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9136 - loss: 0.3076\n",
      "Epoch 2/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9575 - loss: 0.1464\n",
      "Epoch 3/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9690 - loss: 0.1062\n",
      "Epoch 4/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9747 - loss: 0.0851\n",
      "Epoch 5/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9790 - loss: 0.0694\n",
      "Epoch 6/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9821 - loss: 0.0586\n",
      "Epoch 7/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9843 - loss: 0.0512\n",
      "Epoch 8/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9870 - loss: 0.0421\n",
      "Epoch 9/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9883 - loss: 0.0380\n",
      "Epoch 10/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9900 - loss: 0.0320\n",
      "Epoch 11/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9915 - loss: 0.0282\n",
      "Epoch 12/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9926 - loss: 0.0249\n",
      "Epoch 13/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9936 - loss: 0.0214\n",
      "Epoch 14/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9944 - loss: 0.0193\n",
      "Epoch 15/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9951 - loss: 0.0164\n",
      "Epoch 16/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9955 - loss: 0.0149\n",
      "Epoch 17/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9965 - loss: 0.0129\n",
      "Epoch 18/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9965 - loss: 0.0119\n",
      "Epoch 19/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9963 - loss: 0.0119\n",
      "Epoch 20/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9972 - loss: 0.0093\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9703 - loss: 0.1281\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9980411470502399\n",
      "Recall Score: 0.9980333333333333\n",
      "F1 Score 0.998033514092578\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9742053890937171\n",
      "Recall Score: 0.9741\n",
      "F1 Score 0.9740989776087757\n",
      "Epoch 1/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9153 - loss: 0.3015\n",
      "Epoch 2/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9535 - loss: 0.1582\n",
      "Epoch 3/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9648 - loss: 0.1172\n",
      "Epoch 4/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9721 - loss: 0.0953\n",
      "Epoch 5/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9760 - loss: 0.0814\n",
      "Epoch 6/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9791 - loss: 0.0731\n",
      "Epoch 7/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9812 - loss: 0.0653\n",
      "Epoch 8/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9831 - loss: 0.0592\n",
      "Epoch 9/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9847 - loss: 0.0545\n",
      "Epoch 10/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9858 - loss: 0.0497\n",
      "Epoch 11/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9865 - loss: 0.0466\n",
      "Epoch 12/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9877 - loss: 0.0431\n",
      "Epoch 13/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9887 - loss: 0.0401\n",
      "Epoch 14/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9897 - loss: 0.0365\n",
      "Epoch 15/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9905 - loss: 0.0354\n",
      "Epoch 16/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9911 - loss: 0.0321\n",
      "Epoch 17/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9914 - loss: 0.0305\n",
      "Epoch 18/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9922 - loss: 0.0280\n",
      "Epoch 19/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9930 - loss: 0.0263\n",
      "Epoch 20/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9932 - loss: 0.0251\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9667 - loss: 0.1578\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9941140310914329\n",
      "Recall Score: 0.9940666666666667\n",
      "F1 Score 0.9940715766751812\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9720571955269648\n",
      "Recall Score: 0.9719\n",
      "F1 Score 0.9719039034945168\n",
      "Epoch 1/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9004 - loss: 0.3521\n",
      "Epoch 2/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9524 - loss: 0.1671\n",
      "Epoch 3/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9638 - loss: 0.1239\n",
      "Epoch 4/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9711 - loss: 0.0991\n",
      "Epoch 5/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9757 - loss: 0.0818\n",
      "Epoch 6/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9794 - loss: 0.0695\n",
      "Epoch 7/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9819 - loss: 0.0596\n",
      "Epoch 8/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9841 - loss: 0.0518\n",
      "Epoch 9/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9860 - loss: 0.0454\n",
      "Epoch 10/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9886 - loss: 0.0390\n",
      "Epoch 11/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9895 - loss: 0.0351\n",
      "Epoch 12/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9909 - loss: 0.0307\n",
      "Epoch 13/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9926 - loss: 0.0263\n",
      "Epoch 14/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9937 - loss: 0.0237\n",
      "Epoch 15/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9943 - loss: 0.0209\n",
      "Epoch 16/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9951 - loss: 0.0187\n",
      "Epoch 17/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9954 - loss: 0.0170\n",
      "Epoch 18/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9965 - loss: 0.0141\n",
      "Epoch 19/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9962 - loss: 0.0137\n",
      "Epoch 20/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9970 - loss: 0.0117\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9721 - loss: 0.1189\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9979054676399388\n",
      "Recall Score: 0.9979\n",
      "F1 Score 0.9978997562699087\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9754414050708939\n",
      "Recall Score: 0.9753\n",
      "F1 Score 0.9753194740362225\n",
      "Epoch 1/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9076 - loss: 0.3365\n",
      "Epoch 2/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9510 - loss: 0.1711\n",
      "Epoch 3/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9615 - loss: 0.1292\n",
      "Epoch 4/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9695 - loss: 0.1046\n",
      "Epoch 5/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9739 - loss: 0.0877\n",
      "Epoch 6/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9779 - loss: 0.0766\n",
      "Epoch 7/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9792 - loss: 0.0674\n",
      "Epoch 8/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9817 - loss: 0.0609\n",
      "Epoch 9/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9839 - loss: 0.0543\n",
      "Epoch 10/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9855 - loss: 0.0492\n",
      "Epoch 11/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9863 - loss: 0.0450\n",
      "Epoch 12/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9874 - loss: 0.0407\n",
      "Epoch 13/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9885 - loss: 0.0376\n",
      "Epoch 14/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9899 - loss: 0.0344\n",
      "Epoch 15/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9905 - loss: 0.0316\n",
      "Epoch 16/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9916 - loss: 0.0288\n",
      "Epoch 17/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9924 - loss: 0.0270\n",
      "Epoch 18/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9931 - loss: 0.0245\n",
      "Epoch 19/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9935 - loss: 0.0227\n",
      "Epoch 20/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9940 - loss: 0.0210\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9687 - loss: 0.1098\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9948396732370164\n",
      "Recall Score: 0.9948\n",
      "F1 Score 0.9948054251008268\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9734263517519296\n",
      "Recall Score: 0.9733\n",
      "F1 Score 0.9733028225338106\n",
      "Epoch 1/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.8823 - loss: 0.4292\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9383 - loss: 0.2195\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9508 - loss: 0.1721\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9596 - loss: 0.1407\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9661 - loss: 0.1181\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9703 - loss: 0.1009\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9738 - loss: 0.0880\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9782 - loss: 0.0770\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 1ms/step - acc: 0.9801 - loss: 0.0684\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 1ms/step - acc: 0.9817 - loss: 0.0616\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9839 - loss: 0.0552\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9858 - loss: 0.0495\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9866 - loss: 0.0459\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9877 - loss: 0.0412\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9895 - loss: 0.0378\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9903 - loss: 0.0345\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9916 - loss: 0.0312\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9923 - loss: 0.0289\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9932 - loss: 0.0258\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9940 - loss: 0.0238\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9700 - loss: 0.1105\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.996405309019754\n",
      "Recall Score: 0.9964\n",
      "F1 Score 0.9963999811297155\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9739309984214451\n",
      "Recall Score: 0.9739\n",
      "F1 Score 0.9739000178663387\n",
      "Epoch 1/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.8935 - loss: 0.3906\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9430 - loss: 0.1981\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9570 - loss: 0.1513\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9639 - loss: 0.1233\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 1ms/step - acc: 0.9698 - loss: 0.1035\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 1ms/step - acc: 0.9733 - loss: 0.0899\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 1ms/step - acc: 0.9766 - loss: 0.0791\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9797 - loss: 0.0698\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9812 - loss: 0.0627\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9830 - loss: 0.0567\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9845 - loss: 0.0521\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 1ms/step - acc: 0.9858 - loss: 0.0476\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9870 - loss: 0.0436\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9880 - loss: 0.0400\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9894 - loss: 0.0369\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9903 - loss: 0.0336\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 1ms/step - acc: 0.9912 - loss: 0.0313\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9918 - loss: 0.0286\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9929 - loss: 0.0263\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9936 - loss: 0.0244\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9738 - loss: 0.0989\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.994561039023261\n",
      "Recall Score: 0.9945333333333334\n",
      "F1 Score 0.9945339949264681\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.977316095102528\n",
      "Recall Score: 0.9772\n",
      "F1 Score 0.9771992522982721\n",
      "Epoch 1/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.8552 - loss: 0.5266\n",
      "Epoch 2/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9312 - loss: 0.2438\n",
      "Epoch 3/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9441 - loss: 0.1937\n",
      "Epoch 4/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9542 - loss: 0.1626\n",
      "Epoch 5/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9599 - loss: 0.1414\n",
      "Epoch 6/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9637 - loss: 0.1252\n",
      "Epoch 7/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9679 - loss: 0.1120\n",
      "Epoch 8/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9713 - loss: 0.1009\n",
      "Epoch 9/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9743 - loss: 0.0914\n",
      "Epoch 10/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9755 - loss: 0.0841\n",
      "Epoch 11/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9781 - loss: 0.0764\n",
      "Epoch 12/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9793 - loss: 0.0712\n",
      "Epoch 13/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9807 - loss: 0.0657\n",
      "Epoch 14/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9822 - loss: 0.0609\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9840 - loss: 0.0555\n",
      "Epoch 16/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9851 - loss: 0.0528\n",
      "Epoch 17/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9867 - loss: 0.0480\n",
      "Epoch 18/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9872 - loss: 0.0457\n",
      "Epoch 19/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9883 - loss: 0.0428\n",
      "Epoch 20/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9892 - loss: 0.0393\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9713 - loss: 0.1001\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9922401633357885\n",
      "Recall Score: 0.9922333333333333\n",
      "F1 Score 0.9922340547384605\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9752223303863861\n",
      "Recall Score: 0.9752\n",
      "F1 Score 0.9751990154345737\n",
      "Epoch 1/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.8808 - loss: 0.4622\n",
      "Epoch 2/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9342 - loss: 0.2313\n",
      "Epoch 3/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9487 - loss: 0.1824\n",
      "Epoch 4/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9562 - loss: 0.1533\n",
      "Epoch 5/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9626 - loss: 0.1325\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9670 - loss: 0.1167\n",
      "Epoch 7/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9705 - loss: 0.1041\n",
      "Epoch 8/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9742 - loss: 0.0937\n",
      "Epoch 9/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9764 - loss: 0.0849\n",
      "Epoch 10/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9784 - loss: 0.0772\n",
      "Epoch 11/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9799 - loss: 0.0710\n",
      "Epoch 12/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9817 - loss: 0.0651\n",
      "Epoch 13/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9830 - loss: 0.0603\n",
      "Epoch 14/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9848 - loss: 0.0557\n",
      "Epoch 15/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9854 - loss: 0.0516\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9866 - loss: 0.0481\n",
      "Epoch 17/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9872 - loss: 0.0450\n",
      "Epoch 18/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9883 - loss: 0.0415\n",
      "Epoch 19/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9894 - loss: 0.0390\n",
      "Epoch 20/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9901 - loss: 0.0365\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9709 - loss: 0.0968\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9921133013250399\n",
      "Recall Score: 0.9921\n",
      "F1 Score 0.9920946461981353\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點64, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9745772704728696\n",
      "Recall Score: 0.9745\n",
      "F1 Score 0.9744676282257582\n",
      "Epoch 1/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9260 - loss: 0.2614\n",
      "Epoch 2/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9650 - loss: 0.1172\n",
      "Epoch 3/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9749 - loss: 0.0824\n",
      "Epoch 4/5\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9819 - loss: 0.0605\n",
      "Epoch 5/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9854 - loss: 0.0476\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9734 - loss: 0.0936\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.99037771702798\n",
      "Recall Score: 0.9903333333333333\n",
      "F1 Score 0.990332955459535\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.97657921614097\n",
      "Recall Score: 0.9765\n",
      "F1 Score 0.9765024783396122\n",
      "Epoch 1/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9241 - loss: 0.2655\n",
      "Epoch 2/5\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9643 - loss: 0.1200\n",
      "Epoch 3/5\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9750 - loss: 0.0853\n",
      "Epoch 4/5\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9804 - loss: 0.0672\n",
      "Epoch 5/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9834 - loss: 0.0574\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9706 - loss: 0.1010\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9873893475742203\n",
      "Recall Score: 0.9873166666666666\n",
      "F1 Score 0.9873145476899138\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9742980005336928\n",
      "Recall Score: 0.9741\n",
      "F1 Score 0.9741013514009285\n",
      "Epoch 1/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9147 - loss: 0.3011\n",
      "Epoch 2/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9598 - loss: 0.1373\n",
      "Epoch 3/5\n",
      "938/938 - 1s - 2ms/step - acc: 0.9723 - loss: 0.0944\n",
      "Epoch 4/5\n",
      "938/938 - 1s - 2ms/step - acc: 0.9787 - loss: 0.0714\n",
      "Epoch 5/5\n",
      "938/938 - 1s - 2ms/step - acc: 0.9831 - loss: 0.0565\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9736 - loss: 0.0860\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9894603863531681\n",
      "Recall Score: 0.9894333333333334\n",
      "F1 Score 0.9894315383917724\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9772211288928196\n",
      "Recall Score: 0.9771\n",
      "F1 Score 0.9770873554507882\n",
      "Epoch 1/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9160 - loss: 0.2971\n",
      "Epoch 2/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9603 - loss: 0.1378\n",
      "Epoch 3/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9719 - loss: 0.0966\n",
      "Epoch 4/5\n",
      "938/938 - 1s - 2ms/step - acc: 0.9777 - loss: 0.0752\n",
      "Epoch 5/5\n",
      "938/938 - 1s - 1ms/step - acc: 0.9819 - loss: 0.0605\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9716 - loss: 0.0963\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9859764700306021\n",
      "Recall Score: 0.9859\n",
      "F1 Score 0.9858929481760845\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9749384888453057\n",
      "Recall Score: 0.9747\n",
      "F1 Score 0.974679498275498\n",
      "Epoch 1/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9030 - loss: 0.3567\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9524 - loss: 0.1656\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9662 - loss: 0.1183\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9730 - loss: 0.0923\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9782 - loss: 0.0742\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9720 - loss: 0.0949\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9841108087580853\n",
      "Recall Score: 0.9840833333333333\n",
      "F1 Score 0.9840795890409563\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9743037764326993\n",
      "Recall Score: 0.9743\n",
      "F1 Score 0.9742879021063549\n",
      "Epoch 1/5\n",
      "469/469 - 2s - 3ms/step - acc: 0.9087 - loss: 0.3322\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9540 - loss: 0.1588\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9670 - loss: 0.1116\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9743 - loss: 0.0867\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9790 - loss: 0.0701\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9694 - loss: 0.1033\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.982172027307669\n",
      "Recall Score: 0.98205\n",
      "F1 Score 0.982056397357344\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9735212294139941\n",
      "Recall Score: 0.9734\n",
      "F1 Score 0.9733825305621818\n",
      "Epoch 1/5\n",
      "235/235 - 1s - 5ms/step - acc: 0.8778 - loss: 0.4442\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9435 - loss: 0.2020\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9574 - loss: 0.1504\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9661 - loss: 0.1209\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9714 - loss: 0.0995\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9642 - loss: 0.1200\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9756672025562905\n",
      "Recall Score: 0.9755666666666667\n",
      "F1 Score 0.9755298729810589\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.968972267979087\n",
      "Recall Score: 0.9688\n",
      "F1 Score 0.9687481013147566\n",
      "Epoch 1/5\n",
      "235/235 - 1s - 5ms/step - acc: 0.8912 - loss: 0.4082\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9438 - loss: 0.1997\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9580 - loss: 0.1468\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9673 - loss: 0.1160\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9732 - loss: 0.0950\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9641 - loss: 0.1124\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9769163275137633\n",
      "Recall Score: 0.9768166666666667\n",
      "F1 Score 0.9767999877600778\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9691652948843795\n",
      "Recall Score: 0.9691\n",
      "F1 Score 0.9690789569565572\n",
      "Epoch 1/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9260 - loss: 0.2606\n",
      "Epoch 2/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9662 - loss: 0.1144\n",
      "Epoch 3/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9765 - loss: 0.0779\n",
      "Epoch 4/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9825 - loss: 0.0580\n",
      "Epoch 5/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9860 - loss: 0.0458\n",
      "Epoch 6/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9890 - loss: 0.0357\n",
      "Epoch 7/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9908 - loss: 0.0296\n",
      "Epoch 8/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9932 - loss: 0.0230\n",
      "Epoch 9/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9939 - loss: 0.0197\n",
      "Epoch 10/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9953 - loss: 0.0158\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9744 - loss: 0.0952\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.996569065809689\n",
      "Recall Score: 0.99655\n",
      "F1 Score 0.9965504609539013\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9790428866588758\n",
      "Recall Score: 0.9789\n",
      "F1 Score 0.9789030128221148\n",
      "Epoch 1/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9264 - loss: 0.2569\n",
      "Epoch 2/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9643 - loss: 0.1205\n",
      "Epoch 3/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9736 - loss: 0.0876\n",
      "Epoch 4/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9801 - loss: 0.0692\n",
      "Epoch 5/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9837 - loss: 0.0566\n",
      "Epoch 6/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9866 - loss: 0.0477\n",
      "Epoch 7/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9876 - loss: 0.0418\n",
      "Epoch 8/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9898 - loss: 0.0349\n",
      "Epoch 9/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9912 - loss: 0.0303\n",
      "Epoch 10/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9923 - loss: 0.0265\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9736 - loss: 0.1043\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9944467939398556\n",
      "Recall Score: 0.9944333333333333\n",
      "F1 Score 0.99443307488693\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9769565882192802\n",
      "Recall Score: 0.9769\n",
      "F1 Score 0.9768847398348275\n",
      "Epoch 1/10\n",
      "938/938 - 2s - 3ms/step - acc: 0.9178 - loss: 0.2962\n",
      "Epoch 2/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9606 - loss: 0.1350\n",
      "Epoch 3/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9721 - loss: 0.0948\n",
      "Epoch 4/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9780 - loss: 0.0733\n",
      "Epoch 5/10\n",
      "938/938 - 1s - 2ms/step - acc: 0.9829 - loss: 0.0571\n",
      "Epoch 6/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9866 - loss: 0.0459\n",
      "Epoch 7/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9887 - loss: 0.0374\n",
      "Epoch 8/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9912 - loss: 0.0302\n",
      "Epoch 9/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9928 - loss: 0.0251\n",
      "Epoch 10/10\n",
      "938/938 - 1s - 2ms/step - acc: 0.9942 - loss: 0.0208\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9728 - loss: 0.0956\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9937291996162788\n",
      "Recall Score: 0.9936166666666667\n",
      "F1 Score 0.9936314292307685\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9762294486251277\n",
      "Recall Score: 0.9759\n",
      "F1 Score 0.9759392918067382\n",
      "Epoch 1/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9186 - loss: 0.2909\n",
      "Epoch 2/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9608 - loss: 0.1351\n",
      "Epoch 3/10\n",
      "938/938 - 1s - 2ms/step - acc: 0.9724 - loss: 0.0936\n",
      "Epoch 4/10\n",
      "938/938 - 1s - 2ms/step - acc: 0.9780 - loss: 0.0737\n",
      "Epoch 5/10\n",
      "938/938 - 1s - 2ms/step - acc: 0.9818 - loss: 0.0602\n",
      "Epoch 6/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9850 - loss: 0.0499\n",
      "Epoch 7/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9877 - loss: 0.0424\n",
      "Epoch 8/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9893 - loss: 0.0361\n",
      "Epoch 9/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9909 - loss: 0.0310\n",
      "Epoch 10/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9921 - loss: 0.0267\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9728 - loss: 0.0936\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9933441795578416\n",
      "Recall Score: 0.9933\n",
      "F1 Score 0.9932944975385576\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9758102198837956\n",
      "Recall Score: 0.9757\n",
      "F1 Score 0.9756785017966242\n",
      "Epoch 1/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9046 - loss: 0.3528\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9556 - loss: 0.1578\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9676 - loss: 0.1131\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9745 - loss: 0.0885\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9803 - loss: 0.0700\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9837 - loss: 0.0574\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9861 - loss: 0.0487\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9889 - loss: 0.0405\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9901 - loss: 0.0344\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9928 - loss: 0.0285\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9756 - loss: 0.0859\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9949226602824921\n",
      "Recall Score: 0.9949166666666667\n",
      "F1 Score 0.9949163774129253\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9789279923413143\n",
      "Recall Score: 0.9789\n",
      "F1 Score 0.9788956410027487\n",
      "Epoch 1/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9079 - loss: 0.3327\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9555 - loss: 0.1561\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9682 - loss: 0.1116\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9750 - loss: 0.0871\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9796 - loss: 0.0714\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9831 - loss: 0.0592\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9852 - loss: 0.0511\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9875 - loss: 0.0438\n",
      "Epoch 9/10\n",
      "469/469 - 2s - 3ms/step - acc: 0.9891 - loss: 0.0377\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9915 - loss: 0.0322\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9748 - loss: 0.0838\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9932367865124158\n",
      "Recall Score: 0.9932166666666666\n",
      "F1 Score 0.9932196246745542\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9786682574599137\n",
      "Recall Score: 0.9786\n",
      "F1 Score 0.9786005223989589\n",
      "Epoch 1/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.8837 - loss: 0.4338\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9437 - loss: 0.2004\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9579 - loss: 0.1493\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9659 - loss: 0.1186\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9729 - loss: 0.0975\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9767 - loss: 0.0821\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9802 - loss: 0.0707\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9825 - loss: 0.0611\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9852 - loss: 0.0537\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9872 - loss: 0.0467\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9733 - loss: 0.0903\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9906324145803933\n",
      "Recall Score: 0.9906166666666667\n",
      "F1 Score 0.9906174963604955\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9768502684383132\n",
      "Recall Score: 0.9768\n",
      "F1 Score 0.9767901455011364\n",
      "Epoch 1/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.8914 - loss: 0.4023\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9437 - loss: 0.1987\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9581 - loss: 0.1461\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9672 - loss: 0.1155\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9724 - loss: 0.0951\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9769 - loss: 0.0805\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9796 - loss: 0.0689\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9831 - loss: 0.0598\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9853 - loss: 0.0527\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9870 - loss: 0.0462\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9720 - loss: 0.0858\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9891915235668595\n",
      "Recall Score: 0.98915\n",
      "F1 Score 0.9891493848324894\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9757684560446054\n",
      "Recall Score: 0.9756\n",
      "F1 Score 0.9755882618231386\n",
      "Epoch 1/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9249 - loss: 0.2623\n",
      "Epoch 2/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9653 - loss: 0.1151\n",
      "Epoch 3/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9766 - loss: 0.0780\n",
      "Epoch 4/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9821 - loss: 0.0590\n",
      "Epoch 5/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9859 - loss: 0.0450\n",
      "Epoch 6/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9891 - loss: 0.0352\n",
      "Epoch 7/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9917 - loss: 0.0281\n",
      "Epoch 8/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9928 - loss: 0.0228\n",
      "Epoch 9/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9930 - loss: 0.0205\n",
      "Epoch 10/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9955 - loss: 0.0146\n",
      "Epoch 11/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9956 - loss: 0.0136\n",
      "Epoch 12/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9973 - loss: 0.0103\n",
      "Epoch 13/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9966 - loss: 0.0110\n",
      "Epoch 14/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9970 - loss: 0.0090\n",
      "Epoch 15/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9980 - loss: 0.0068\n",
      "Epoch 16/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9978 - loss: 0.0075\n",
      "Epoch 17/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9979 - loss: 0.0065\n",
      "Epoch 18/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9981 - loss: 0.0058\n",
      "Epoch 19/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9984 - loss: 0.0049\n",
      "Epoch 20/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9983 - loss: 0.0057\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9739 - loss: 0.1277\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9986851782643114\n",
      "Recall Score: 0.9986833333333334\n",
      "F1 Score 0.9986827761616391\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9785740619046106\n",
      "Recall Score: 0.9785\n",
      "F1 Score 0.9784997316261188\n",
      "Epoch 1/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9258 - loss: 0.2598\n",
      "Epoch 2/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9642 - loss: 0.1223\n",
      "Epoch 3/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9742 - loss: 0.0878\n",
      "Epoch 4/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9797 - loss: 0.0692\n",
      "Epoch 5/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9832 - loss: 0.0565\n",
      "Epoch 6/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9860 - loss: 0.0482\n",
      "Epoch 7/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9884 - loss: 0.0408\n",
      "Epoch 8/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9898 - loss: 0.0349\n",
      "Epoch 9/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9912 - loss: 0.0305\n",
      "Epoch 10/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9923 - loss: 0.0272\n",
      "Epoch 11/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9934 - loss: 0.0234\n",
      "Epoch 12/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9944 - loss: 0.0208\n",
      "Epoch 13/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9951 - loss: 0.0173\n",
      "Epoch 14/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9960 - loss: 0.0151\n",
      "Epoch 15/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9962 - loss: 0.0134\n",
      "Epoch 16/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9966 - loss: 0.0114\n",
      "Epoch 17/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9975 - loss: 0.0096\n",
      "Epoch 18/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9977 - loss: 0.0081\n",
      "Epoch 19/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9977 - loss: 0.0080\n",
      "Epoch 20/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9984 - loss: 0.0062\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9757 - loss: 0.1291\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.999034125804266\n",
      "Recall Score: 0.9990333333333333\n",
      "F1 Score 0.9990333496033855\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9789044052460808\n",
      "Recall Score: 0.9788\n",
      "F1 Score 0.9788051958238789\n",
      "Epoch 1/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9163 - loss: 0.2986\n",
      "Epoch 2/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9597 - loss: 0.1389\n",
      "Epoch 3/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9716 - loss: 0.0974\n",
      "Epoch 4/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9782 - loss: 0.0751\n",
      "Epoch 5/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9831 - loss: 0.0577\n",
      "Epoch 6/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9854 - loss: 0.0477\n",
      "Epoch 7/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9882 - loss: 0.0390\n",
      "Epoch 8/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9911 - loss: 0.0310\n",
      "Epoch 9/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9925 - loss: 0.0263\n",
      "Epoch 10/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9938 - loss: 0.0216\n",
      "Epoch 11/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9954 - loss: 0.0170\n",
      "Epoch 12/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9966 - loss: 0.0135\n",
      "Epoch 13/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9965 - loss: 0.0126\n",
      "Epoch 14/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9974 - loss: 0.0109\n",
      "Epoch 15/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9980 - loss: 0.0084\n",
      "Epoch 16/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9974 - loss: 0.0089\n",
      "Epoch 17/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9980 - loss: 0.0072\n",
      "Epoch 18/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9984 - loss: 0.0062\n",
      "Epoch 19/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9992 - loss: 0.0045\n",
      "Epoch 20/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9983 - loss: 0.0062\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9743 - loss: 0.1089\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9989511808111673\n",
      "Recall Score: 0.99895\n",
      "F1 Score 0.998950119617806\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9776812841895682\n",
      "Recall Score: 0.9776\n",
      "F1 Score 0.9775993616970158\n",
      "Epoch 1/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9187 - loss: 0.2888\n",
      "Epoch 2/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9605 - loss: 0.1364\n",
      "Epoch 3/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9716 - loss: 0.0959\n",
      "Epoch 4/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9783 - loss: 0.0740\n",
      "Epoch 5/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9821 - loss: 0.0606\n",
      "Epoch 6/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9849 - loss: 0.0494\n",
      "Epoch 7/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9876 - loss: 0.0418\n",
      "Epoch 8/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9890 - loss: 0.0363\n",
      "Epoch 9/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9913 - loss: 0.0305\n",
      "Epoch 10/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9920 - loss: 0.0268\n",
      "Epoch 11/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9936 - loss: 0.0229\n",
      "Epoch 12/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9943 - loss: 0.0194\n",
      "Epoch 13/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9950 - loss: 0.0171\n",
      "Epoch 14/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9961 - loss: 0.0137\n",
      "Epoch 15/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9964 - loss: 0.0126\n",
      "Epoch 16/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9973 - loss: 0.0098\n",
      "Epoch 17/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9976 - loss: 0.0088\n",
      "Epoch 18/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9984 - loss: 0.0075\n",
      "Epoch 19/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9986 - loss: 0.0063\n",
      "Epoch 20/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9989 - loss: 0.0052\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9760 - loss: 0.1095  \n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9995667950174904\n",
      "Recall Score: 0.9995666666666667\n",
      "F1 Score 0.9995666669077832\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9795454256388466\n",
      "Recall Score: 0.9795\n",
      "F1 Score 0.9795011987870914\n",
      "Epoch 1/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9016 - loss: 0.3554\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9542 - loss: 0.1619\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9659 - loss: 0.1173\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9737 - loss: 0.0903\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9789 - loss: 0.0728\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9824 - loss: 0.0594\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9855 - loss: 0.0499\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9882 - loss: 0.0418\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9903 - loss: 0.0348\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9919 - loss: 0.0300\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9931 - loss: 0.0254\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9947 - loss: 0.0213\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9952 - loss: 0.0188\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9967 - loss: 0.0155\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9971 - loss: 0.0132\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9979 - loss: 0.0116\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9981 - loss: 0.0100\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9987 - loss: 0.0075\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9984 - loss: 0.0077\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9989 - loss: 0.0066\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9769 - loss: 0.0931\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.998968047144063\n",
      "Recall Score: 0.9989666666666667\n",
      "F1 Score 0.9989661685972162\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9795680568736862\n",
      "Recall Score: 0.9795\n",
      "F1 Score 0.9794909781821521\n",
      "Epoch 1/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9050 - loss: 0.3433\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9545 - loss: 0.1600\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9663 - loss: 0.1139\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9744 - loss: 0.0891\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9786 - loss: 0.0730\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9816 - loss: 0.0610\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9845 - loss: 0.0523\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9867 - loss: 0.0448\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9889 - loss: 0.0386\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9903 - loss: 0.0332\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9920 - loss: 0.0291\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9929 - loss: 0.0252\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9940 - loss: 0.0220\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9950 - loss: 0.0188\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9958 - loss: 0.0169\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9965 - loss: 0.0144\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9973 - loss: 0.0123\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9974 - loss: 0.0109\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9981 - loss: 0.0092\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9986 - loss: 0.0077\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9753 - loss: 0.0904\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.99893403209195\n",
      "Recall Score: 0.9989333333333333\n",
      "F1 Score 0.9989334220045348\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9799524642796048\n",
      "Recall Score: 0.9799\n",
      "F1 Score 0.979904405570073\n",
      "Epoch 1/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.8811 - loss: 0.4411\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9431 - loss: 0.2049\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9567 - loss: 0.1521\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9653 - loss: 0.1211\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9718 - loss: 0.0994\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9760 - loss: 0.0846\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9798 - loss: 0.0723\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9823 - loss: 0.0624\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9845 - loss: 0.0550\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9862 - loss: 0.0487\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9881 - loss: 0.0428\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9899 - loss: 0.0383\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9907 - loss: 0.0343\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9921 - loss: 0.0300\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9934 - loss: 0.0267\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9946 - loss: 0.0236\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9952 - loss: 0.0213\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9961 - loss: 0.0187\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9969 - loss: 0.0162\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9971 - loss: 0.0148\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9730 - loss: 0.0894\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9977427513555003\n",
      "Recall Score: 0.9977333333333334\n",
      "F1 Score 0.9977332935740002\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9774020122502115\n",
      "Recall Score: 0.9773\n",
      "F1 Score 0.9772947099323208\n",
      "Epoch 1/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.8898 - loss: 0.4050\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9432 - loss: 0.2001\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9585 - loss: 0.1459\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9682 - loss: 0.1137\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9735 - loss: 0.0933\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9774 - loss: 0.0782\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9811 - loss: 0.0666\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9840 - loss: 0.0582\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9851 - loss: 0.0510\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9877 - loss: 0.0450\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9890 - loss: 0.0397\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9907 - loss: 0.0352\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9918 - loss: 0.0314\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9928 - loss: 0.0277\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9939 - loss: 0.0244\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9946 - loss: 0.0220\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9954 - loss: 0.0193\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9960 - loss: 0.0172\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9969 - loss: 0.0154\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9972 - loss: 0.0137\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9753 - loss: 0.0828\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9985345004322992\n",
      "Recall Score: 0.9985333333333334\n",
      "F1 Score 0.9985333511049017\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點128, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9788273621703852\n",
      "Recall Score: 0.9788\n",
      "F1 Score 0.978788786583452\n",
      "Epoch 1/5\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9340 - loss: 0.2276\n",
      "Epoch 2/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9726 - loss: 0.0931\n",
      "Epoch 3/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9807 - loss: 0.0621\n",
      "Epoch 4/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9864 - loss: 0.0438\n",
      "Epoch 5/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9895 - loss: 0.0332\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9698 - loss: 0.1002\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9910631631977213\n",
      "Recall Score: 0.9909\n",
      "F1 Score 0.9909171262897329\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9758005546508842\n",
      "Recall Score: 0.9753\n",
      "F1 Score 0.9753423384521964\n",
      "Epoch 1/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9356 - loss: 0.2239\n",
      "Epoch 2/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9713 - loss: 0.0972\n",
      "Epoch 3/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9797 - loss: 0.0675\n",
      "Epoch 4/5\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9840 - loss: 0.0526\n",
      "Epoch 5/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9879 - loss: 0.0410\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9740 - loss: 0.1008\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9915813684291518\n",
      "Recall Score: 0.99155\n",
      "F1 Score 0.99154911451424\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9775437626409504\n",
      "Recall Score: 0.9774\n",
      "F1 Score 0.9773925468792611\n",
      "Epoch 1/5\n",
      "938/938 - 5s - 5ms/step - acc: 0.9272 - loss: 0.2582\n",
      "Epoch 2/5\n",
      "938/938 - 4s - 4ms/step - acc: 0.9679 - loss: 0.1084\n",
      "Epoch 3/5\n",
      "938/938 - 3s - 4ms/step - acc: 0.9778 - loss: 0.0725\n",
      "Epoch 4/5\n",
      "938/938 - 4s - 4ms/step - acc: 0.9838 - loss: 0.0532\n",
      "Epoch 5/5\n",
      "938/938 - 3s - 4ms/step - acc: 0.9877 - loss: 0.0399\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9737 - loss: 0.0846\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9921901413911057\n",
      "Recall Score: 0.99215\n",
      "F1 Score 0.9921536115438901\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9786194942867272\n",
      "Recall Score: 0.9785\n",
      "F1 Score 0.9785082000858367\n",
      "Epoch 1/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9274 - loss: 0.2558\n",
      "Epoch 2/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9676 - loss: 0.1086\n",
      "Epoch 3/5\n",
      "938/938 - 2s - 3ms/step - acc: 0.9781 - loss: 0.0732\n",
      "Epoch 4/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9835 - loss: 0.0547\n",
      "Epoch 5/5\n",
      "938/938 - 2s - 3ms/step - acc: 0.9871 - loss: 0.0429\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9788 - loss: 0.0736\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9910359714579552\n",
      "Recall Score: 0.9909833333333333\n",
      "F1 Score 0.9909843528473048\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9806186676739157\n",
      "Recall Score: 0.9806\n",
      "F1 Score 0.9805886837575155\n",
      "Epoch 1/5\n",
      "469/469 - 3s - 6ms/step - acc: 0.9122 - loss: 0.3097\n",
      "Epoch 2/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9615 - loss: 0.1328\n",
      "Epoch 3/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9737 - loss: 0.0899\n",
      "Epoch 4/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9808 - loss: 0.0674\n",
      "Epoch 5/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9847 - loss: 0.0519\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9745 - loss: 0.0801\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9886936243030289\n",
      "Recall Score: 0.9886166666666667\n",
      "F1 Score 0.9886105714163754\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9771821434160485\n",
      "Recall Score: 0.9771\n",
      "F1 Score 0.9770772433935266\n",
      "Epoch 1/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9182 - loss: 0.2948\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9616 - loss: 0.1309\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9739 - loss: 0.0887\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9802 - loss: 0.0661\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9844 - loss: 0.0521\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9751 - loss: 0.0827\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9903731367153014\n",
      "Recall Score: 0.9903666666666666\n",
      "F1 Score 0.9903620837510554\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9784378227739368\n",
      "Recall Score: 0.9784\n",
      "F1 Score 0.9783865456203154\n",
      "Epoch 1/5\n",
      "235/235 - 1s - 6ms/step - acc: 0.8943 - loss: 0.3821\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9524 - loss: 0.1674\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9658 - loss: 0.1181\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9740 - loss: 0.0910\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9790 - loss: 0.0729\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9722 - loss: 0.0959\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9847284527083197\n",
      "Recall Score: 0.9847\n",
      "F1 Score 0.9846932300330427\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9748456029166961\n",
      "Recall Score: 0.9748\n",
      "F1 Score 0.9747795474634088\n",
      "Epoch 1/5\n",
      "235/235 - 1s - 5ms/step - acc: 0.9003 - loss: 0.3592\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9518 - loss: 0.1677\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9663 - loss: 0.1177\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9746 - loss: 0.0884\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9795 - loss: 0.0706\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9722 - loss: 0.0999\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9820512653321866\n",
      "Recall Score: 0.9818833333333333\n",
      "F1 Score 0.9818728299410656\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9746898870837208\n",
      "Recall Score: 0.9745\n",
      "F1 Score 0.9744738710088555\n",
      "Epoch 1/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9346 - loss: 0.2231\n",
      "Epoch 2/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9722 - loss: 0.0930\n",
      "Epoch 3/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9811 - loss: 0.0602\n",
      "Epoch 4/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9855 - loss: 0.0452\n",
      "Epoch 5/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9896 - loss: 0.0322\n",
      "Epoch 6/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9919 - loss: 0.0249\n",
      "Epoch 7/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9936 - loss: 0.0196\n",
      "Epoch 8/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9954 - loss: 0.0151\n",
      "Epoch 9/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9956 - loss: 0.0141\n",
      "Epoch 10/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9958 - loss: 0.0126\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9764 - loss: 0.0940\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9967741666046608\n",
      "Recall Score: 0.9967666666666667\n",
      "F1 Score 0.996766401488962\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9789925901591098\n",
      "Recall Score: 0.9789\n",
      "F1 Score 0.978908203910565\n",
      "Epoch 1/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9356 - loss: 0.2203\n",
      "Epoch 2/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9714 - loss: 0.0968\n",
      "Epoch 3/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9794 - loss: 0.0679\n",
      "Epoch 4/10\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9844 - loss: 0.0524\n",
      "Epoch 5/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9872 - loss: 0.0424\n",
      "Epoch 6/10\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9897 - loss: 0.0351\n",
      "Epoch 7/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9917 - loss: 0.0280\n",
      "Epoch 8/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9929 - loss: 0.0224\n",
      "Epoch 9/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9942 - loss: 0.0193\n",
      "Epoch 10/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9957 - loss: 0.0148\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9759 - loss: 0.1040\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9965261580549818\n",
      "Recall Score: 0.9965166666666667\n",
      "F1 Score 0.9965159174294415\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.979282993099858\n",
      "Recall Score: 0.9792\n",
      "F1 Score 0.9792025464451566\n",
      "Epoch 1/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9266 - loss: 0.2581\n",
      "Epoch 2/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9684 - loss: 0.1069\n",
      "Epoch 3/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9785 - loss: 0.0708\n",
      "Epoch 4/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9847 - loss: 0.0515\n",
      "Epoch 5/10\n",
      "938/938 - 5s - 6ms/step - acc: 0.9885 - loss: 0.0383\n",
      "Epoch 6/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9914 - loss: 0.0294\n",
      "Epoch 7/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9929 - loss: 0.0239\n",
      "Epoch 8/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9950 - loss: 0.0170\n",
      "Epoch 9/10\n",
      "938/938 - 5s - 5ms/step - acc: 0.9962 - loss: 0.0140\n",
      "Epoch 10/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9965 - loss: 0.0117\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9772 - loss: 0.0865\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9973279783618413\n",
      "Recall Score: 0.9973166666666666\n",
      "F1 Score 0.9973165887057907\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9795067738799613\n",
      "Recall Score: 0.9794\n",
      "F1 Score 0.979402116518271\n",
      "Epoch 1/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9278 - loss: 0.2504\n",
      "Epoch 2/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9686 - loss: 0.1065\n",
      "Epoch 3/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9785 - loss: 0.0712\n",
      "Epoch 4/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9838 - loss: 0.0543\n",
      "Epoch 5/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9876 - loss: 0.0420\n",
      "Epoch 6/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9897 - loss: 0.0349\n",
      "Epoch 7/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9916 - loss: 0.0280\n",
      "Epoch 8/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9936 - loss: 0.0223\n",
      "Epoch 9/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9948 - loss: 0.0180\n",
      "Epoch 10/10\n",
      "938/938 - 2s - 3ms/step - acc: 0.9960 - loss: 0.0143\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9749 - loss: 0.0869\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.996072769489215\n",
      "Recall Score: 0.9960166666666667\n",
      "F1 Score 0.9960131064405497\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9781155175526275\n",
      "Recall Score: 0.9779\n",
      "F1 Score 0.9779026228397782\n",
      "Epoch 1/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9140 - loss: 0.3088\n",
      "Epoch 2/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9621 - loss: 0.1310\n",
      "Epoch 3/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9744 - loss: 0.0890\n",
      "Epoch 4/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9805 - loss: 0.0666\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9857 - loss: 0.0506\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9884 - loss: 0.0407\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9903 - loss: 0.0329\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9933 - loss: 0.0249\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9952 - loss: 0.0198\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9961 - loss: 0.0164\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9796 - loss: 0.0707\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9977735555241647\n",
      "Recall Score: 0.9977666666666667\n",
      "F1 Score 0.9977671529533612\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9818359342282278\n",
      "Recall Score: 0.9818\n",
      "F1 Score 0.9818047019765368\n",
      "Epoch 1/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9151 - loss: 0.3019\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9615 - loss: 0.1310\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9741 - loss: 0.0896\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9808 - loss: 0.0656\n",
      "Epoch 5/10\n",
      "469/469 - 2s - 3ms/step - acc: 0.9848 - loss: 0.0512\n",
      "Epoch 6/10\n",
      "469/469 - 2s - 3ms/step - acc: 0.9876 - loss: 0.0404\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9905 - loss: 0.0325\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9924 - loss: 0.0268\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9942 - loss: 0.0211\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9956 - loss: 0.0169\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9790 - loss: 0.0818\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.996943018306726\n",
      "Recall Score: 0.9969333333333333\n",
      "F1 Score 0.9969324321438465\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9821249458327975\n",
      "Recall Score: 0.9821\n",
      "F1 Score 0.9820854662861193\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.8947 - loss: 0.3806\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9521 - loss: 0.1690\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9660 - loss: 0.1183\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9739 - loss: 0.0890\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9800 - loss: 0.0711\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9839 - loss: 0.0577\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9863 - loss: 0.0478\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9894 - loss: 0.0390\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9915 - loss: 0.0328\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9935 - loss: 0.0269\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9735 - loss: 0.0762\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9948650246799293\n",
      "Recall Score: 0.9948333333333333\n",
      "F1 Score 0.9948371621401009\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9779458975894693\n",
      "Recall Score: 0.9778\n",
      "F1 Score 0.9778050321677362\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.9003 - loss: 0.3585\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9521 - loss: 0.1661\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9666 - loss: 0.1160\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9743 - loss: 0.0891\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9797 - loss: 0.0709\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9831 - loss: 0.0580\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9860 - loss: 0.0479\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9886 - loss: 0.0410\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9906 - loss: 0.0342\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9923 - loss: 0.0289\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9763 - loss: 0.0763\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9942529486502829\n",
      "Recall Score: 0.9942333333333333\n",
      "F1 Score 0.9942365017980924\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9794840078967498\n",
      "Recall Score: 0.9794\n",
      "F1 Score 0.9794089623644808\n",
      "Epoch 1/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9355 - loss: 0.2251\n",
      "Epoch 2/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9724 - loss: 0.0934\n",
      "Epoch 3/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9811 - loss: 0.0629\n",
      "Epoch 4/20\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9864 - loss: 0.0439\n",
      "Epoch 5/20\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9902 - loss: 0.0324\n",
      "Epoch 6/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9917 - loss: 0.0263\n",
      "Epoch 7/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9941 - loss: 0.0189\n",
      "Epoch 8/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9952 - loss: 0.0154\n",
      "Epoch 9/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9955 - loss: 0.0132\n",
      "Epoch 10/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9958 - loss: 0.0124\n",
      "Epoch 11/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9972 - loss: 0.0089\n",
      "Epoch 12/20\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9972 - loss: 0.0088\n",
      "Epoch 13/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9973 - loss: 0.0081\n",
      "Epoch 14/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9979 - loss: 0.0063\n",
      "Epoch 15/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9973 - loss: 0.0076\n",
      "Epoch 16/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9982 - loss: 0.0055\n",
      "Epoch 17/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9975 - loss: 0.0072\n",
      "Epoch 18/20\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9986 - loss: 0.0041\n",
      "Epoch 19/20\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9981 - loss: 0.0056\n",
      "Epoch 20/20\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9985 - loss: 0.0047\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9775 - loss: 0.1217\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9982225693599278\n",
      "Recall Score: 0.9982166666666666\n",
      "F1 Score 0.9982163088553493\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9801616766197161\n",
      "Recall Score: 0.9801\n",
      "F1 Score 0.9800977052581759\n",
      "Epoch 1/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9336 - loss: 0.2279\n",
      "Epoch 2/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9702 - loss: 0.1000\n",
      "Epoch 3/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9789 - loss: 0.0708\n",
      "Epoch 4/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9837 - loss: 0.0544\n",
      "Epoch 5/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9870 - loss: 0.0438\n",
      "Epoch 6/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9896 - loss: 0.0349\n",
      "Epoch 7/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9912 - loss: 0.0286\n",
      "Epoch 8/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9930 - loss: 0.0242\n",
      "Epoch 9/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9944 - loss: 0.0192\n",
      "Epoch 10/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9954 - loss: 0.0156\n",
      "Epoch 11/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9961 - loss: 0.0127\n",
      "Epoch 12/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9971 - loss: 0.0101\n",
      "Epoch 13/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9974 - loss: 0.0087\n",
      "Epoch 14/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9985 - loss: 0.0061\n",
      "Epoch 15/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9986 - loss: 0.0051\n",
      "Epoch 16/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9991 - loss: 0.0033\n",
      "Epoch 17/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9993 - loss: 0.0027\n",
      "Epoch 18/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9995 - loss: 0.0018\n",
      "Epoch 19/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9996 - loss: 0.0014\n",
      "Epoch 20/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9998 - loss: 9.5998e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9784 - loss: 0.1181\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9999500003755968\n",
      "Recall Score: 0.99995\n",
      "F1 Score 0.9999499988578814\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.981970847768589\n",
      "Recall Score: 0.9819\n",
      "F1 Score 0.9819068083118102\n",
      "Epoch 1/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9248 - loss: 0.2626\n",
      "Epoch 2/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9675 - loss: 0.1091\n",
      "Epoch 3/20\n",
      "938/938 - 5s - 6ms/step - acc: 0.9790 - loss: 0.0720\n",
      "Epoch 4/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9838 - loss: 0.0533\n",
      "Epoch 5/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9880 - loss: 0.0397\n",
      "Epoch 6/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9908 - loss: 0.0309\n",
      "Epoch 7/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9932 - loss: 0.0238\n",
      "Epoch 8/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9944 - loss: 0.0188\n",
      "Epoch 9/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9959 - loss: 0.0148\n",
      "Epoch 10/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9961 - loss: 0.0132\n",
      "Epoch 11/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9978 - loss: 0.0086\n",
      "Epoch 12/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9978 - loss: 0.0078\n",
      "Epoch 13/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9975 - loss: 0.0084\n",
      "Epoch 14/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9980 - loss: 0.0072\n",
      "Epoch 15/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9983 - loss: 0.0065\n",
      "Epoch 16/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9991 - loss: 0.0037\n",
      "Epoch 17/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9983 - loss: 0.0053\n",
      "Epoch 18/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9980 - loss: 0.0067\n",
      "Epoch 19/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9988 - loss: 0.0041\n",
      "Epoch 20/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9990 - loss: 0.0038\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9779 - loss: 0.0931  \n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9994505380908775\n",
      "Recall Score: 0.99945\n",
      "F1 Score 0.9994499765928216\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9816303681522126\n",
      "Recall Score: 0.9816\n",
      "F1 Score 0.9815983349789514\n",
      "Epoch 1/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9268 - loss: 0.2545\n",
      "Epoch 2/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9677 - loss: 0.1096\n",
      "Epoch 3/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9778 - loss: 0.0747\n",
      "Epoch 4/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9831 - loss: 0.0559\n",
      "Epoch 5/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9868 - loss: 0.0432\n",
      "Epoch 6/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9894 - loss: 0.0349\n",
      "Epoch 7/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9918 - loss: 0.0278\n",
      "Epoch 8/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9932 - loss: 0.0225\n",
      "Epoch 9/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9948 - loss: 0.0179\n",
      "Epoch 10/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9961 - loss: 0.0143\n",
      "Epoch 11/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9967 - loss: 0.0116\n",
      "Epoch 12/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9973 - loss: 0.0095\n",
      "Epoch 13/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9982 - loss: 0.0069\n",
      "Epoch 14/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9986 - loss: 0.0054\n",
      "Epoch 15/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9990 - loss: 0.0044\n",
      "Epoch 16/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9995 - loss: 0.0029\n",
      "Epoch 17/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9995 - loss: 0.0023\n",
      "Epoch 18/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9998 - loss: 0.0017\n",
      "Epoch 19/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9998 - loss: 0.0012\n",
      "Epoch 20/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9999 - loss: 6.9747e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9779 - loss: 0.1040\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9999833358050324\n",
      "Recall Score: 0.9999833333333333\n",
      "F1 Score 0.9999833332861803\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9815278813845462\n",
      "Recall Score: 0.9815\n",
      "F1 Score 0.9814940165862784\n",
      "Epoch 1/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9143 - loss: 0.3087\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9615 - loss: 0.1328\n",
      "Epoch 3/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9737 - loss: 0.0900\n",
      "Epoch 4/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9804 - loss: 0.0679\n",
      "Epoch 5/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9847 - loss: 0.0521\n",
      "Epoch 6/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9879 - loss: 0.0422\n",
      "Epoch 7/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9905 - loss: 0.0331\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9929 - loss: 0.0258\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9947 - loss: 0.0207\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9958 - loss: 0.0166\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9969 - loss: 0.0130\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9976 - loss: 0.0107\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9984 - loss: 0.0085\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9988 - loss: 0.0067\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9983 - loss: 0.0075\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9991 - loss: 0.0054\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9988 - loss: 0.0054\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9990 - loss: 0.0049\n",
      "Epoch 19/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9990 - loss: 0.0041\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9990 - loss: 0.0042\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9787 - loss: 0.0907  \n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9999500087377167\n",
      "Recall Score: 0.99995\n",
      "F1 Score 0.9999500000912148\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9816979035818855\n",
      "Recall Score: 0.9817\n",
      "F1 Score 0.9816971024822222\n",
      "Epoch 1/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9160 - loss: 0.2986\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9621 - loss: 0.1293\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9739 - loss: 0.0878\n",
      "Epoch 4/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9805 - loss: 0.0658\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9851 - loss: 0.0516\n",
      "Epoch 6/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9879 - loss: 0.0415\n",
      "Epoch 7/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9902 - loss: 0.0337\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9920 - loss: 0.0274\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9939 - loss: 0.0222\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9952 - loss: 0.0178\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9964 - loss: 0.0146\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9973 - loss: 0.0116\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9978 - loss: 0.0095\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9986 - loss: 0.0073\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9989 - loss: 0.0060\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9991 - loss: 0.0049\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9993 - loss: 0.0038\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9994 - loss: 0.0031\n",
      "Epoch 19/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9997 - loss: 0.0021\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9998 - loss: 0.0016\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9793 - loss: 0.0903\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9999500054609779\n",
      "Recall Score: 0.99995\n",
      "F1 Score 0.999949998892924\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9820325078465055\n",
      "Recall Score: 0.982\n",
      "F1 Score 0.9819988594991781\n",
      "Epoch 1/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.8960 - loss: 0.3825\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9532 - loss: 0.1644\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9673 - loss: 0.1152\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9749 - loss: 0.0884\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9805 - loss: 0.0697\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9844 - loss: 0.0569\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9871 - loss: 0.0472\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9888 - loss: 0.0395\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9913 - loss: 0.0335\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9929 - loss: 0.0276\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9946 - loss: 0.0233\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9952 - loss: 0.0205\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9966 - loss: 0.0164\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9970 - loss: 0.0145\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9978 - loss: 0.0120\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9984 - loss: 0.0101\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9989 - loss: 0.0083\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9989 - loss: 0.0077\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9993 - loss: 0.0060\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9994 - loss: 0.0055\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9801 - loss: 0.0690\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9999166901414634\n",
      "Recall Score: 0.9999166666666667\n",
      "F1 Score 0.9999166674838158\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.982324435426674\n",
      "Recall Score: 0.9823\n",
      "F1 Score 0.9823022952757947\n",
      "Epoch 1/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.8999 - loss: 0.3629\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9521 - loss: 0.1665\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9668 - loss: 0.1167\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9746 - loss: 0.0896\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9795 - loss: 0.0712\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9836 - loss: 0.0580\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9865 - loss: 0.0481\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9882 - loss: 0.0408\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9902 - loss: 0.0346\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9923 - loss: 0.0291\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9936 - loss: 0.0246\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9947 - loss: 0.0209\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9959 - loss: 0.0176\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9967 - loss: 0.0149\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9972 - loss: 0.0128\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9981 - loss: 0.0104\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9982 - loss: 0.0091\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9987 - loss: 0.0076\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9991 - loss: 0.0061\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9994 - loss: 0.0050\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9764 - loss: 0.0875\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9987942453774673\n",
      "Recall Score: 0.9987833333333334\n",
      "F1 Score 0.9987854432657566\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點256, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9791638313981748\n",
      "Recall Score: 0.979\n",
      "F1 Score 0.9790147419164846\n",
      "Epoch 1/5\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9410 - loss: 0.2004\n",
      "Epoch 2/5\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9757 - loss: 0.0795\n",
      "Epoch 3/5\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9836 - loss: 0.0529\n",
      "Epoch 4/5\n",
      "1875/1875 - 12s - 7ms/step - acc: 0.9882 - loss: 0.0368\n",
      "Epoch 5/5\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9911 - loss: 0.0281\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9774 - loss: 0.0798\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.993523087780216\n",
      "Recall Score: 0.9934333333333333\n",
      "F1 Score 0.9934431229051409\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9794538366710007\n",
      "Recall Score: 0.9792\n",
      "F1 Score 0.9792393843702237\n",
      "Epoch 1/5\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9410 - loss: 0.1999\n",
      "Epoch 2/5\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9742 - loss: 0.0856\n",
      "Epoch 3/5\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9827 - loss: 0.0590\n",
      "Epoch 4/5\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9867 - loss: 0.0448\n",
      "Epoch 5/5\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9897 - loss: 0.0357\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9750 - loss: 0.0988\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9930481538060576\n",
      "Recall Score: 0.993\n",
      "F1 Score 0.9930049124636318\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9793901004237028\n",
      "Recall Score: 0.9792\n",
      "F1 Score 0.9792136826164808\n",
      "Epoch 1/5\n",
      "938/938 - 7s - 8ms/step - acc: 0.9357 - loss: 0.2261\n",
      "Epoch 2/5\n",
      "938/938 - 6s - 6ms/step - acc: 0.9731 - loss: 0.0908\n",
      "Epoch 3/5\n",
      "938/938 - 6s - 7ms/step - acc: 0.9826 - loss: 0.0578\n",
      "Epoch 4/5\n",
      "938/938 - 7s - 7ms/step - acc: 0.9870 - loss: 0.0413\n",
      "Epoch 5/5\n",
      "938/938 - 6s - 6ms/step - acc: 0.9919 - loss: 0.0277\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9781 - loss: 0.0735\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.995232457196684\n",
      "Recall Score: 0.9952166666666666\n",
      "F1 Score 0.9952148369309577\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.981063028596848\n",
      "Recall Score: 0.981\n",
      "F1 Score 0.9809980409938344\n",
      "Epoch 1/5\n",
      "938/938 - 5s - 6ms/step - acc: 0.9332 - loss: 0.2296\n",
      "Epoch 2/5\n",
      "938/938 - 4s - 5ms/step - acc: 0.9729 - loss: 0.0913\n",
      "Epoch 3/5\n",
      "938/938 - 5s - 5ms/step - acc: 0.9812 - loss: 0.0608\n",
      "Epoch 4/5\n",
      "938/938 - 4s - 5ms/step - acc: 0.9869 - loss: 0.0436\n",
      "Epoch 5/5\n",
      "938/938 - 5s - 5ms/step - acc: 0.9891 - loss: 0.0333\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9778 - loss: 0.0720\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9942072494984081\n",
      "Recall Score: 0.9941833333333333\n",
      "F1 Score 0.9941869814017772\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9815569702060256\n",
      "Recall Score: 0.9815\n",
      "F1 Score 0.9815084439666736\n",
      "Epoch 1/5\n",
      "469/469 - 4s - 9ms/step - acc: 0.9226 - loss: 0.2692\n",
      "Epoch 2/5\n",
      "469/469 - 3s - 7ms/step - acc: 0.9677 - loss: 0.1102\n",
      "Epoch 3/5\n",
      "469/469 - 3s - 7ms/step - acc: 0.9790 - loss: 0.0716\n",
      "Epoch 4/5\n",
      "469/469 - 3s - 7ms/step - acc: 0.9847 - loss: 0.0517\n",
      "Epoch 5/5\n",
      "469/469 - 3s - 7ms/step - acc: 0.9887 - loss: 0.0381\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9774 - loss: 0.0717\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9939758175790686\n",
      "Recall Score: 0.9939666666666667\n",
      "F1 Score 0.9939667045961287\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9802645644276892\n",
      "Recall Score: 0.9802\n",
      "F1 Score 0.9802034731215196\n",
      "Epoch 1/5\n",
      "469/469 - 3s - 6ms/step - acc: 0.9247 - loss: 0.2619\n",
      "Epoch 2/5\n",
      "469/469 - 3s - 7ms/step - acc: 0.9667 - loss: 0.1098\n",
      "Epoch 3/5\n",
      "469/469 - 3s - 6ms/step - acc: 0.9785 - loss: 0.0721\n",
      "Epoch 4/5\n",
      "469/469 - 3s - 6ms/step - acc: 0.9850 - loss: 0.0517\n",
      "Epoch 5/5\n",
      "469/469 - 3s - 6ms/step - acc: 0.9882 - loss: 0.0392\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9780 - loss: 0.0719\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9939980943820742\n",
      "Recall Score: 0.9939833333333333\n",
      "F1 Score 0.9939844127458709\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9814347301871172\n",
      "Recall Score: 0.9814\n",
      "F1 Score 0.9814030757632342\n",
      "Epoch 1/5\n",
      "235/235 - 3s - 12ms/step - acc: 0.9110 - loss: 0.3218\n",
      "Epoch 2/5\n",
      "235/235 - 2s - 9ms/step - acc: 0.9604 - loss: 0.1378\n",
      "Epoch 3/5\n",
      "235/235 - 2s - 8ms/step - acc: 0.9732 - loss: 0.0922\n",
      "Epoch 4/5\n",
      "235/235 - 2s - 8ms/step - acc: 0.9807 - loss: 0.0677\n",
      "Epoch 5/5\n",
      "235/235 - 2s - 7ms/step - acc: 0.9854 - loss: 0.0515\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9732 - loss: 0.0803\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9893791605615246\n",
      "Recall Score: 0.9893333333333333\n",
      "F1 Score 0.9893322517280666\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9774503201975809\n",
      "Recall Score: 0.9774\n",
      "F1 Score 0.9773979465448424\n",
      "Epoch 1/5\n",
      "235/235 - 2s - 9ms/step - acc: 0.9087 - loss: 0.3219\n",
      "Epoch 2/5\n",
      "235/235 - 2s - 7ms/step - acc: 0.9608 - loss: 0.1386\n",
      "Epoch 3/5\n",
      "235/235 - 2s - 7ms/step - acc: 0.9730 - loss: 0.0931\n",
      "Epoch 4/5\n",
      "235/235 - 2s - 6ms/step - acc: 0.9804 - loss: 0.0686\n",
      "Epoch 5/5\n",
      "235/235 - 2s - 6ms/step - acc: 0.9846 - loss: 0.0528\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9719 - loss: 0.0928\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9876679834651427\n",
      "Recall Score: 0.9875833333333334\n",
      "F1 Score 0.9875787640722647\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9772213115748959\n",
      "Recall Score: 0.9771\n",
      "F1 Score 0.9770902402794136\n",
      "Epoch 1/10\n",
      "1875/1875 - 12s - 7ms/step - acc: 0.9420 - loss: 0.2008\n",
      "Epoch 2/10\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9754 - loss: 0.0810\n",
      "Epoch 3/10\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9830 - loss: 0.0538\n",
      "Epoch 4/10\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9882 - loss: 0.0369\n",
      "Epoch 5/10\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9911 - loss: 0.0285\n",
      "Epoch 6/10\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9929 - loss: 0.0214\n",
      "Epoch 7/10\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9943 - loss: 0.0174\n",
      "Epoch 8/10\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9947 - loss: 0.0150\n",
      "Epoch 9/10\n",
      "1875/1875 - 12s - 7ms/step - acc: 0.9963 - loss: 0.0119\n",
      "Epoch 10/10\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9964 - loss: 0.0104\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9769 - loss: 0.1071\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9967229447819316\n",
      "Recall Score: 0.9967166666666667\n",
      "F1 Score 0.9967166963150176\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9793522535046232\n",
      "Recall Score: 0.9793\n",
      "F1 Score 0.9793024457153225\n",
      "Epoch 1/10\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9390 - loss: 0.2036\n",
      "Epoch 2/10\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9740 - loss: 0.0877\n",
      "Epoch 3/10\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9819 - loss: 0.0613\n",
      "Epoch 4/10\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9865 - loss: 0.0459\n",
      "Epoch 5/10\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9901 - loss: 0.0345\n",
      "Epoch 6/10\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9920 - loss: 0.0267\n",
      "Epoch 7/10\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9936 - loss: 0.0212\n",
      "Epoch 8/10\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9954 - loss: 0.0153\n",
      "Epoch 9/10\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9959 - loss: 0.0127\n",
      "Epoch 10/10\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9973 - loss: 0.0090\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9776 - loss: 0.1003\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9980375971044662\n",
      "Recall Score: 0.9980333333333333\n",
      "F1 Score 0.9980317641985043\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9819654503436647\n",
      "Recall Score: 0.9819\n",
      "F1 Score 0.9818819864097728\n",
      "Epoch 1/10\n",
      "938/938 - 6s - 6ms/step - acc: 0.9336 - loss: 0.2278\n",
      "Epoch 2/10\n",
      "938/938 - 5s - 5ms/step - acc: 0.9728 - loss: 0.0903\n",
      "Epoch 3/10\n",
      "938/938 - 5s - 6ms/step - acc: 0.9829 - loss: 0.0576\n",
      "Epoch 4/10\n",
      "938/938 - 5s - 5ms/step - acc: 0.9879 - loss: 0.0404\n",
      "Epoch 5/10\n",
      "938/938 - 5s - 5ms/step - acc: 0.9910 - loss: 0.0287\n",
      "Epoch 6/10\n",
      "938/938 - 5s - 5ms/step - acc: 0.9937 - loss: 0.0216\n",
      "Epoch 7/10\n",
      "938/938 - 5s - 6ms/step - acc: 0.9950 - loss: 0.0171\n",
      "Epoch 8/10\n",
      "938/938 - 5s - 5ms/step - acc: 0.9965 - loss: 0.0120\n",
      "Epoch 9/10\n",
      "938/938 - 5s - 5ms/step - acc: 0.9958 - loss: 0.0125\n",
      "Epoch 10/10\n",
      "938/938 - 5s - 6ms/step - acc: 0.9971 - loss: 0.0092\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9768 - loss: 0.0869\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9973830777949936\n",
      "Recall Score: 0.9973666666666666\n",
      "F1 Score 0.9973675828742716\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9805274114620953\n",
      "Recall Score: 0.9803\n",
      "F1 Score 0.9803191649000961\n",
      "Epoch 1/10\n",
      "938/938 - 5s - 5ms/step - acc: 0.9339 - loss: 0.2259\n",
      "Epoch 2/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9725 - loss: 0.0912\n",
      "Epoch 3/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9825 - loss: 0.0601\n",
      "Epoch 4/10\n",
      "938/938 - 5s - 5ms/step - acc: 0.9865 - loss: 0.0443\n",
      "Epoch 5/10\n",
      "938/938 - 4s - 5ms/step - acc: 0.9897 - loss: 0.0332\n",
      "Epoch 6/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9921 - loss: 0.0256\n",
      "Epoch 7/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9947 - loss: 0.0177\n",
      "Epoch 8/10\n",
      "938/938 - 5s - 5ms/step - acc: 0.9959 - loss: 0.0139\n",
      "Epoch 9/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9972 - loss: 0.0106\n",
      "Epoch 10/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9980 - loss: 0.0077\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9789 - loss: 0.0857\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9987525233360306\n",
      "Recall Score: 0.99875\n",
      "F1 Score 0.9987497381309678\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9825563606096573\n",
      "Recall Score: 0.9825\n",
      "F1 Score 0.9824955489780212\n",
      "Epoch 1/10\n",
      "469/469 - 3s - 7ms/step - acc: 0.9236 - loss: 0.2704\n",
      "Epoch 2/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9683 - loss: 0.1093\n",
      "Epoch 3/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9792 - loss: 0.0710\n",
      "Epoch 4/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9851 - loss: 0.0505\n",
      "Epoch 5/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9898 - loss: 0.0367\n",
      "Epoch 6/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9926 - loss: 0.0269\n",
      "Epoch 7/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9944 - loss: 0.0206\n",
      "Epoch 8/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9962 - loss: 0.0158\n",
      "Epoch 9/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9971 - loss: 0.0117\n",
      "Epoch 10/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9977 - loss: 0.0099\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9780 - loss: 0.0769\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9983856787097048\n",
      "Recall Score: 0.9983833333333333\n",
      "F1 Score 0.998383291318085\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9815378798298702\n",
      "Recall Score: 0.9815\n",
      "F1 Score 0.9814928420671711\n",
      "Epoch 1/10\n",
      "469/469 - 3s - 7ms/step - acc: 0.9246 - loss: 0.2632\n",
      "Epoch 2/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9681 - loss: 0.1099\n",
      "Epoch 3/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9791 - loss: 0.0713\n",
      "Epoch 4/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9847 - loss: 0.0512\n",
      "Epoch 5/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9879 - loss: 0.0394\n",
      "Epoch 6/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9915 - loss: 0.0294\n",
      "Epoch 7/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9933 - loss: 0.0228\n",
      "Epoch 8/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9955 - loss: 0.0168\n",
      "Epoch 9/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9969 - loss: 0.0129\n",
      "Epoch 10/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9975 - loss: 0.0100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9786 - loss: 0.0766\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9987360672783331\n",
      "Recall Score: 0.9987333333333334\n",
      "F1 Score 0.9987335371286085\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9813890220361827\n",
      "Recall Score: 0.9813\n",
      "F1 Score 0.9813102238054578\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 10ms/step - acc: 0.9078 - loss: 0.3283\n",
      "Epoch 2/10\n",
      "235/235 - 2s - 8ms/step - acc: 0.9603 - loss: 0.1394\n",
      "Epoch 3/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.9731 - loss: 0.0930\n",
      "Epoch 4/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.9806 - loss: 0.0678\n",
      "Epoch 5/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.9853 - loss: 0.0517\n",
      "Epoch 6/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.9889 - loss: 0.0400\n",
      "Epoch 7/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.9913 - loss: 0.0322\n",
      "Epoch 8/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.9936 - loss: 0.0252\n",
      "Epoch 9/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.9955 - loss: 0.0198\n",
      "Epoch 10/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.9970 - loss: 0.0151\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9774 - loss: 0.0747\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.998436624615943\n",
      "Recall Score: 0.9984333333333333\n",
      "F1 Score 0.9984336194204644\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9812663178159915\n",
      "Recall Score: 0.9812\n",
      "F1 Score 0.9812086973696578\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 9ms/step - acc: 0.9051 - loss: 0.3307\n",
      "Epoch 2/10\n",
      "235/235 - 2s - 6ms/step - acc: 0.9608 - loss: 0.1381\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9728 - loss: 0.0927\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9800 - loss: 0.0681\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9843 - loss: 0.0521\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9879 - loss: 0.0411\n",
      "Epoch 7/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9910 - loss: 0.0327\n",
      "Epoch 8/10\n",
      "235/235 - 2s - 6ms/step - acc: 0.9926 - loss: 0.0265\n",
      "Epoch 9/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.9943 - loss: 0.0212\n",
      "Epoch 10/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.9961 - loss: 0.0168\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9778 - loss: 0.0712\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9978393472091491\n",
      "Recall Score: 0.9978333333333333\n",
      "F1 Score 0.9978341140940791\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9804589467105188\n",
      "Recall Score: 0.9804\n",
      "F1 Score 0.980399795736554\n",
      "Epoch 1/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9399 - loss: 0.2029\n",
      "Epoch 2/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9748 - loss: 0.0807\n",
      "Epoch 3/20\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9845 - loss: 0.0509\n",
      "Epoch 4/20\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9881 - loss: 0.0369\n",
      "Epoch 5/20\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9913 - loss: 0.0268\n",
      "Epoch 6/20\n",
      "1875/1875 - 12s - 7ms/step - acc: 0.9926 - loss: 0.0221\n",
      "Epoch 7/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9946 - loss: 0.0157\n",
      "Epoch 8/20\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9954 - loss: 0.0145\n",
      "Epoch 9/20\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9959 - loss: 0.0123\n",
      "Epoch 10/20\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9965 - loss: 0.0104\n",
      "Epoch 11/20\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9966 - loss: 0.0110\n",
      "Epoch 12/20\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9974 - loss: 0.0074\n",
      "Epoch 13/20\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9970 - loss: 0.0082\n",
      "Epoch 14/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9977 - loss: 0.0077\n",
      "Epoch 15/20\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9979 - loss: 0.0063\n",
      "Epoch 16/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9980 - loss: 0.0062\n",
      "Epoch 17/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9979 - loss: 0.0059\n",
      "Epoch 18/20\n",
      "1875/1875 - 18s - 9ms/step - acc: 0.9981 - loss: 0.0051\n",
      "Epoch 19/20\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9978 - loss: 0.0062\n",
      "Epoch 20/20\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9977 - loss: 0.0069\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9789 - loss: 0.1314\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.998551467946111\n",
      "Recall Score: 0.99855\n",
      "F1 Score 0.9985497497041635\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9824859598385131\n",
      "Recall Score: 0.9824\n",
      "F1 Score 0.9824118960011722\n",
      "Epoch 1/20\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9390 - loss: 0.2022\n",
      "Epoch 2/20\n",
      "1875/1875 - 8s - 5ms/step - acc: 0.9737 - loss: 0.0845\n",
      "Epoch 3/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9821 - loss: 0.0585\n",
      "Epoch 4/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9872 - loss: 0.0450\n",
      "Epoch 5/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9902 - loss: 0.0336\n",
      "Epoch 6/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9928 - loss: 0.0249\n",
      "Epoch 7/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9936 - loss: 0.0206\n",
      "Epoch 8/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9954 - loss: 0.0149\n",
      "Epoch 9/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9966 - loss: 0.0111\n",
      "Epoch 10/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9974 - loss: 0.0083\n",
      "Epoch 11/20\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9980 - loss: 0.0068\n",
      "Epoch 12/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9987 - loss: 0.0049\n",
      "Epoch 13/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9991 - loss: 0.0032\n",
      "Epoch 14/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9994 - loss: 0.0022\n",
      "Epoch 15/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9996 - loss: 0.0017\n",
      "Epoch 16/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9998 - loss: 8.8314e-04\n",
      "Epoch 17/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9999 - loss: 6.4244e-04\n",
      "Epoch 18/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9999 - loss: 2.4614e-04\n",
      "Epoch 19/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 1.0000 - loss: 1.5148e-04\n",
      "Epoch 20/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 1.0000 - loss: 1.2034e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9810 - loss: 0.1031\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9843242766070096\n",
      "Recall Score: 0.9843\n",
      "F1 Score 0.9842999609364781\n",
      "Epoch 1/20\n",
      "938/938 - 6s - 6ms/step - acc: 0.9349 - loss: 0.2255\n",
      "Epoch 2/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9728 - loss: 0.0896\n",
      "Epoch 3/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9823 - loss: 0.0581\n",
      "Epoch 4/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9878 - loss: 0.0399\n",
      "Epoch 5/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9915 - loss: 0.0282\n",
      "Epoch 6/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9936 - loss: 0.0221\n",
      "Epoch 7/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9948 - loss: 0.0170\n",
      "Epoch 8/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9950 - loss: 0.0151\n",
      "Epoch 9/20\n",
      "938/938 - 6s - 6ms/step - acc: 0.9966 - loss: 0.0107\n",
      "Epoch 10/20\n",
      "938/938 - 6s - 6ms/step - acc: 0.9974 - loss: 0.0088\n",
      "Epoch 11/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9983 - loss: 0.0062\n",
      "Epoch 12/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9964 - loss: 0.0107\n",
      "Epoch 13/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9981 - loss: 0.0062\n",
      "Epoch 14/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9976 - loss: 0.0079\n",
      "Epoch 15/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9982 - loss: 0.0055\n",
      "Epoch 16/20\n",
      "938/938 - 5s - 6ms/step - acc: 0.9985 - loss: 0.0047\n",
      "Epoch 17/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9986 - loss: 0.0044\n",
      "Epoch 18/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9981 - loss: 0.0060\n",
      "Epoch 19/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9990 - loss: 0.0037\n",
      "Epoch 20/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9985 - loss: 0.0052\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9777 - loss: 0.1193\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9969492603982473\n",
      "Recall Score: 0.9969333333333333\n",
      "F1 Score 0.9969336733964689\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9797455293859666\n",
      "Recall Score: 0.9796\n",
      "F1 Score 0.979599853691383\n",
      "Epoch 1/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9344 - loss: 0.2259\n",
      "Epoch 2/20\n",
      "938/938 - 4s - 5ms/step - acc: 0.9725 - loss: 0.0905\n",
      "Epoch 3/20\n",
      "938/938 - 4s - 5ms/step - acc: 0.9816 - loss: 0.0595\n",
      "Epoch 4/20\n",
      "938/938 - 4s - 5ms/step - acc: 0.9860 - loss: 0.0440\n",
      "Epoch 5/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9902 - loss: 0.0318\n",
      "Epoch 6/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9925 - loss: 0.0247\n",
      "Epoch 7/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9944 - loss: 0.0177\n",
      "Epoch 8/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9959 - loss: 0.0140\n",
      "Epoch 9/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9970 - loss: 0.0104\n",
      "Epoch 10/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9977 - loss: 0.0075\n",
      "Epoch 11/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9986 - loss: 0.0055\n",
      "Epoch 12/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9989 - loss: 0.0044\n",
      "Epoch 13/20\n",
      "938/938 - 4s - 5ms/step - acc: 0.9994 - loss: 0.0027\n",
      "Epoch 14/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9998 - loss: 0.0014\n",
      "Epoch 15/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9999 - loss: 6.6834e-04\n",
      "Epoch 16/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9999 - loss: 4.9791e-04\n",
      "Epoch 17/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9999 - loss: 3.2187e-04\n",
      "Epoch 18/20\n",
      "938/938 - 4s - 5ms/step - acc: 1.0000 - loss: 1.9597e-04\n",
      "Epoch 19/20\n",
      "938/938 - 4s - 5ms/step - acc: 1.0000 - loss: 1.2072e-04\n",
      "Epoch 20/20\n",
      "938/938 - 4s - 4ms/step - acc: 1.0000 - loss: 1.0807e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9795 - loss: 0.0915\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.983106841847332\n",
      "Recall Score: 0.9831\n",
      "F1 Score 0.983098281407508\n",
      "Epoch 1/20\n",
      "469/469 - 4s - 8ms/step - acc: 0.9250 - loss: 0.2639\n",
      "Epoch 2/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9689 - loss: 0.1060\n",
      "Epoch 3/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9795 - loss: 0.0693\n",
      "Epoch 4/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9856 - loss: 0.0491\n",
      "Epoch 5/20\n",
      "469/469 - 3s - 7ms/step - acc: 0.9898 - loss: 0.0354\n",
      "Epoch 6/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9929 - loss: 0.0261\n",
      "Epoch 7/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9950 - loss: 0.0194\n",
      "Epoch 8/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9967 - loss: 0.0144\n",
      "Epoch 9/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9972 - loss: 0.0114\n",
      "Epoch 10/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9978 - loss: 0.0096\n",
      "Epoch 11/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9978 - loss: 0.0085\n",
      "Epoch 12/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9981 - loss: 0.0072\n",
      "Epoch 13/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9990 - loss: 0.0050\n",
      "Epoch 14/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9981 - loss: 0.0066\n",
      "Epoch 15/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9987 - loss: 0.0052\n",
      "Epoch 16/20\n",
      "469/469 - 3s - 7ms/step - acc: 0.9998 - loss: 0.0020\n",
      "Epoch 17/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9987 - loss: 0.0046\n",
      "Epoch 18/20\n",
      "469/469 - 3s - 7ms/step - acc: 0.9975 - loss: 0.0082\n",
      "Epoch 19/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9986 - loss: 0.0049\n",
      "Epoch 20/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9995 - loss: 0.0022\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9798 - loss: 0.0830\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9998833806003737\n",
      "Recall Score: 0.9998833333333333\n",
      "F1 Score 0.999883333058716\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9833718975959062\n",
      "Recall Score: 0.9833\n",
      "F1 Score 0.983310641062306\n",
      "Epoch 1/20\n",
      "469/469 - 3s - 7ms/step - acc: 0.9251 - loss: 0.2588\n",
      "Epoch 2/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9685 - loss: 0.1057\n",
      "Epoch 3/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9789 - loss: 0.0698\n",
      "Epoch 4/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9844 - loss: 0.0508\n",
      "Epoch 5/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9889 - loss: 0.0375\n",
      "Epoch 6/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9912 - loss: 0.0288\n",
      "Epoch 7/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9935 - loss: 0.0223\n",
      "Epoch 8/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9956 - loss: 0.0165\n",
      "Epoch 9/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9967 - loss: 0.0125\n",
      "Epoch 10/20\n",
      "469/469 - 3s - 5ms/step - acc: 0.9977 - loss: 0.0091\n",
      "Epoch 11/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9985 - loss: 0.0067\n",
      "Epoch 12/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9991 - loss: 0.0048\n",
      "Epoch 13/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9995 - loss: 0.0035\n",
      "Epoch 14/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9995 - loss: 0.0027\n",
      "Epoch 15/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9998 - loss: 0.0016\n",
      "Epoch 16/20\n",
      "469/469 - 2s - 5ms/step - acc: 1.0000 - loss: 9.8247e-04\n",
      "Epoch 17/20\n",
      "469/469 - 2s - 5ms/step - acc: 1.0000 - loss: 6.4843e-04\n",
      "Epoch 18/20\n",
      "469/469 - 2s - 5ms/step - acc: 1.0000 - loss: 5.1976e-04\n",
      "Epoch 19/20\n",
      "469/469 - 2s - 5ms/step - acc: 1.0000 - loss: 4.0207e-04\n",
      "Epoch 20/20\n",
      "469/469 - 2s - 5ms/step - acc: 1.0000 - loss: 3.5375e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9805 - loss: 0.0828\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9828063912550475\n",
      "Recall Score: 0.9828\n",
      "F1 Score 0.9827987652392319\n",
      "Epoch 1/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9101 - loss: 0.3249\n",
      "Epoch 2/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9606 - loss: 0.1379\n",
      "Epoch 3/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9731 - loss: 0.0938\n",
      "Epoch 4/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9808 - loss: 0.0681\n",
      "Epoch 5/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9855 - loss: 0.0515\n",
      "Epoch 6/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9884 - loss: 0.0408\n",
      "Epoch 7/20\n",
      "235/235 - 2s - 8ms/step - acc: 0.9915 - loss: 0.0318\n",
      "Epoch 8/20\n",
      "235/235 - 2s - 8ms/step - acc: 0.9939 - loss: 0.0249\n",
      "Epoch 9/20\n",
      "235/235 - 2s - 8ms/step - acc: 0.9950 - loss: 0.0202\n",
      "Epoch 10/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9966 - loss: 0.0159\n",
      "Epoch 11/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9976 - loss: 0.0126\n",
      "Epoch 12/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9985 - loss: 0.0097\n",
      "Epoch 13/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9990 - loss: 0.0074\n",
      "Epoch 14/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9993 - loss: 0.0060\n",
      "Epoch 15/20\n",
      "235/235 - 2s - 8ms/step - acc: 0.9994 - loss: 0.0050\n",
      "Epoch 16/20\n",
      "235/235 - 2s - 8ms/step - acc: 0.9996 - loss: 0.0040\n",
      "Epoch 17/20\n",
      "235/235 - 2s - 8ms/step - acc: 0.9998 - loss: 0.0030\n",
      "Epoch 18/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9998 - loss: 0.0024\n",
      "Epoch 19/20\n",
      "235/235 - 2s - 8ms/step - acc: 1.0000 - loss: 0.0018\n",
      "Epoch 20/20\n",
      "235/235 - 2s - 7ms/step - acc: 1.0000 - loss: 0.0014\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9781 - loss: 0.0787\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9819188866944687\n",
      "Recall Score: 0.9819\n",
      "F1 Score 0.9818962770879307\n",
      "Epoch 1/20\n",
      "235/235 - 2s - 9ms/step - acc: 0.9086 - loss: 0.3248\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9601 - loss: 0.1389\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9735 - loss: 0.0924\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9799 - loss: 0.0685\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9844 - loss: 0.0535\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9880 - loss: 0.0420\n",
      "Epoch 7/20\n",
      "235/235 - 2s - 6ms/step - acc: 0.9906 - loss: 0.0340\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9930 - loss: 0.0268\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9943 - loss: 0.0215\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9955 - loss: 0.0172\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9970 - loss: 0.0135\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9978 - loss: 0.0111\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9985 - loss: 0.0085\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9989 - loss: 0.0067\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9993 - loss: 0.0051\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9996 - loss: 0.0038\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9998 - loss: 0.0031\n",
      "Epoch 18/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9999 - loss: 0.0023\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9999 - loss: 0.0018\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 6ms/step - acc: 1.0000 - loss: 0.0014\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9803 - loss: 0.0754\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點512, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.983424284490296\n",
      "Recall Score: 0.9834\n",
      "F1 Score 0.9834018052542147\n",
      "Epoch 1/5\n",
      "1875/1875 - 26s - 14ms/step - acc: 0.9455 - loss: 0.1834\n",
      "Epoch 2/5\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9775 - loss: 0.0754\n",
      "Epoch 3/5\n",
      "1875/1875 - 18s - 10ms/step - acc: 0.9849 - loss: 0.0489\n",
      "Epoch 4/5\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9885 - loss: 0.0345\n",
      "Epoch 5/5\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9920 - loss: 0.0250\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9707 - loss: 0.1013\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9928275450623895\n",
      "Recall Score: 0.9927333333333334\n",
      "F1 Score 0.9927387732754475\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9761595674742027\n",
      "Recall Score: 0.9757\n",
      "F1 Score 0.9757181625249058\n",
      "Epoch 1/5\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9435 - loss: 0.1895\n",
      "Epoch 2/5\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9764 - loss: 0.0795\n",
      "Epoch 3/5\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9842 - loss: 0.0546\n",
      "Epoch 4/5\n",
      "1875/1875 - 16s - 9ms/step - acc: 0.9879 - loss: 0.0402\n",
      "Epoch 5/5\n",
      "1875/1875 - 16s - 9ms/step - acc: 0.9913 - loss: 0.0295\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9744 - loss: 0.1093\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.992728136610935\n",
      "Recall Score: 0.9926833333333334\n",
      "F1 Score 0.9926758929105297\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9793543065176531\n",
      "Recall Score: 0.9792\n",
      "F1 Score 0.9791798077802457\n",
      "Epoch 1/5\n",
      "938/938 - 11s - 12ms/step - acc: 0.9394 - loss: 0.2032\n",
      "Epoch 2/5\n",
      "938/938 - 11s - 11ms/step - acc: 0.9765 - loss: 0.0783\n",
      "Epoch 3/5\n",
      "938/938 - 11s - 11ms/step - acc: 0.9846 - loss: 0.0508\n",
      "Epoch 4/5\n",
      "938/938 - 11s - 11ms/step - acc: 0.9890 - loss: 0.0338\n",
      "Epoch 5/5\n",
      "938/938 - 11s - 12ms/step - acc: 0.9918 - loss: 0.0246\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9778 - loss: 0.0814\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9943132414532097\n",
      "Recall Score: 0.9942666666666666\n",
      "F1 Score 0.9942719390743451\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9808563705174776\n",
      "Recall Score: 0.9807\n",
      "F1 Score 0.9807163720835861\n",
      "Epoch 1/5\n",
      "938/938 - 10s - 10ms/step - acc: 0.9389 - loss: 0.2061\n",
      "Epoch 2/5\n",
      "938/938 - 8s - 9ms/step - acc: 0.9752 - loss: 0.0809\n",
      "Epoch 3/5\n",
      "938/938 - 9s - 9ms/step - acc: 0.9839 - loss: 0.0524\n",
      "Epoch 4/5\n",
      "938/938 - 9s - 9ms/step - acc: 0.9887 - loss: 0.0360\n",
      "Epoch 5/5\n",
      "938/938 - 9s - 9ms/step - acc: 0.9920 - loss: 0.0257\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9790 - loss: 0.0746\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9954006103179643\n",
      "Recall Score: 0.9953833333333333\n",
      "F1 Score 0.9953847586404436\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9817581151343183\n",
      "Recall Score: 0.9817\n",
      "F1 Score 0.9817039749338241\n",
      "Epoch 1/5\n",
      "469/469 - 7s - 15ms/step - acc: 0.9321 - loss: 0.2363\n",
      "Epoch 2/5\n",
      "469/469 - 6s - 13ms/step - acc: 0.9737 - loss: 0.0908\n",
      "Epoch 3/5\n",
      "469/469 - 6s - 14ms/step - acc: 0.9835 - loss: 0.0565\n",
      "Epoch 4/5\n",
      "469/469 - 6s - 13ms/step - acc: 0.9885 - loss: 0.0393\n",
      "Epoch 5/5\n",
      "469/469 - 6s - 13ms/step - acc: 0.9919 - loss: 0.0269\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9797 - loss: 0.0720\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9957849525010971\n",
      "Recall Score: 0.9957666666666667\n",
      "F1 Score 0.9957696382740575\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9823058521038227\n",
      "Recall Score: 0.9822\n",
      "F1 Score 0.982211585497043\n",
      "Epoch 1/5\n",
      "469/469 - 5s - 11ms/step - acc: 0.9298 - loss: 0.2382\n",
      "Epoch 2/5\n",
      "469/469 - 5s - 10ms/step - acc: 0.9731 - loss: 0.0908\n",
      "Epoch 3/5\n",
      "469/469 - 5s - 11ms/step - acc: 0.9829 - loss: 0.0583\n",
      "Epoch 4/5\n",
      "469/469 - 5s - 10ms/step - acc: 0.9880 - loss: 0.0409\n",
      "Epoch 5/5\n",
      "469/469 - 5s - 10ms/step - acc: 0.9918 - loss: 0.0283\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9743 - loss: 0.0840\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.993580983343827\n",
      "Recall Score: 0.99355\n",
      "F1 Score 0.993545590543315\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9801009967939297\n",
      "Recall Score: 0.9799\n",
      "F1 Score 0.9799025147820816\n",
      "Epoch 1/5\n",
      "235/235 - 5s - 21ms/step - acc: 0.9181 - loss: 0.2884\n",
      "Epoch 2/5\n",
      "235/235 - 4s - 16ms/step - acc: 0.9671 - loss: 0.1133\n",
      "Epoch 3/5\n",
      "235/235 - 4s - 16ms/step - acc: 0.9789 - loss: 0.0732\n",
      "Epoch 4/5\n",
      "235/235 - 4s - 16ms/step - acc: 0.9852 - loss: 0.0506\n",
      "Epoch 5/5\n",
      "235/235 - 3s - 15ms/step - acc: 0.9897 - loss: 0.0367\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9761 - loss: 0.0783\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9933935424002135\n",
      "Recall Score: 0.9933833333333333\n",
      "F1 Score 0.9933753596819896\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9791426278828113\n",
      "Recall Score: 0.9791\n",
      "F1 Score 0.9790750833558124\n",
      "Epoch 1/5\n",
      "235/235 - 3s - 14ms/step - acc: 0.9123 - loss: 0.3013\n",
      "Epoch 2/5\n",
      "235/235 - 3s - 12ms/step - acc: 0.9654 - loss: 0.1189\n",
      "Epoch 3/5\n",
      "235/235 - 3s - 13ms/step - acc: 0.9776 - loss: 0.0774\n",
      "Epoch 4/5\n",
      "235/235 - 3s - 12ms/step - acc: 0.9840 - loss: 0.0553\n",
      "Epoch 5/5\n",
      "235/235 - 3s - 12ms/step - acc: 0.9881 - loss: 0.0409\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9703 - loss: 0.0922\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9865276427260339\n",
      "Recall Score: 0.9858666666666667\n",
      "F1 Score 0.9859504050352996\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9747449749484056\n",
      "Recall Score: 0.9739\n",
      "F1 Score 0.9739663802445111\n",
      "Epoch 1/10\n",
      "1875/1875 - 20s - 11ms/step - acc: 0.9447 - loss: 0.1866\n",
      "Epoch 2/10\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9772 - loss: 0.0746\n",
      "Epoch 3/10\n",
      "1875/1875 - 20s - 11ms/step - acc: 0.9840 - loss: 0.0497\n",
      "Epoch 4/10\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9889 - loss: 0.0345\n",
      "Epoch 5/10\n",
      "1875/1875 - 22s - 12ms/step - acc: 0.9912 - loss: 0.0267\n",
      "Epoch 6/10\n",
      "1875/1875 - 22s - 12ms/step - acc: 0.9934 - loss: 0.0206\n",
      "Epoch 7/10\n",
      "1875/1875 - 26s - 14ms/step - acc: 0.9936 - loss: 0.0186\n",
      "Epoch 8/10\n",
      "1875/1875 - 36s - 19ms/step - acc: 0.9948 - loss: 0.0150\n",
      "Epoch 9/10\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9955 - loss: 0.0125\n",
      "Epoch 10/10\n",
      "1875/1875 - 18s - 9ms/step - acc: 0.9960 - loss: 0.0120\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9786 - loss: 0.1126\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9966809256370497\n",
      "Recall Score: 0.9966666666666667\n",
      "F1 Score 0.9966644792999203\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9815997016560152\n",
      "Recall Score: 0.9815\n",
      "F1 Score 0.9814936078812172\n",
      "Epoch 1/10\n",
      "1875/1875 - 18s - 9ms/step - acc: 0.9434 - loss: 0.1897\n",
      "Epoch 2/10\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9768 - loss: 0.0786\n",
      "Epoch 3/10\n",
      "1875/1875 - 20s - 10ms/step - acc: 0.9848 - loss: 0.0526\n",
      "Epoch 4/10\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9881 - loss: 0.0388\n",
      "Epoch 5/10\n",
      "1875/1875 - 16s - 9ms/step - acc: 0.9915 - loss: 0.0282\n",
      "Epoch 6/10\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9937 - loss: 0.0209\n",
      "Epoch 7/10\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9952 - loss: 0.0162\n",
      "Epoch 8/10\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9968 - loss: 0.0114\n",
      "Epoch 9/10\n",
      "1875/1875 - 14s - 7ms/step - acc: 0.9977 - loss: 0.0077\n",
      "Epoch 10/10\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9983 - loss: 0.0057\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9782 - loss: 0.0993\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9984045949466798\n",
      "Recall Score: 0.9984\n",
      "F1 Score 0.9983990662088451\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9817600193164799\n",
      "Recall Score: 0.9817\n",
      "F1 Score 0.9816975676002992\n",
      "Epoch 1/10\n",
      "938/938 - 11s - 12ms/step - acc: 0.9400 - loss: 0.2033\n",
      "Epoch 2/10\n",
      "938/938 - 9s - 10ms/step - acc: 0.9758 - loss: 0.0784\n",
      "Epoch 3/10\n",
      "938/938 - 9s - 10ms/step - acc: 0.9844 - loss: 0.0488\n",
      "Epoch 4/10\n",
      "938/938 - 10s - 10ms/step - acc: 0.9888 - loss: 0.0348\n",
      "Epoch 5/10\n",
      "938/938 - 9s - 10ms/step - acc: 0.9922 - loss: 0.0245\n",
      "Epoch 6/10\n",
      "938/938 - 10s - 10ms/step - acc: 0.9941 - loss: 0.0187\n",
      "Epoch 7/10\n",
      "938/938 - 9s - 10ms/step - acc: 0.9955 - loss: 0.0141\n",
      "Epoch 8/10\n",
      "938/938 - 10s - 10ms/step - acc: 0.9958 - loss: 0.0133\n",
      "Epoch 9/10\n",
      "938/938 - 9s - 10ms/step - acc: 0.9968 - loss: 0.0100\n",
      "Epoch 10/10\n",
      "938/938 - 10s - 10ms/step - acc: 0.9964 - loss: 0.0112\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9770 - loss: 0.0960\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9977048147687352\n",
      "Recall Score: 0.9977\n",
      "F1 Score 0.9976993303382972\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9801128248615034\n",
      "Recall Score: 0.98\n",
      "F1 Score 0.9800073791729031\n",
      "Epoch 1/10\n",
      "938/938 - 8s - 9ms/step - acc: 0.9378 - loss: 0.2085\n",
      "Epoch 2/10\n",
      "938/938 - 7s - 8ms/step - acc: 0.9756 - loss: 0.0809\n",
      "Epoch 3/10\n",
      "938/938 - 7s - 8ms/step - acc: 0.9843 - loss: 0.0522\n",
      "Epoch 4/10\n",
      "938/938 - 7s - 8ms/step - acc: 0.9878 - loss: 0.0375\n",
      "Epoch 5/10\n",
      "938/938 - 7s - 8ms/step - acc: 0.9916 - loss: 0.0266\n",
      "Epoch 6/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9941 - loss: 0.0191\n",
      "Epoch 7/10\n",
      "938/938 - 7s - 8ms/step - acc: 0.9956 - loss: 0.0139\n",
      "Epoch 8/10\n",
      "938/938 - 7s - 8ms/step - acc: 0.9969 - loss: 0.0098\n",
      "Epoch 9/10\n",
      "938/938 - 8s - 8ms/step - acc: 0.9981 - loss: 0.0067\n",
      "Epoch 10/10\n",
      "938/938 - 8s - 9ms/step - acc: 0.9988 - loss: 0.0046\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9791 - loss: 0.0861\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9994339660375348\n",
      "Recall Score: 0.9994333333333333\n",
      "F1 Score 0.9994334000612722\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9830233881625502\n",
      "Recall Score: 0.983\n",
      "F1 Score 0.9830025773379343\n",
      "Epoch 1/10\n",
      "469/469 - 7s - 15ms/step - acc: 0.9313 - loss: 0.2355\n",
      "Epoch 2/10\n",
      "469/469 - 7s - 14ms/step - acc: 0.9734 - loss: 0.0900\n",
      "Epoch 3/10\n",
      "469/469 - 6s - 13ms/step - acc: 0.9828 - loss: 0.0564\n",
      "Epoch 4/10\n",
      "469/469 - 8s - 16ms/step - acc: 0.9882 - loss: 0.0392\n",
      "Epoch 5/10\n",
      "469/469 - 9s - 19ms/step - acc: 0.9921 - loss: 0.0270\n",
      "Epoch 6/10\n",
      "469/469 - 9s - 20ms/step - acc: 0.9947 - loss: 0.0194\n",
      "Epoch 7/10\n",
      "469/469 - 6s - 12ms/step - acc: 0.9959 - loss: 0.0149\n",
      "Epoch 8/10\n",
      "469/469 - 6s - 13ms/step - acc: 0.9959 - loss: 0.0135\n",
      "Epoch 9/10\n",
      "469/469 - 6s - 13ms/step - acc: 0.9983 - loss: 0.0073\n",
      "Epoch 10/10\n",
      "469/469 - 6s - 12ms/step - acc: 0.9980 - loss: 0.0075\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9766 - loss: 0.0840\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9989177147860585\n",
      "Recall Score: 0.9989166666666667\n",
      "F1 Score 0.9989167987243238\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9815514450153543\n",
      "Recall Score: 0.9815\n",
      "F1 Score 0.9814988709811554\n",
      "Epoch 1/10\n",
      "469/469 - 5s - 12ms/step - acc: 0.9299 - loss: 0.2405\n",
      "Epoch 2/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9728 - loss: 0.0917\n",
      "Epoch 3/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9822 - loss: 0.0594\n",
      "Epoch 4/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9874 - loss: 0.0410\n",
      "Epoch 5/10\n",
      "469/469 - 5s - 11ms/step - acc: 0.9913 - loss: 0.0289\n",
      "Epoch 6/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9939 - loss: 0.0210\n",
      "Epoch 7/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9959 - loss: 0.0141\n",
      "Epoch 8/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9969 - loss: 0.0111\n",
      "Epoch 9/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9983 - loss: 0.0074\n",
      "Epoch 10/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9988 - loss: 0.0050\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9807 - loss: 0.0772\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.999550562354621\n",
      "Recall Score: 0.99955\n",
      "F1 Score 0.9995500440548024\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9834387470506445\n",
      "Recall Score: 0.9834\n",
      "F1 Score 0.9833873700494052\n",
      "Epoch 1/10\n",
      "235/235 - 4s - 18ms/step - acc: 0.9188 - loss: 0.2851\n",
      "Epoch 2/10\n",
      "235/235 - 3s - 14ms/step - acc: 0.9677 - loss: 0.1121\n",
      "Epoch 3/10\n",
      "235/235 - 3s - 14ms/step - acc: 0.9793 - loss: 0.0729\n",
      "Epoch 4/10\n",
      "235/235 - 3s - 14ms/step - acc: 0.9849 - loss: 0.0521\n",
      "Epoch 5/10\n",
      "235/235 - 3s - 14ms/step - acc: 0.9898 - loss: 0.0369\n",
      "Epoch 6/10\n",
      "235/235 - 3s - 14ms/step - acc: 0.9922 - loss: 0.0290\n",
      "Epoch 7/10\n",
      "235/235 - 5s - 22ms/step - acc: 0.9950 - loss: 0.0202\n",
      "Epoch 8/10\n",
      "235/235 - 3s - 14ms/step - acc: 0.9962 - loss: 0.0155\n",
      "Epoch 9/10\n",
      "235/235 - 3s - 15ms/step - acc: 0.9979 - loss: 0.0106\n",
      "Epoch 10/10\n",
      "235/235 - 5s - 21ms/step - acc: 0.9988 - loss: 0.0075\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9802 - loss: 0.0671\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.99925051940824\n",
      "Recall Score: 0.99925\n",
      "F1 Score 0.9992500405543547\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9837406157194679\n",
      "Recall Score: 0.9837\n",
      "F1 Score 0.9837016500673451\n",
      "Epoch 1/10\n",
      "235/235 - 4s - 16ms/step - acc: 0.9124 - loss: 0.3007\n",
      "Epoch 2/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9646 - loss: 0.1190\n",
      "Epoch 3/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9778 - loss: 0.0771\n",
      "Epoch 4/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9839 - loss: 0.0549\n",
      "Epoch 5/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9883 - loss: 0.0411\n",
      "Epoch 6/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9909 - loss: 0.0313\n",
      "Epoch 7/10\n",
      "235/235 - 3s - 12ms/step - acc: 0.9936 - loss: 0.0236\n",
      "Epoch 8/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9952 - loss: 0.0179\n",
      "Epoch 9/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9968 - loss: 0.0138\n",
      "Epoch 10/10\n",
      "235/235 - 3s - 12ms/step - acc: 0.9981 - loss: 0.0100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9791 - loss: 0.0651\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9991839124247908\n",
      "Recall Score: 0.9991833333333333\n",
      "F1 Score 0.9991833699797031\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9825123893423302\n",
      "Recall Score: 0.9825\n",
      "F1 Score 0.9824976249884112\n",
      "Epoch 1/20\n",
      "1875/1875 - 18s - 10ms/step - acc: 0.9441 - loss: 0.1851\n",
      "Epoch 2/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9765 - loss: 0.0754\n",
      "Epoch 3/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9848 - loss: 0.0481\n",
      "Epoch 4/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9887 - loss: 0.0345\n",
      "Epoch 5/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9913 - loss: 0.0259\n",
      "Epoch 6/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9933 - loss: 0.0216\n",
      "Epoch 7/20\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9942 - loss: 0.0177\n",
      "Epoch 8/20\n",
      "1875/1875 - 18s - 9ms/step - acc: 0.9955 - loss: 0.0140\n",
      "Epoch 9/20\n",
      "1875/1875 - 20s - 11ms/step - acc: 0.9953 - loss: 0.0137\n",
      "Epoch 10/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9962 - loss: 0.0120\n",
      "Epoch 11/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9958 - loss: 0.0117\n",
      "Epoch 12/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9968 - loss: 0.0101\n",
      "Epoch 13/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9970 - loss: 0.0084\n",
      "Epoch 14/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9975 - loss: 0.0088\n",
      "Epoch 15/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9979 - loss: 0.0065\n",
      "Epoch 16/20\n",
      "1875/1875 - 18s - 10ms/step - acc: 0.9969 - loss: 0.0100\n",
      "Epoch 17/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9970 - loss: 0.0100\n",
      "Epoch 18/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9982 - loss: 0.0061\n",
      "Epoch 19/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9979 - loss: 0.0069\n",
      "Epoch 20/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9977 - loss: 0.0081\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9752 - loss: 0.1665\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9978212333146476\n",
      "Recall Score: 0.9978166666666667\n",
      "F1 Score 0.9978162735535113\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9800597165551662\n",
      "Recall Score: 0.98\n",
      "F1 Score 0.9799947828216405\n",
      "Epoch 1/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9444 - loss: 0.1873\n",
      "Epoch 2/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9765 - loss: 0.0797\n",
      "Epoch 3/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9837 - loss: 0.0531\n",
      "Epoch 4/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9881 - loss: 0.0395\n",
      "Epoch 5/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9908 - loss: 0.0297\n",
      "Epoch 6/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9937 - loss: 0.0212\n",
      "Epoch 7/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9953 - loss: 0.0160\n",
      "Epoch 8/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9965 - loss: 0.0119\n",
      "Epoch 9/20\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9976 - loss: 0.0082\n",
      "Epoch 10/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9984 - loss: 0.0055\n",
      "Epoch 11/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9989 - loss: 0.0040\n",
      "Epoch 12/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9995 - loss: 0.0022\n",
      "Epoch 13/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9998 - loss: 0.0011\n",
      "Epoch 14/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9999 - loss: 4.2084e-04\n",
      "Epoch 15/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9999 - loss: 2.8785e-04\n",
      "Epoch 16/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 1.0000 - loss: 1.3522e-04\n",
      "Epoch 17/20\n",
      "1875/1875 - 21s - 11ms/step - acc: 1.0000 - loss: 3.9532e-05\n",
      "Epoch 18/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 1.0000 - loss: 3.6392e-05\n",
      "Epoch 19/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 1.0000 - loss: 2.7249e-05\n",
      "Epoch 20/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 1.0000 - loss: 2.4059e-05\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9821 - loss: 0.0969\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9846098283490803\n",
      "Recall Score: 0.9846\n",
      "F1 Score 0.9846008465990969\n",
      "Epoch 1/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9400 - loss: 0.2030\n",
      "Epoch 2/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9769 - loss: 0.0777\n",
      "Epoch 3/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9842 - loss: 0.0512\n",
      "Epoch 4/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9895 - loss: 0.0328\n",
      "Epoch 5/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9922 - loss: 0.0241\n",
      "Epoch 6/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9940 - loss: 0.0196\n",
      "Epoch 7/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9952 - loss: 0.0155\n",
      "Epoch 8/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9957 - loss: 0.0123\n",
      "Epoch 9/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9971 - loss: 0.0092\n",
      "Epoch 10/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9961 - loss: 0.0120\n",
      "Epoch 11/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9973 - loss: 0.0081\n",
      "Epoch 12/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9973 - loss: 0.0079\n",
      "Epoch 13/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9978 - loss: 0.0067\n",
      "Epoch 14/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9981 - loss: 0.0056\n",
      "Epoch 15/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9981 - loss: 0.0056\n",
      "Epoch 16/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9979 - loss: 0.0063\n",
      "Epoch 17/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9983 - loss: 0.0057\n",
      "Epoch 18/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9983 - loss: 0.0046\n",
      "Epoch 19/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9990 - loss: 0.0042\n",
      "Epoch 20/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9973 - loss: 0.0087\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9778 - loss: 0.1221\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9974386405800958\n",
      "Recall Score: 0.9974333333333333\n",
      "F1 Score 0.9974331300144815\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9807374848788047\n",
      "Recall Score: 0.9807\n",
      "F1 Score 0.9806872050152297\n",
      "Epoch 1/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9386 - loss: 0.2059\n",
      "Epoch 2/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9755 - loss: 0.0793\n",
      "Epoch 3/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9842 - loss: 0.0518\n",
      "Epoch 4/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9887 - loss: 0.0371\n",
      "Epoch 5/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9917 - loss: 0.0271\n",
      "Epoch 6/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9944 - loss: 0.0185\n",
      "Epoch 7/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9960 - loss: 0.0138\n",
      "Epoch 8/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9974 - loss: 0.0095\n",
      "Epoch 9/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9984 - loss: 0.0064\n",
      "Epoch 10/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9990 - loss: 0.0044\n",
      "Epoch 11/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9993 - loss: 0.0028\n",
      "Epoch 12/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9997 - loss: 0.0013\n",
      "Epoch 13/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9999 - loss: 6.7325e-04\n",
      "Epoch 14/20\n",
      "938/938 - 7s - 8ms/step - acc: 1.0000 - loss: 4.0451e-04\n",
      "Epoch 15/20\n",
      "938/938 - 7s - 8ms/step - acc: 1.0000 - loss: 1.8882e-04\n",
      "Epoch 16/20\n",
      "938/938 - 7s - 8ms/step - acc: 1.0000 - loss: 1.1260e-04\n",
      "Epoch 17/20\n",
      "938/938 - 7s - 8ms/step - acc: 1.0000 - loss: 9.8414e-05\n",
      "Epoch 18/20\n",
      "938/938 - 7s - 8ms/step - acc: 1.0000 - loss: 8.5586e-05\n",
      "Epoch 19/20\n",
      "938/938 - 7s - 8ms/step - acc: 1.0000 - loss: 7.5936e-05\n",
      "Epoch 20/20\n",
      "938/938 - 7s - 8ms/step - acc: 1.0000 - loss: 7.1288e-05\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9820 - loss: 0.0871\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9850149846470151\n",
      "Recall Score: 0.985\n",
      "F1 Score 0.9850027420047905\n",
      "Epoch 1/20\n",
      "469/469 - 7s - 14ms/step - acc: 0.9334 - loss: 0.2317\n",
      "Epoch 2/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9735 - loss: 0.0908\n",
      "Epoch 3/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9827 - loss: 0.0574\n",
      "Epoch 4/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9882 - loss: 0.0384\n",
      "Epoch 5/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9920 - loss: 0.0268\n",
      "Epoch 6/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9941 - loss: 0.0202\n",
      "Epoch 7/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9957 - loss: 0.0152\n",
      "Epoch 8/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9970 - loss: 0.0112\n",
      "Epoch 9/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9975 - loss: 0.0089\n",
      "Epoch 10/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9983 - loss: 0.0067\n",
      "Epoch 11/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9977 - loss: 0.0080\n",
      "Epoch 12/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9975 - loss: 0.0087\n",
      "Epoch 13/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9970 - loss: 0.0092\n",
      "Epoch 14/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9987 - loss: 0.0046\n",
      "Epoch 15/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9985 - loss: 0.0043\n",
      "Epoch 16/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9984 - loss: 0.0056\n",
      "Epoch 17/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9998 - loss: 0.0013\n",
      "Epoch 18/20\n",
      "469/469 - 7s - 14ms/step - acc: 0.9999 - loss: 5.4888e-04\n",
      "Epoch 19/20\n",
      "469/469 - 5s - 12ms/step - acc: 1.0000 - loss: 1.6193e-04\n",
      "Epoch 20/20\n",
      "469/469 - 5s - 11ms/step - acc: 1.0000 - loss: 1.0709e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9820 - loss: 0.0815\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9863111393395935\n",
      "Recall Score: 0.9863\n",
      "F1 Score 0.9863018174929994\n",
      "Epoch 1/20\n",
      "469/469 - 6s - 14ms/step - acc: 0.9286 - loss: 0.2386\n",
      "Epoch 2/20\n",
      "469/469 - 4s - 10ms/step - acc: 0.9724 - loss: 0.0904\n",
      "Epoch 3/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9818 - loss: 0.0583\n",
      "Epoch 4/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9874 - loss: 0.0410\n",
      "Epoch 5/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9915 - loss: 0.0292\n",
      "Epoch 6/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9934 - loss: 0.0218\n",
      "Epoch 7/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9961 - loss: 0.0148\n",
      "Epoch 8/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9974 - loss: 0.0103\n",
      "Epoch 9/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9983 - loss: 0.0075\n",
      "Epoch 10/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9990 - loss: 0.0048\n",
      "Epoch 11/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9993 - loss: 0.0034\n",
      "Epoch 12/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9998 - loss: 0.0018\n",
      "Epoch 13/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9999 - loss: 0.0010\n",
      "Epoch 14/20\n",
      "469/469 - 4s - 9ms/step - acc: 1.0000 - loss: 6.1247e-04\n",
      "Epoch 15/20\n",
      "469/469 - 4s - 9ms/step - acc: 1.0000 - loss: 4.4570e-04\n",
      "Epoch 16/20\n",
      "469/469 - 4s - 9ms/step - acc: 1.0000 - loss: 3.3745e-04\n",
      "Epoch 17/20\n",
      "469/469 - 5s - 11ms/step - acc: 1.0000 - loss: 2.9508e-04\n",
      "Epoch 18/20\n",
      "469/469 - 4s - 9ms/step - acc: 1.0000 - loss: 2.6109e-04\n",
      "Epoch 19/20\n",
      "469/469 - 4s - 9ms/step - acc: 1.0000 - loss: 2.3882e-04\n",
      "Epoch 20/20\n",
      "469/469 - 4s - 9ms/step - acc: 1.0000 - loss: 2.1704e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9829 - loss: 0.0754\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9852132047723157\n",
      "Recall Score: 0.9852\n",
      "F1 Score 0.9852003786533089\n",
      "Epoch 1/20\n",
      "235/235 - 5s - 21ms/step - acc: 0.9208 - loss: 0.2831\n",
      "Epoch 2/20\n",
      "235/235 - 4s - 17ms/step - acc: 0.9679 - loss: 0.1133\n",
      "Epoch 3/20\n",
      "235/235 - 3s - 15ms/step - acc: 0.9785 - loss: 0.0735\n",
      "Epoch 4/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9845 - loss: 0.0516\n",
      "Epoch 5/20\n",
      "235/235 - 5s - 22ms/step - acc: 0.9897 - loss: 0.0368\n",
      "Epoch 6/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9931 - loss: 0.0270\n",
      "Epoch 7/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9949 - loss: 0.0206\n",
      "Epoch 8/20\n",
      "235/235 - 5s - 22ms/step - acc: 0.9959 - loss: 0.0159\n",
      "Epoch 9/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9978 - loss: 0.0114\n",
      "Epoch 10/20\n",
      "235/235 - 5s - 22ms/step - acc: 0.9982 - loss: 0.0090\n",
      "Epoch 11/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9989 - loss: 0.0064\n",
      "Epoch 12/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9992 - loss: 0.0055\n",
      "Epoch 13/20\n",
      "235/235 - 3s - 15ms/step - acc: 0.9995 - loss: 0.0040\n",
      "Epoch 14/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9996 - loss: 0.0032\n",
      "Epoch 15/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9999 - loss: 0.0018\n",
      "Epoch 16/20\n",
      "235/235 - 3s - 14ms/step - acc: 1.0000 - loss: 0.0012\n",
      "Epoch 17/20\n",
      "235/235 - 3s - 14ms/step - acc: 1.0000 - loss: 9.1997e-04\n",
      "Epoch 18/20\n",
      "235/235 - 3s - 14ms/step - acc: 1.0000 - loss: 8.4111e-04\n",
      "Epoch 19/20\n",
      "235/235 - 3s - 14ms/step - acc: 1.0000 - loss: 8.0693e-04\n",
      "Epoch 20/20\n",
      "235/235 - 5s - 22ms/step - acc: 1.0000 - loss: 5.5470e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9816 - loss: 0.0742\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9841046933521995\n",
      "Recall Score: 0.9841\n",
      "F1 Score 0.9841000213540528\n",
      "Epoch 1/20\n",
      "235/235 - 4s - 18ms/step - acc: 0.9108 - loss: 0.3016\n",
      "Epoch 2/20\n",
      "235/235 - 3s - 15ms/step - acc: 0.9651 - loss: 0.1185\n",
      "Epoch 3/20\n",
      "235/235 - 3s - 12ms/step - acc: 0.9770 - loss: 0.0769\n",
      "Epoch 4/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9837 - loss: 0.0560\n",
      "Epoch 5/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9873 - loss: 0.0420\n",
      "Epoch 6/20\n",
      "235/235 - 5s - 22ms/step - acc: 0.9908 - loss: 0.0308\n",
      "Epoch 7/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9936 - loss: 0.0233\n",
      "Epoch 8/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9957 - loss: 0.0175\n",
      "Epoch 9/20\n",
      "235/235 - 3s - 12ms/step - acc: 0.9969 - loss: 0.0132\n",
      "Epoch 10/20\n",
      "235/235 - 3s - 13ms/step - acc: 0.9979 - loss: 0.0097\n",
      "Epoch 11/20\n",
      "235/235 - 3s - 13ms/step - acc: 0.9987 - loss: 0.0070\n",
      "Epoch 12/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9993 - loss: 0.0052\n",
      "Epoch 13/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9996 - loss: 0.0037\n",
      "Epoch 14/20\n",
      "235/235 - 3s - 12ms/step - acc: 0.9998 - loss: 0.0025\n",
      "Epoch 15/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9999 - loss: 0.0018\n",
      "Epoch 16/20\n",
      "235/235 - 3s - 12ms/step - acc: 0.9999 - loss: 0.0015\n",
      "Epoch 17/20\n",
      "235/235 - 3s - 11ms/step - acc: 1.0000 - loss: 0.0012\n",
      "Epoch 18/20\n",
      "235/235 - 3s - 11ms/step - acc: 1.0000 - loss: 9.7151e-04\n",
      "Epoch 19/20\n",
      "235/235 - 3s - 12ms/step - acc: 1.0000 - loss: 8.1019e-04\n",
      "Epoch 20/20\n",
      "235/235 - 3s - 11ms/step - acc: 1.0000 - loss: 7.3310e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9803 - loss: 0.0735\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層1, 節點1024, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9835207876107155\n",
      "Recall Score: 0.9835\n",
      "F1 Score 0.9835034110547218\n",
      "==================================================\n",
      "Completed batch for layer 1\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9208 - loss: 0.2746\n",
      "Epoch 2/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9636 - loss: 0.1234\n",
      "Epoch 3/5\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9728 - loss: 0.0901\n",
      "Epoch 4/5\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9779 - loss: 0.0702\n",
      "Epoch 5/5\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9815 - loss: 0.0585\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9703 - loss: 0.1014\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.985351284375898\n",
      "Recall Score: 0.9852833333333333\n",
      "F1 Score 0.9852910561840799\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9752595027825975\n",
      "Recall Score: 0.9752\n",
      "F1 Score 0.975196234006008\n",
      "Epoch 1/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9159 - loss: 0.2879\n",
      "Epoch 2/5\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9601 - loss: 0.1362\n",
      "Epoch 3/5\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9692 - loss: 0.1006\n",
      "Epoch 4/5\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9761 - loss: 0.0812\n",
      "Epoch 5/5\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9793 - loss: 0.0690\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9691 - loss: 0.1063\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9842801927014743\n",
      "Recall Score: 0.9841166666666666\n",
      "F1 Score 0.9841390944725529\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.972820816684749\n",
      "Recall Score: 0.9726\n",
      "F1 Score 0.9726446436499595\n",
      "Epoch 1/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9067 - loss: 0.3217\n",
      "Epoch 2/5\n",
      "938/938 - 1s - 2ms/step - acc: 0.9580 - loss: 0.1419\n",
      "Epoch 3/5\n",
      "938/938 - 1s - 2ms/step - acc: 0.9683 - loss: 0.1036\n",
      "Epoch 4/5\n",
      "938/938 - 1s - 1ms/step - acc: 0.9755 - loss: 0.0818\n",
      "Epoch 5/5\n",
      "938/938 - 1s - 1ms/step - acc: 0.9786 - loss: 0.0686\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9669 - loss: 0.0991\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9835902294894856\n",
      "Recall Score: 0.9834333333333334\n",
      "F1 Score 0.9834391305742592\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9725850555938148\n",
      "Recall Score: 0.9724\n",
      "F1 Score 0.9724307800124327\n",
      "Epoch 1/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9122 - loss: 0.3054\n",
      "Epoch 2/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9585 - loss: 0.1385\n",
      "Epoch 3/5\n",
      "938/938 - 1s - 2ms/step - acc: 0.9699 - loss: 0.0986\n",
      "Epoch 4/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9766 - loss: 0.0783\n",
      "Epoch 5/5\n",
      "938/938 - 1s - 1ms/step - acc: 0.9804 - loss: 0.0648\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9693 - loss: 0.0994\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.98558898321895\n",
      "Recall Score: 0.98555\n",
      "F1 Score 0.9855521328639979\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9740823725619379\n",
      "Recall Score: 0.974\n",
      "F1 Score 0.9740034803528704\n",
      "Epoch 1/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.8951 - loss: 0.3804\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9516 - loss: 0.1657\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9632 - loss: 0.1238\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9701 - loss: 0.0987\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9751 - loss: 0.0825\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9655 - loss: 0.1155\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9800052229740223\n",
      "Recall Score: 0.9799\n",
      "F1 Score 0.979899969421461\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9700864246535595\n",
      "Recall Score: 0.9697\n",
      "F1 Score 0.9697061061606553\n",
      "Epoch 1/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.8995 - loss: 0.3673\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9510 - loss: 0.1663\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9637 - loss: 0.1222\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9706 - loss: 0.0975\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9753 - loss: 0.0809\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9672 - loss: 0.1025\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9805956109010134\n",
      "Recall Score: 0.9805166666666667\n",
      "F1 Score 0.9805042180788602\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9723185501122215\n",
      "Recall Score: 0.9722\n",
      "F1 Score 0.9721806968680785\n",
      "Epoch 1/5\n",
      "235/235 - 2s - 7ms/step - acc: 0.8510 - loss: 0.5180\n",
      "Epoch 2/5\n",
      "235/235 - 0s - 2ms/step - acc: 0.9447 - loss: 0.1943\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9576 - loss: 0.1455\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9657 - loss: 0.1183\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9709 - loss: 0.0995\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9613 - loss: 0.1179\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9767071235017143\n",
      "Recall Score: 0.97665\n",
      "F1 Score 0.9766565876043023\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.966189356157122\n",
      "Recall Score: 0.9661\n",
      "F1 Score 0.9660891751991696\n",
      "Epoch 1/5\n",
      "235/235 - 1s - 6ms/step - acc: 0.8830 - loss: 0.4341\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9419 - loss: 0.1962\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9570 - loss: 0.1450\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9653 - loss: 0.1181\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9700 - loss: 0.1003\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9626 - loss: 0.1234\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9752430604445353\n",
      "Recall Score: 0.9751833333333333\n",
      "F1 Score 0.9751371606843336\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9670886736430002\n",
      "Recall Score: 0.9669\n",
      "F1 Score 0.9668311264049556\n",
      "Epoch 1/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9162 - loss: 0.2862\n",
      "Epoch 2/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9621 - loss: 0.1244\n",
      "Epoch 3/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9722 - loss: 0.0908\n",
      "Epoch 4/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9773 - loss: 0.0723\n",
      "Epoch 5/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9814 - loss: 0.0584\n",
      "Epoch 6/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9845 - loss: 0.0487\n",
      "Epoch 7/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9865 - loss: 0.0416\n",
      "Epoch 8/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9878 - loss: 0.0372\n",
      "Epoch 9/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9902 - loss: 0.0305\n",
      "Epoch 10/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9899 - loss: 0.0289\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9702 - loss: 0.1140\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9923525824009133\n",
      "Recall Score: 0.9923\n",
      "F1 Score 0.9922974436042411\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9739415623242892\n",
      "Recall Score: 0.9738\n",
      "F1 Score 0.9738056551774367\n",
      "Epoch 1/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9179 - loss: 0.2813\n",
      "Epoch 2/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9601 - loss: 0.1347\n",
      "Epoch 3/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9704 - loss: 0.1031\n",
      "Epoch 4/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9758 - loss: 0.0842\n",
      "Epoch 5/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9786 - loss: 0.0723\n",
      "Epoch 6/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9815 - loss: 0.0632\n",
      "Epoch 7/10\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9836 - loss: 0.0562\n",
      "Epoch 8/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9852 - loss: 0.0497\n",
      "Epoch 9/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9863 - loss: 0.0455\n",
      "Epoch 10/10\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9880 - loss: 0.0409\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9699 - loss: 0.1279\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9869748330324504\n",
      "Recall Score: 0.9868333333333333\n",
      "F1 Score 0.9868250511927094\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9737070274670127\n",
      "Recall Score: 0.9734\n",
      "F1 Score 0.9733972209834157\n",
      "Epoch 1/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9060 - loss: 0.3309\n",
      "Epoch 2/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9573 - loss: 0.1474\n",
      "Epoch 3/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9675 - loss: 0.1097\n",
      "Epoch 4/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9728 - loss: 0.0882\n",
      "Epoch 5/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9770 - loss: 0.0733\n",
      "Epoch 6/10\n",
      "938/938 - 1s - 2ms/step - acc: 0.9811 - loss: 0.0604\n",
      "Epoch 7/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9833 - loss: 0.0524\n",
      "Epoch 8/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9853 - loss: 0.0458\n",
      "Epoch 9/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9870 - loss: 0.0404\n",
      "Epoch 10/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9890 - loss: 0.0347\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9692 - loss: 0.1100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9904014725587469\n",
      "Recall Score: 0.9903333333333333\n",
      "F1 Score 0.9903270844767008\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9739208346618315\n",
      "Recall Score: 0.9738\n",
      "F1 Score 0.9738051971577238\n",
      "Epoch 1/10\n",
      "938/938 - 2s - 3ms/step - acc: 0.9075 - loss: 0.3202\n",
      "Epoch 2/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9548 - loss: 0.1509\n",
      "Epoch 3/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9671 - loss: 0.1081\n",
      "Epoch 4/10\n",
      "938/938 - 1s - 2ms/step - acc: 0.9738 - loss: 0.0858\n",
      "Epoch 5/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9789 - loss: 0.0716\n",
      "Epoch 6/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9810 - loss: 0.0619\n",
      "Epoch 7/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9842 - loss: 0.0538\n",
      "Epoch 8/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9855 - loss: 0.0475\n",
      "Epoch 9/10\n",
      "938/938 - 1s - 2ms/step - acc: 0.9874 - loss: 0.0414\n",
      "Epoch 10/10\n",
      "938/938 - 1s - 1ms/step - acc: 0.9879 - loss: 0.0375\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9738 - loss: 0.1046\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9915446141493057\n",
      "Recall Score: 0.9915166666666667\n",
      "F1 Score 0.9915134214709056\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9763817323314412\n",
      "Recall Score: 0.9763\n",
      "F1 Score 0.9762742887895711\n",
      "Epoch 1/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.8886 - loss: 0.3946\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9491 - loss: 0.1750\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9627 - loss: 0.1265\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9705 - loss: 0.1011\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9750 - loss: 0.0847\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9785 - loss: 0.0723\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9810 - loss: 0.0619\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9831 - loss: 0.0558\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9853 - loss: 0.0483\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9872 - loss: 0.0417\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9695 - loss: 0.1098\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9906777030384183\n",
      "Recall Score: 0.9906333333333334\n",
      "F1 Score 0.9906375137286918\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9738086354783975\n",
      "Recall Score: 0.9737\n",
      "F1 Score 0.973692817036768\n",
      "Epoch 1/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.8950 - loss: 0.3741\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9513 - loss: 0.1662\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9637 - loss: 0.1210\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9709 - loss: 0.0963\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9757 - loss: 0.0795\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9791 - loss: 0.0676\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9816 - loss: 0.0593\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9839 - loss: 0.0517\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9859 - loss: 0.0455\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9875 - loss: 0.0407\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9756 - loss: 0.0853\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9918911435276843\n",
      "Recall Score: 0.9918666666666667\n",
      "F1 Score 0.991871356650434\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9784669409079546\n",
      "Recall Score: 0.9784\n",
      "F1 Score 0.9784062699629021\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 8ms/step - acc: 0.8588 - loss: 0.5003\n",
      "Epoch 2/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9429 - loss: 0.1981\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9569 - loss: 0.1497\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9644 - loss: 0.1223\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9698 - loss: 0.1029\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9736 - loss: 0.0896\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9767 - loss: 0.0777\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9791 - loss: 0.0688\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9813 - loss: 0.0624\n",
      "Epoch 10/10\n",
      "235/235 - 0s - 2ms/step - acc: 0.9836 - loss: 0.0541\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9644 - loss: 0.1070\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9847251825814161\n",
      "Recall Score: 0.9845833333333334\n",
      "F1 Score 0.9845755288698083\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9693566122619617\n",
      "Recall Score: 0.969\n",
      "F1 Score 0.9689843185718179\n",
      "Epoch 1/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.8808 - loss: 0.4431\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9408 - loss: 0.2060\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9552 - loss: 0.1546\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9628 - loss: 0.1239\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9693 - loss: 0.1032\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9736 - loss: 0.0886\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9769 - loss: 0.0772\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9796 - loss: 0.0684\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9815 - loss: 0.0612\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9837 - loss: 0.0548\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9572 - loss: 0.1422\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9790897216408996\n",
      "Recall Score: 0.9784333333333334\n",
      "F1 Score 0.9783792932592144\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9658953680268944\n",
      "Recall Score: 0.9646\n",
      "F1 Score 0.9646192134561543\n",
      "Epoch 1/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9168 - loss: 0.2867\n",
      "Epoch 2/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9609 - loss: 0.1299\n",
      "Epoch 3/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9713 - loss: 0.0960\n",
      "Epoch 4/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9765 - loss: 0.0754\n",
      "Epoch 5/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9803 - loss: 0.0624\n",
      "Epoch 6/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9841 - loss: 0.0518\n",
      "Epoch 7/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9854 - loss: 0.0437\n",
      "Epoch 8/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9879 - loss: 0.0383\n",
      "Epoch 9/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9891 - loss: 0.0344\n",
      "Epoch 10/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9906 - loss: 0.0288\n",
      "Epoch 11/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9911 - loss: 0.0260\n",
      "Epoch 12/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9916 - loss: 0.0245\n",
      "Epoch 13/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9930 - loss: 0.0213\n",
      "Epoch 14/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9936 - loss: 0.0193\n",
      "Epoch 15/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9937 - loss: 0.0179\n",
      "Epoch 16/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9946 - loss: 0.0162\n",
      "Epoch 17/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9951 - loss: 0.0139\n",
      "Epoch 18/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9946 - loss: 0.0155\n",
      "Epoch 19/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9951 - loss: 0.0145\n",
      "Epoch 20/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9959 - loss: 0.0122\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9702 - loss: 0.1623\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9981542052497664\n",
      "Recall Score: 0.99815\n",
      "F1 Score 0.9981504539078294\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9740360699099502\n",
      "Recall Score: 0.974\n",
      "F1 Score 0.9739972118472795\n",
      "Epoch 1/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9208 - loss: 0.2743\n",
      "Epoch 2/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9609 - loss: 0.1302\n",
      "Epoch 3/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9710 - loss: 0.0987\n",
      "Epoch 4/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9757 - loss: 0.0813\n",
      "Epoch 5/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9792 - loss: 0.0709\n",
      "Epoch 6/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9820 - loss: 0.0618\n",
      "Epoch 7/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9837 - loss: 0.0544\n",
      "Epoch 8/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9853 - loss: 0.0497\n",
      "Epoch 9/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9866 - loss: 0.0451\n",
      "Epoch 10/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9881 - loss: 0.0411\n",
      "Epoch 11/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9890 - loss: 0.0372\n",
      "Epoch 12/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9903 - loss: 0.0335\n",
      "Epoch 13/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9916 - loss: 0.0293\n",
      "Epoch 14/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9920 - loss: 0.0268\n",
      "Epoch 15/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9922 - loss: 0.0263\n",
      "Epoch 16/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9931 - loss: 0.0237\n",
      "Epoch 17/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9934 - loss: 0.0224\n",
      "Epoch 18/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9942 - loss: 0.0202\n",
      "Epoch 19/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9940 - loss: 0.0198\n",
      "Epoch 20/20\n",
      "1875/1875 - 2s - 1ms/step - acc: 0.9945 - loss: 0.0179\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9720 - loss: 0.1912\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9972407730101432\n",
      "Recall Score: 0.9972333333333333\n",
      "F1 Score 0.9972340568406748\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9775476152584839\n",
      "Recall Score: 0.9775\n",
      "F1 Score 0.9775027867742472\n",
      "Epoch 1/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9062 - loss: 0.3260\n",
      "Epoch 2/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9569 - loss: 0.1456\n",
      "Epoch 3/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9685 - loss: 0.1045\n",
      "Epoch 4/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9751 - loss: 0.0811\n",
      "Epoch 5/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9789 - loss: 0.0673\n",
      "Epoch 6/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9828 - loss: 0.0557\n",
      "Epoch 7/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9850 - loss: 0.0474\n",
      "Epoch 8/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9865 - loss: 0.0408\n",
      "Epoch 9/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9891 - loss: 0.0348\n",
      "Epoch 10/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9905 - loss: 0.0300\n",
      "Epoch 11/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9913 - loss: 0.0267\n",
      "Epoch 12/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9921 - loss: 0.0236\n",
      "Epoch 13/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9933 - loss: 0.0204\n",
      "Epoch 14/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9935 - loss: 0.0190\n",
      "Epoch 15/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9941 - loss: 0.0168\n",
      "Epoch 16/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9949 - loss: 0.0153\n",
      "Epoch 17/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9958 - loss: 0.0131\n",
      "Epoch 18/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9962 - loss: 0.0113\n",
      "Epoch 19/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9948 - loss: 0.0148\n",
      "Epoch 20/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9960 - loss: 0.0115\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9677 - loss: 0.1542\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9947049149421144\n",
      "Recall Score: 0.99465\n",
      "F1 Score 0.9946491258185343\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9733032621584553\n",
      "Recall Score: 0.973\n",
      "F1 Score 0.9729912087953034\n",
      "Epoch 1/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9124 - loss: 0.3084\n",
      "Epoch 2/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9584 - loss: 0.1412\n",
      "Epoch 3/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9690 - loss: 0.1037\n",
      "Epoch 4/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9747 - loss: 0.0848\n",
      "Epoch 5/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9783 - loss: 0.0703\n",
      "Epoch 6/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9813 - loss: 0.0612\n",
      "Epoch 7/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9841 - loss: 0.0527\n",
      "Epoch 8/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9857 - loss: 0.0465\n",
      "Epoch 9/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9867 - loss: 0.0419\n",
      "Epoch 10/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9882 - loss: 0.0371\n",
      "Epoch 11/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9900 - loss: 0.0329\n",
      "Epoch 12/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9910 - loss: 0.0295\n",
      "Epoch 13/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9919 - loss: 0.0260\n",
      "Epoch 14/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9930 - loss: 0.0233\n",
      "Epoch 15/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9934 - loss: 0.0207\n",
      "Epoch 16/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9939 - loss: 0.0194\n",
      "Epoch 17/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9946 - loss: 0.0177\n",
      "Epoch 18/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9951 - loss: 0.0151\n",
      "Epoch 19/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9954 - loss: 0.0148\n",
      "Epoch 20/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9960 - loss: 0.0133\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9652 - loss: 0.1647\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9963583520105026\n",
      "Recall Score: 0.9963166666666666\n",
      "F1 Score 0.9963240640565574\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.971901680213027\n",
      "Recall Score: 0.9716\n",
      "F1 Score 0.9716419296626305\n",
      "Epoch 1/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.8931 - loss: 0.3866\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9508 - loss: 0.1681\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9631 - loss: 0.1244\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9706 - loss: 0.0987\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9761 - loss: 0.0811\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9787 - loss: 0.0707\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9815 - loss: 0.0607\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9843 - loss: 0.0517\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9865 - loss: 0.0451\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9884 - loss: 0.0386\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9893 - loss: 0.0354\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9913 - loss: 0.0297\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9915 - loss: 0.0271\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9933 - loss: 0.0223\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9938 - loss: 0.0204\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9942 - loss: 0.0189\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9948 - loss: 0.0180\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9961 - loss: 0.0137\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9958 - loss: 0.0140\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9957 - loss: 0.0131\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9712 - loss: 0.1258\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9971379743710952\n",
      "Recall Score: 0.9971166666666667\n",
      "F1 Score 0.9971190926602322\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9753015169533424\n",
      "Recall Score: 0.9752\n",
      "F1 Score 0.9751991389543977\n",
      "Epoch 1/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.8980 - loss: 0.3668\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9503 - loss: 0.1716\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9627 - loss: 0.1267\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9699 - loss: 0.0999\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9755 - loss: 0.0825\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9790 - loss: 0.0705\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9815 - loss: 0.0612\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9840 - loss: 0.0528\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9851 - loss: 0.0485\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9875 - loss: 0.0418\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9887 - loss: 0.0377\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9902 - loss: 0.0332\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9914 - loss: 0.0297\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9918 - loss: 0.0262\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9926 - loss: 0.0238\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9938 - loss: 0.0214\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9943 - loss: 0.0191\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 1ms/step - acc: 0.9949 - loss: 0.0168\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9955 - loss: 0.0145\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9958 - loss: 0.0140\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9757 - loss: 0.1059\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9971230191322586\n",
      "Recall Score: 0.9971166666666667\n",
      "F1 Score 0.9971152820084703\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9784862419948198\n",
      "Recall Score: 0.9784\n",
      "F1 Score 0.9783903156986493\n",
      "Epoch 1/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.8545 - loss: 0.5115\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9414 - loss: 0.2024\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9557 - loss: 0.1520\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9635 - loss: 0.1229\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9694 - loss: 0.1015\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9741 - loss: 0.0875\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9765 - loss: 0.0761\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9802 - loss: 0.0659\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9830 - loss: 0.0582\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9845 - loss: 0.0516\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9857 - loss: 0.0472\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9874 - loss: 0.0413\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9892 - loss: 0.0375\n",
      "Epoch 14/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9899 - loss: 0.0343\n",
      "Epoch 15/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9918 - loss: 0.0296\n",
      "Epoch 16/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9927 - loss: 0.0265\n",
      "Epoch 17/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9926 - loss: 0.0252\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9941 - loss: 0.0212\n",
      "Epoch 19/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9940 - loss: 0.0210\n",
      "Epoch 20/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9953 - loss: 0.0173\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9721 - loss: 0.1076\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.996368992220894\n",
      "Recall Score: 0.99635\n",
      "F1 Score 0.9963471778586164\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9762007428342744\n",
      "Recall Score: 0.9761\n",
      "F1 Score 0.9761086151591897\n",
      "Epoch 1/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.8753 - loss: 0.4510\n",
      "Epoch 2/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9378 - loss: 0.2163\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9532 - loss: 0.1647\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9615 - loss: 0.1328\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9674 - loss: 0.1115\n",
      "Epoch 6/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9721 - loss: 0.0947\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9753 - loss: 0.0818\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9787 - loss: 0.0715\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9812 - loss: 0.0630\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9828 - loss: 0.0568\n",
      "Epoch 11/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9847 - loss: 0.0509\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9862 - loss: 0.0461\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9871 - loss: 0.0415\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9890 - loss: 0.0379\n",
      "Epoch 15/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9896 - loss: 0.0337\n",
      "Epoch 16/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9907 - loss: 0.0312\n",
      "Epoch 17/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9919 - loss: 0.0279\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9924 - loss: 0.0262\n",
      "Epoch 19/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9932 - loss: 0.0233\n",
      "Epoch 20/20\n",
      "235/235 - 0s - 2ms/step - acc: 0.9941 - loss: 0.0211\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9718 - loss: 0.0997\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9962783580775748\n",
      "Recall Score: 0.9962666666666666\n",
      "F1 Score 0.996267733682232\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點64, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9763558333363486\n",
      "Recall Score: 0.9763\n",
      "F1 Score 0.9762943501815827\n",
      "Epoch 1/5\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9323 - loss: 0.2281\n",
      "Epoch 2/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9699 - loss: 0.0958\n",
      "Epoch 3/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9789 - loss: 0.0678\n",
      "Epoch 4/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9829 - loss: 0.0518\n",
      "Epoch 5/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9872 - loss: 0.0400\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9716 - loss: 0.0989\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9908413569455836\n",
      "Recall Score: 0.9908\n",
      "F1 Score 0.9908035659281649\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9759561300887161\n",
      "Recall Score: 0.9758\n",
      "F1 Score 0.9757937073524227\n",
      "Epoch 1/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9312 - loss: 0.2330\n",
      "Epoch 2/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9695 - loss: 0.1027\n",
      "Epoch 3/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9775 - loss: 0.0771\n",
      "Epoch 4/5\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9816 - loss: 0.0624\n",
      "Epoch 5/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9847 - loss: 0.0530\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9736 - loss: 0.1013\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9912963549858596\n",
      "Recall Score: 0.9912833333333333\n",
      "F1 Score 0.9912827249212669\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9765339931383494\n",
      "Recall Score: 0.9764\n",
      "F1 Score 0.976412535987805\n",
      "Epoch 1/5\n",
      "938/938 - 3s - 4ms/step - acc: 0.9231 - loss: 0.2613\n",
      "Epoch 2/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9683 - loss: 0.1066\n",
      "Epoch 3/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9772 - loss: 0.0738\n",
      "Epoch 4/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9826 - loss: 0.0564\n",
      "Epoch 5/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9863 - loss: 0.0439\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9729 - loss: 0.0913\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9897951905325877\n",
      "Recall Score: 0.9897166666666667\n",
      "F1 Score 0.989730053531046\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9763807867938569\n",
      "Recall Score: 0.9762\n",
      "F1 Score 0.976225941860153\n",
      "Epoch 1/5\n",
      "938/938 - 3s - 4ms/step - acc: 0.9240 - loss: 0.2595\n",
      "Epoch 2/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9662 - loss: 0.1098\n",
      "Epoch 3/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9764 - loss: 0.0774\n",
      "Epoch 4/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9815 - loss: 0.0598\n",
      "Epoch 5/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9853 - loss: 0.0477\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9729 - loss: 0.0904\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9904013588786006\n",
      "Recall Score: 0.9903666666666666\n",
      "F1 Score 0.9903689164926098\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9772772734120261\n",
      "Recall Score: 0.9772\n",
      "F1 Score 0.9771935335593307\n",
      "Epoch 1/5\n",
      "469/469 - 3s - 5ms/step - acc: 0.9108 - loss: 0.3167\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9634 - loss: 0.1232\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9751 - loss: 0.0830\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9801 - loss: 0.0642\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9847 - loss: 0.0495\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9674 - loss: 0.0970\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9888852707164406\n",
      "Recall Score: 0.9888333333333333\n",
      "F1 Score 0.9888298832731142\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9730422410288262\n",
      "Recall Score: 0.9728\n",
      "F1 Score 0.9727754395414089\n",
      "Epoch 1/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9144 - loss: 0.3007\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9622 - loss: 0.1244\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9728 - loss: 0.0856\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9792 - loss: 0.0659\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9837 - loss: 0.0512\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9769 - loss: 0.0774\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9909492251262498\n",
      "Recall Score: 0.9909333333333333\n",
      "F1 Score 0.9909350963029475\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9798518214624601\n",
      "Recall Score: 0.9798\n",
      "F1 Score 0.9797965766087853\n",
      "Epoch 1/5\n",
      "235/235 - 2s - 8ms/step - acc: 0.8881 - loss: 0.3952\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9536 - loss: 0.1570\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9684 - loss: 0.1095\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9758 - loss: 0.0832\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9809 - loss: 0.0668\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9676 - loss: 0.0938\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9863688118111436\n",
      "Recall Score: 0.9863333333333333\n",
      "F1 Score 0.9863332381482794\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9729813798554585\n",
      "Recall Score: 0.9729\n",
      "F1 Score 0.9729061479559877\n",
      "Epoch 1/5\n",
      "235/235 - 2s - 7ms/step - acc: 0.8951 - loss: 0.3700\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9523 - loss: 0.1612\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9668 - loss: 0.1109\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9748 - loss: 0.0844\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 6ms/step - acc: 0.9795 - loss: 0.0676\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9709 - loss: 0.0889\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9864087792113243\n",
      "Recall Score: 0.9864\n",
      "F1 Score 0.9863972820465458\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9748574824452233\n",
      "Recall Score: 0.9748\n",
      "F1 Score 0.9747962194456607\n",
      "Epoch 1/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9334 - loss: 0.2242\n",
      "Epoch 2/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9708 - loss: 0.0951\n",
      "Epoch 3/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9791 - loss: 0.0674\n",
      "Epoch 4/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9836 - loss: 0.0512\n",
      "Epoch 5/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9871 - loss: 0.0398\n",
      "Epoch 6/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9887 - loss: 0.0337\n",
      "Epoch 7/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9906 - loss: 0.0288\n",
      "Epoch 8/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9922 - loss: 0.0230\n",
      "Epoch 9/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9933 - loss: 0.0197\n",
      "Epoch 10/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9937 - loss: 0.0183\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9727 - loss: 0.1186\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9954137406748111\n",
      "Recall Score: 0.9954\n",
      "F1 Score 0.9953993064759202\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9770252691372505\n",
      "Recall Score: 0.9769\n",
      "F1 Score 0.9768963210472869\n",
      "Epoch 1/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9317 - loss: 0.2286\n",
      "Epoch 2/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9702 - loss: 0.1026\n",
      "Epoch 3/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9779 - loss: 0.0735\n",
      "Epoch 4/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9818 - loss: 0.0604\n",
      "Epoch 5/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9860 - loss: 0.0478\n",
      "Epoch 6/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9883 - loss: 0.0419\n",
      "Epoch 7/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9893 - loss: 0.0352\n",
      "Epoch 8/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9913 - loss: 0.0314\n",
      "Epoch 9/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9924 - loss: 0.0264\n",
      "Epoch 10/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9931 - loss: 0.0237\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9783 - loss: 0.1411\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9959126311734895\n",
      "Recall Score: 0.9959\n",
      "F1 Score 0.9958993314887391\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9805543780143428\n",
      "Recall Score: 0.9805\n",
      "F1 Score 0.9804959004161301\n",
      "Epoch 1/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9236 - loss: 0.2646\n",
      "Epoch 2/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9662 - loss: 0.1109\n",
      "Epoch 3/10\n",
      "938/938 - 2s - 3ms/step - acc: 0.9762 - loss: 0.0768\n",
      "Epoch 4/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9822 - loss: 0.0581\n",
      "Epoch 5/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9858 - loss: 0.0438\n",
      "Epoch 6/10\n",
      "938/938 - 3s - 4ms/step - acc: 0.9886 - loss: 0.0344\n",
      "Epoch 7/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9901 - loss: 0.0300\n",
      "Epoch 8/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9922 - loss: 0.0235\n",
      "Epoch 9/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9931 - loss: 0.0202\n",
      "Epoch 10/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9946 - loss: 0.0167\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9680 - loss: 0.1333\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9937426478728001\n",
      "Recall Score: 0.9936666666666667\n",
      "F1 Score 0.9936763502196517\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9726173153441183\n",
      "Recall Score: 0.9722\n",
      "F1 Score 0.9722438923442407\n",
      "Epoch 1/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9272 - loss: 0.2509\n",
      "Epoch 2/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9680 - loss: 0.1045\n",
      "Epoch 3/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9769 - loss: 0.0735\n",
      "Epoch 4/10\n",
      "938/938 - 2s - 3ms/step - acc: 0.9831 - loss: 0.0555\n",
      "Epoch 5/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9865 - loss: 0.0453\n",
      "Epoch 6/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9884 - loss: 0.0362\n",
      "Epoch 7/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9906 - loss: 0.0300\n",
      "Epoch 8/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9922 - loss: 0.0251\n",
      "Epoch 9/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9936 - loss: 0.0203\n",
      "Epoch 10/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9944 - loss: 0.0176\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9775 - loss: 0.1073\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9971531058536377\n",
      "Recall Score: 0.99715\n",
      "F1 Score 0.9971500938888402\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9809591520905684\n",
      "Recall Score: 0.9809\n",
      "F1 Score 0.9808978732186883\n",
      "Epoch 1/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9095 - loss: 0.3161\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9627 - loss: 0.1258\n",
      "Epoch 3/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9742 - loss: 0.0856\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9800 - loss: 0.0653\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9845 - loss: 0.0510\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9869 - loss: 0.0410\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9896 - loss: 0.0329\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9915 - loss: 0.0268\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9924 - loss: 0.0226\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9939 - loss: 0.0193\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9750 - loss: 0.0883\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9961322742301251\n",
      "Recall Score: 0.9961166666666667\n",
      "F1 Score 0.9961164937093879\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9787317221242123\n",
      "Recall Score: 0.9786\n",
      "F1 Score 0.9785970508645668\n",
      "Epoch 1/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9119 - loss: 0.3028\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9615 - loss: 0.1285\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9727 - loss: 0.0903\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9789 - loss: 0.0682\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9839 - loss: 0.0526\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9868 - loss: 0.0430\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9890 - loss: 0.0349\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9911 - loss: 0.0282\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9924 - loss: 0.0235\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9939 - loss: 0.0195\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9712 - loss: 0.1143\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9938643331852909\n",
      "Recall Score: 0.9937\n",
      "F1 Score 0.993723884046867\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9758655544334808\n",
      "Recall Score: 0.9752\n",
      "F1 Score 0.975270856545977\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.8874 - loss: 0.4051\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9546 - loss: 0.1549\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9673 - loss: 0.1099\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9746 - loss: 0.0847\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9794 - loss: 0.0670\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9846 - loss: 0.0535\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9862 - loss: 0.0447\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9887 - loss: 0.0366\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9918 - loss: 0.0292\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9937 - loss: 0.0242\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9751 - loss: 0.0921\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.995115056444157\n",
      "Recall Score: 0.9951\n",
      "F1 Score 0.9951010281322586\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9777973691658685\n",
      "Recall Score: 0.9777\n",
      "F1 Score 0.9777017488415272\n",
      "Epoch 1/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.8920 - loss: 0.3828\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9526 - loss: 0.1593\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9671 - loss: 0.1103\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9754 - loss: 0.0832\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9797 - loss: 0.0666\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9837 - loss: 0.0539\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9860 - loss: 0.0447\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9885 - loss: 0.0376\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9900 - loss: 0.0314\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9922 - loss: 0.0259\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9769 - loss: 0.0830\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9960900659861\n",
      "Recall Score: 0.9960833333333333\n",
      "F1 Score 0.9960838122818215\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9805450940722943\n",
      "Recall Score: 0.9805\n",
      "F1 Score 0.9804966688916684\n",
      "Epoch 1/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9320 - loss: 0.2280\n",
      "Epoch 2/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9707 - loss: 0.0958\n",
      "Epoch 3/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9789 - loss: 0.0677\n",
      "Epoch 4/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9836 - loss: 0.0513\n",
      "Epoch 5/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9866 - loss: 0.0414\n",
      "Epoch 6/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9887 - loss: 0.0342\n",
      "Epoch 7/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9910 - loss: 0.0266\n",
      "Epoch 8/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9913 - loss: 0.0258\n",
      "Epoch 9/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9934 - loss: 0.0195\n",
      "Epoch 10/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9929 - loss: 0.0215\n",
      "Epoch 11/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9944 - loss: 0.0159\n",
      "Epoch 12/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9939 - loss: 0.0175\n",
      "Epoch 13/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9952 - loss: 0.0146\n",
      "Epoch 14/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9953 - loss: 0.0133\n",
      "Epoch 15/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9955 - loss: 0.0128\n",
      "Epoch 16/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9962 - loss: 0.0112\n",
      "Epoch 17/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9959 - loss: 0.0122\n",
      "Epoch 18/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9964 - loss: 0.0112\n",
      "Epoch 19/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9970 - loss: 0.0091\n",
      "Epoch 20/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9963 - loss: 0.0111\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9766 - loss: 0.1348\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9963484817370523\n",
      "Recall Score: 0.9963333333333333\n",
      "F1 Score 0.9963334839638083\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9805417362953536\n",
      "Recall Score: 0.9804\n",
      "F1 Score 0.980401448484472\n",
      "Epoch 1/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9313 - loss: 0.2291\n",
      "Epoch 2/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9695 - loss: 0.1024\n",
      "Epoch 3/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9784 - loss: 0.0733\n",
      "Epoch 4/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9827 - loss: 0.0595\n",
      "Epoch 5/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9870 - loss: 0.0470\n",
      "Epoch 6/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9882 - loss: 0.0414\n",
      "Epoch 7/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9895 - loss: 0.0364\n",
      "Epoch 8/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9913 - loss: 0.0300\n",
      "Epoch 9/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9925 - loss: 0.0257\n",
      "Epoch 10/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9928 - loss: 0.0229\n",
      "Epoch 11/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9939 - loss: 0.0210\n",
      "Epoch 12/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9952 - loss: 0.0164\n",
      "Epoch 13/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9959 - loss: 0.0142\n",
      "Epoch 14/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9962 - loss: 0.0131\n",
      "Epoch 15/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9962 - loss: 0.0138\n",
      "Epoch 16/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9967 - loss: 0.0113\n",
      "Epoch 17/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9974 - loss: 0.0097\n",
      "Epoch 18/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9973 - loss: 0.0093\n",
      "Epoch 19/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9977 - loss: 0.0072\n",
      "Epoch 20/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9977 - loss: 0.0074\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9770 - loss: 0.2087 \n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9988185309508619\n",
      "Recall Score: 0.9988166666666667\n",
      "F1 Score 0.9988165943632138\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9804715426505787\n",
      "Recall Score: 0.9804\n",
      "F1 Score 0.9804050953715412\n",
      "Epoch 1/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9234 - loss: 0.2638\n",
      "Epoch 2/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9676 - loss: 0.1087\n",
      "Epoch 3/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9766 - loss: 0.0749\n",
      "Epoch 4/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9820 - loss: 0.0559\n",
      "Epoch 5/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9859 - loss: 0.0436\n",
      "Epoch 6/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9887 - loss: 0.0349\n",
      "Epoch 7/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9901 - loss: 0.0295\n",
      "Epoch 8/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9922 - loss: 0.0234\n",
      "Epoch 9/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9925 - loss: 0.0217\n",
      "Epoch 10/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9941 - loss: 0.0177\n",
      "Epoch 11/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9946 - loss: 0.0165\n",
      "Epoch 12/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9954 - loss: 0.0144\n",
      "Epoch 13/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9959 - loss: 0.0122\n",
      "Epoch 14/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9959 - loss: 0.0125\n",
      "Epoch 15/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9959 - loss: 0.0117\n",
      "Epoch 16/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9968 - loss: 0.0096\n",
      "Epoch 17/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9965 - loss: 0.0098\n",
      "Epoch 18/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9964 - loss: 0.0106\n",
      "Epoch 19/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9966 - loss: 0.0095\n",
      "Epoch 20/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9974 - loss: 0.0078\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9750 - loss: 0.1294\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9948614617575819\n",
      "Recall Score: 0.9948\n",
      "F1 Score 0.994800333718404\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.977861243073892\n",
      "Recall Score: 0.9777\n",
      "F1 Score 0.9777073010994842\n",
      "Epoch 1/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9243 - loss: 0.2561\n",
      "Epoch 2/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9681 - loss: 0.1085\n",
      "Epoch 3/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9769 - loss: 0.0757\n",
      "Epoch 4/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9820 - loss: 0.0587\n",
      "Epoch 5/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9852 - loss: 0.0478\n",
      "Epoch 6/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9885 - loss: 0.0380\n",
      "Epoch 7/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9902 - loss: 0.0325\n",
      "Epoch 8/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9920 - loss: 0.0263\n",
      "Epoch 9/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9934 - loss: 0.0222\n",
      "Epoch 10/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9948 - loss: 0.0184\n",
      "Epoch 11/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9951 - loss: 0.0159\n",
      "Epoch 12/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9962 - loss: 0.0132\n",
      "Epoch 13/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9965 - loss: 0.0111\n",
      "Epoch 14/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9965 - loss: 0.0108\n",
      "Epoch 15/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9972 - loss: 0.0092\n",
      "Epoch 16/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9974 - loss: 0.0080\n",
      "Epoch 17/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9981 - loss: 0.0071\n",
      "Epoch 18/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9984 - loss: 0.0059\n",
      "Epoch 19/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9986 - loss: 0.0047\n",
      "Epoch 20/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9986 - loss: 0.0039\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9759 - loss: 0.1632\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9988856800650436\n",
      "Recall Score: 0.9988833333333333\n",
      "F1 Score 0.9988835256501098\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9798039605170363\n",
      "Recall Score: 0.9797\n",
      "F1 Score 0.9797032183100604\n",
      "Epoch 1/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9118 - loss: 0.3149\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9621 - loss: 0.1292\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9733 - loss: 0.0884\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9804 - loss: 0.0650\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9844 - loss: 0.0515\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9876 - loss: 0.0405\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9898 - loss: 0.0324\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9912 - loss: 0.0281\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9940 - loss: 0.0208\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9941 - loss: 0.0183\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9942 - loss: 0.0179\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9960 - loss: 0.0136\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9960 - loss: 0.0127\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9966 - loss: 0.0100\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9964 - loss: 0.0108\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9966 - loss: 0.0100\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9966 - loss: 0.0100\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9978 - loss: 0.0068\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9979 - loss: 0.0064\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9969 - loss: 0.0084\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9746 - loss: 0.1230 \n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9983525932391714\n",
      "Recall Score: 0.99835\n",
      "F1 Score 0.9983498340023161\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9785494926797293\n",
      "Recall Score: 0.9785\n",
      "F1 Score 0.9784993923704255\n",
      "Epoch 1/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9130 - loss: 0.3027\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9617 - loss: 0.1284\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9731 - loss: 0.0876\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9798 - loss: 0.0657\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9839 - loss: 0.0524\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9869 - loss: 0.0428\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9897 - loss: 0.0338\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9911 - loss: 0.0281\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9930 - loss: 0.0229\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9937 - loss: 0.0198\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9948 - loss: 0.0162\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9957 - loss: 0.0136\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9964 - loss: 0.0113\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9973 - loss: 0.0095\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9973 - loss: 0.0086\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9977 - loss: 0.0071\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9980 - loss: 0.0063\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9986 - loss: 0.0050\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9985 - loss: 0.0048\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9987 - loss: 0.0038\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9756 - loss: 0.1335\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9993175907975231\n",
      "Recall Score: 0.9993166666666666\n",
      "F1 Score 0.9993165185061693\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9797852897956006\n",
      "Recall Score: 0.9797\n",
      "F1 Score 0.9796995824833999\n",
      "Epoch 1/20\n",
      "235/235 - 2s - 8ms/step - acc: 0.8916 - loss: 0.3949\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9546 - loss: 0.1596\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9670 - loss: 0.1120\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9742 - loss: 0.0860\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9793 - loss: 0.0691\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9838 - loss: 0.0548\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9861 - loss: 0.0454\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9890 - loss: 0.0376\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9909 - loss: 0.0316\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9924 - loss: 0.0253\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9937 - loss: 0.0218\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9954 - loss: 0.0168\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9967 - loss: 0.0134\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9972 - loss: 0.0112\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9973 - loss: 0.0103\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9976 - loss: 0.0092\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9982 - loss: 0.0073\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9981 - loss: 0.0072\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9969 - loss: 0.0096\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9978 - loss: 0.0081\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9761 - loss: 0.1028\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9984042649384295\n",
      "Recall Score: 0.9984\n",
      "F1 Score 0.9983998773903922\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9784202447417992\n",
      "Recall Score: 0.9783\n",
      "F1 Score 0.9782959344923756\n",
      "Epoch 1/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.8930 - loss: 0.3825\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9517 - loss: 0.1641\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9661 - loss: 0.1131\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9740 - loss: 0.0859\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9796 - loss: 0.0677\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9823 - loss: 0.0559\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9855 - loss: 0.0469\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9877 - loss: 0.0393\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9901 - loss: 0.0323\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9918 - loss: 0.0275\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9929 - loss: 0.0230\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9937 - loss: 0.0200\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9949 - loss: 0.0169\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9957 - loss: 0.0142\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9966 - loss: 0.0117\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9970 - loss: 0.0103\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9972 - loss: 0.0095\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9980 - loss: 0.0075\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9981 - loss: 0.0067\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9987 - loss: 0.0053\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9768 - loss: 0.1200\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9981451613375972\n",
      "Recall Score: 0.9981333333333333\n",
      "F1 Score 0.998134146144987\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點128, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9789154443970373\n",
      "Recall Score: 0.9788\n",
      "F1 Score 0.978799624600487\n",
      "Epoch 1/5\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9411 - loss: 0.1977\n",
      "Epoch 2/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9734 - loss: 0.0856\n",
      "Epoch 3/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9808 - loss: 0.0594\n",
      "Epoch 4/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9855 - loss: 0.0434\n",
      "Epoch 5/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9883 - loss: 0.0356\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9749 - loss: 0.0938\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9926547899365021\n",
      "Recall Score: 0.9926\n",
      "F1 Score 0.9926040610372053\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.978162143467105\n",
      "Recall Score: 0.9779\n",
      "F1 Score 0.977922670306783\n",
      "Epoch 1/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9394 - loss: 0.2016\n",
      "Epoch 2/5\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9738 - loss: 0.0892\n",
      "Epoch 3/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9817 - loss: 0.0639\n",
      "Epoch 4/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9855 - loss: 0.0499\n",
      "Epoch 5/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9887 - loss: 0.0406\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9745 - loss: 0.1038\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9919039899374997\n",
      "Recall Score: 0.99185\n",
      "F1 Score 0.99185118332401\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9789200888822661\n",
      "Recall Score: 0.9788\n",
      "F1 Score 0.9787829652681957\n",
      "Epoch 1/5\n",
      "938/938 - 4s - 4ms/step - acc: 0.9352 - loss: 0.2196\n",
      "Epoch 2/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9733 - loss: 0.0854\n",
      "Epoch 3/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9814 - loss: 0.0587\n",
      "Epoch 4/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9870 - loss: 0.0404\n",
      "Epoch 5/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9891 - loss: 0.0331\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9754 - loss: 0.0883\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9919483047908233\n",
      "Recall Score: 0.9918666666666667\n",
      "F1 Score 0.9918670703091872\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9785445928474693\n",
      "Recall Score: 0.9784\n",
      "F1 Score 0.9784026900896744\n",
      "Epoch 1/5\n",
      "938/938 - 4s - 4ms/step - acc: 0.9344 - loss: 0.2214\n",
      "Epoch 2/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9733 - loss: 0.0881\n",
      "Epoch 3/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9818 - loss: 0.0604\n",
      "Epoch 4/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9861 - loss: 0.0458\n",
      "Epoch 5/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9893 - loss: 0.0353\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9755 - loss: 0.0894\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9930365521474613\n",
      "Recall Score: 0.993\n",
      "F1 Score 0.9929959794428271\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9798229636973141\n",
      "Recall Score: 0.9796\n",
      "F1 Score 0.979591793552813\n",
      "Epoch 1/5\n",
      "469/469 - 3s - 7ms/step - acc: 0.9231 - loss: 0.2645\n",
      "Epoch 2/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9704 - loss: 0.0969\n",
      "Epoch 3/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9806 - loss: 0.0629\n",
      "Epoch 4/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9862 - loss: 0.0443\n",
      "Epoch 5/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9896 - loss: 0.0322\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9738 - loss: 0.0853\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9939635868141049\n",
      "Recall Score: 0.9939333333333333\n",
      "F1 Score 0.993937220632725\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.977912573443532\n",
      "Recall Score: 0.9778\n",
      "F1 Score 0.9778071695061109\n",
      "Epoch 1/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.9228 - loss: 0.2553\n",
      "Epoch 2/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9703 - loss: 0.0977\n",
      "Epoch 3/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9796 - loss: 0.0654\n",
      "Epoch 4/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9851 - loss: 0.0473\n",
      "Epoch 5/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.9884 - loss: 0.0358\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9746 - loss: 0.0912\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.990827650570299\n",
      "Recall Score: 0.9907166666666667\n",
      "F1 Score 0.9907179184865508\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9787087987683836\n",
      "Recall Score: 0.9785\n",
      "F1 Score 0.978523200072978\n",
      "Epoch 1/5\n",
      "235/235 - 2s - 9ms/step - acc: 0.9101 - loss: 0.3214\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9652 - loss: 0.1194\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9767 - loss: 0.0777\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9830 - loss: 0.0565\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9869 - loss: 0.0428\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9753 - loss: 0.0812\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9914946994689021\n",
      "Recall Score: 0.9914833333333334\n",
      "F1 Score 0.991482141320219\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9790231005340997\n",
      "Recall Score: 0.979\n",
      "F1 Score 0.9789912693994285\n",
      "Epoch 1/5\n",
      "235/235 - 2s - 7ms/step - acc: 0.9040 - loss: 0.3260\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9634 - loss: 0.1212\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9758 - loss: 0.0794\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9817 - loss: 0.0584\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9868 - loss: 0.0435\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9758 - loss: 0.0781\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9924416335508965\n",
      "Recall Score: 0.9924166666666666\n",
      "F1 Score 0.9924165899766817\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9800061826285781\n",
      "Recall Score: 0.9799\n",
      "F1 Score 0.979904248280138\n",
      "Epoch 1/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9387 - loss: 0.2028\n",
      "Epoch 2/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9741 - loss: 0.0853\n",
      "Epoch 3/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9813 - loss: 0.0579\n",
      "Epoch 4/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9861 - loss: 0.0441\n",
      "Epoch 5/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9885 - loss: 0.0356\n",
      "Epoch 6/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9901 - loss: 0.0299\n",
      "Epoch 7/10\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9913 - loss: 0.0253\n",
      "Epoch 8/10\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9931 - loss: 0.0210\n",
      "Epoch 9/10\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9941 - loss: 0.0173\n",
      "Epoch 10/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9933 - loss: 0.0198\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9764 - loss: 0.0992\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9953400976037949\n",
      "Recall Score: 0.9953333333333333\n",
      "F1 Score 0.9953315740375622\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9797136503352676\n",
      "Recall Score: 0.9797\n",
      "F1 Score 0.9796946162480729\n",
      "Epoch 1/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9390 - loss: 0.1986\n",
      "Epoch 2/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9730 - loss: 0.0910\n",
      "Epoch 3/10\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9819 - loss: 0.0660\n",
      "Epoch 4/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9863 - loss: 0.0497\n",
      "Epoch 5/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9884 - loss: 0.0423\n",
      "Epoch 6/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9904 - loss: 0.0344\n",
      "Epoch 7/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9915 - loss: 0.0296\n",
      "Epoch 8/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9928 - loss: 0.0256\n",
      "Epoch 9/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9944 - loss: 0.0195\n",
      "Epoch 10/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9952 - loss: 0.0164\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9748 - loss: 0.1808\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9955576094635337\n",
      "Recall Score: 0.9955166666666667\n",
      "F1 Score 0.9955205980555637\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9786602776098143\n",
      "Recall Score: 0.9785\n",
      "F1 Score 0.9785036714926011\n",
      "Epoch 1/10\n",
      "938/938 - 5s - 5ms/step - acc: 0.9358 - loss: 0.2194\n",
      "Epoch 2/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9738 - loss: 0.0839\n",
      "Epoch 3/10\n",
      "938/938 - 3s - 4ms/step - acc: 0.9822 - loss: 0.0565\n",
      "Epoch 4/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9876 - loss: 0.0404\n",
      "Epoch 5/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9895 - loss: 0.0326\n",
      "Epoch 6/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9911 - loss: 0.0261\n",
      "Epoch 7/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9920 - loss: 0.0229\n",
      "Epoch 8/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9941 - loss: 0.0181\n",
      "Epoch 9/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9942 - loss: 0.0178\n",
      "Epoch 10/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9956 - loss: 0.0131\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9754 - loss: 0.1087\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9961852146540989\n",
      "Recall Score: 0.9961666666666666\n",
      "F1 Score 0.9961659757095679\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9795465171319191\n",
      "Recall Score: 0.9794\n",
      "F1 Score 0.9793990943718667\n",
      "Epoch 1/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9338 - loss: 0.2181\n",
      "Epoch 2/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9741 - loss: 0.0885\n",
      "Epoch 3/10\n",
      "938/938 - 3s - 4ms/step - acc: 0.9820 - loss: 0.0610\n",
      "Epoch 4/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9863 - loss: 0.0446\n",
      "Epoch 5/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9892 - loss: 0.0346\n",
      "Epoch 6/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9920 - loss: 0.0273\n",
      "Epoch 7/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9932 - loss: 0.0206\n",
      "Epoch 8/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9943 - loss: 0.0175\n",
      "Epoch 9/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9953 - loss: 0.0154\n",
      "Epoch 10/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9962 - loss: 0.0119\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9777 - loss: 0.1166\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9980023733426856\n",
      "Recall Score: 0.998\n",
      "F1 Score 0.9979995025549444\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9816661073115002\n",
      "Recall Score: 0.9816\n",
      "F1 Score 0.9815903695097798\n",
      "Epoch 1/10\n",
      "469/469 - 3s - 7ms/step - acc: 0.9282 - loss: 0.2551\n",
      "Epoch 2/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9714 - loss: 0.0940\n",
      "Epoch 3/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9811 - loss: 0.0613\n",
      "Epoch 4/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9858 - loss: 0.0451\n",
      "Epoch 5/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9897 - loss: 0.0330\n",
      "Epoch 6/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9919 - loss: 0.0249\n",
      "Epoch 7/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9931 - loss: 0.0216\n",
      "Epoch 8/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9941 - loss: 0.0176\n",
      "Epoch 9/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9950 - loss: 0.0146\n",
      "Epoch 10/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9959 - loss: 0.0120\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9694 - loss: 0.1279\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9939529879287312\n",
      "Recall Score: 0.9938833333333333\n",
      "F1 Score 0.9938880538517206\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9755992011872553\n",
      "Recall Score: 0.9753\n",
      "F1 Score 0.9753085970652273\n",
      "Epoch 1/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9219 - loss: 0.2587\n",
      "Epoch 2/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9700 - loss: 0.0989\n",
      "Epoch 3/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9802 - loss: 0.0642\n",
      "Epoch 4/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9850 - loss: 0.0467\n",
      "Epoch 5/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9889 - loss: 0.0347\n",
      "Epoch 6/10\n",
      "469/469 - 2s - 3ms/step - acc: 0.9912 - loss: 0.0272\n",
      "Epoch 7/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9931 - loss: 0.0204\n",
      "Epoch 8/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9947 - loss: 0.0163\n",
      "Epoch 9/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9959 - loss: 0.0124\n",
      "Epoch 10/10\n",
      "469/469 - 2s - 3ms/step - acc: 0.9968 - loss: 0.0101\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9775 - loss: 0.1070\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9975265743107685\n",
      "Recall Score: 0.9975166666666667\n",
      "F1 Score 0.9975159808585174\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9812076268138202\n",
      "Recall Score: 0.9811\n",
      "F1 Score 0.981091087951243\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 10ms/step - acc: 0.9136 - loss: 0.3110\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9651 - loss: 0.1162\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9767 - loss: 0.0770\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9840 - loss: 0.0548\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9874 - loss: 0.0407\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9908 - loss: 0.0304\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9924 - loss: 0.0251\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9947 - loss: 0.0179\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9961 - loss: 0.0133\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9972 - loss: 0.0105\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9784 - loss: 0.0727\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9990504179542328\n",
      "Recall Score: 0.99905\n",
      "F1 Score 0.9990499500135076\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9817981577525828\n",
      "Recall Score: 0.9818\n",
      "F1 Score 0.98179287912071\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 8ms/step - acc: 0.9012 - loss: 0.3323\n",
      "Epoch 2/10\n",
      "235/235 - 2s - 11ms/step - acc: 0.9623 - loss: 0.1243\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9744 - loss: 0.0822\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9809 - loss: 0.0590\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9862 - loss: 0.0444\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9893 - loss: 0.0336\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9918 - loss: 0.0259\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9934 - loss: 0.0211\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9949 - loss: 0.0163\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9959 - loss: 0.0129\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9798 - loss: 0.0787\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9982520775918425\n",
      "Recall Score: 0.99825\n",
      "F1 Score 0.9982497243841487\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9822185899676333\n",
      "Recall Score: 0.9822\n",
      "F1 Score 0.9821995693596316\n",
      "Epoch 1/20\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9404 - loss: 0.1973\n",
      "Epoch 2/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9742 - loss: 0.0835\n",
      "Epoch 3/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9818 - loss: 0.0573\n",
      "Epoch 4/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9864 - loss: 0.0426\n",
      "Epoch 5/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9893 - loss: 0.0335\n",
      "Epoch 6/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9903 - loss: 0.0296\n",
      "Epoch 7/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9922 - loss: 0.0233\n",
      "Epoch 8/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9935 - loss: 0.0199\n",
      "Epoch 9/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9940 - loss: 0.0186\n",
      "Epoch 10/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9947 - loss: 0.0166\n",
      "Epoch 11/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9953 - loss: 0.0149\n",
      "Epoch 12/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9947 - loss: 0.0163\n",
      "Epoch 13/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9968 - loss: 0.0102\n",
      "Epoch 14/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9950 - loss: 0.0163\n",
      "Epoch 15/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9957 - loss: 0.0132\n",
      "Epoch 16/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9958 - loss: 0.0143\n",
      "Epoch 17/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9973 - loss: 0.0089\n",
      "Epoch 18/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9969 - loss: 0.0102\n",
      "Epoch 19/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9967 - loss: 0.0119\n",
      "Epoch 20/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9966 - loss: 0.0121\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9797 - loss: 0.1499\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9974430553453125\n",
      "Recall Score: 0.9974333333333333\n",
      "F1 Score 0.9974338193991888\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9807381233076521\n",
      "Recall Score: 0.9806\n",
      "F1 Score 0.9806169872268612\n",
      "Epoch 1/20\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9399 - loss: 0.2010\n",
      "Epoch 2/20\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9740 - loss: 0.0917\n",
      "Epoch 3/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9814 - loss: 0.0657\n",
      "Epoch 4/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9856 - loss: 0.0524\n",
      "Epoch 5/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9879 - loss: 0.0426\n",
      "Epoch 6/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9907 - loss: 0.0344\n",
      "Epoch 7/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9918 - loss: 0.0300\n",
      "Epoch 8/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9934 - loss: 0.0238\n",
      "Epoch 9/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9944 - loss: 0.0208\n",
      "Epoch 10/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9951 - loss: 0.0180\n",
      "Epoch 11/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9959 - loss: 0.0148\n",
      "Epoch 12/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9968 - loss: 0.0107\n",
      "Epoch 13/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9973 - loss: 0.0116\n",
      "Epoch 14/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9972 - loss: 0.0103\n",
      "Epoch 15/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9977 - loss: 0.0084\n",
      "Epoch 16/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9981 - loss: 0.0063\n",
      "Epoch 17/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9983 - loss: 0.0060\n",
      "Epoch 18/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9984 - loss: 0.0056\n",
      "Epoch 19/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9987 - loss: 0.0047\n",
      "Epoch 20/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9990 - loss: 0.0032\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9776 - loss: 0.2174\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9993671814321037\n",
      "Recall Score: 0.9993666666666666\n",
      "F1 Score 0.9993665724666987\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9814687230786039\n",
      "Recall Score: 0.9814\n",
      "F1 Score 0.981397346798378\n",
      "Epoch 1/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9344 - loss: 0.2189\n",
      "Epoch 2/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9732 - loss: 0.0877\n",
      "Epoch 3/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9821 - loss: 0.0567\n",
      "Epoch 4/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9862 - loss: 0.0424\n",
      "Epoch 5/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9893 - loss: 0.0333\n",
      "Epoch 6/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9916 - loss: 0.0264\n",
      "Epoch 7/20\n",
      "938/938 - 5s - 6ms/step - acc: 0.9923 - loss: 0.0226\n",
      "Epoch 8/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9938 - loss: 0.0195\n",
      "Epoch 9/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9943 - loss: 0.0170\n",
      "Epoch 10/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9946 - loss: 0.0158\n",
      "Epoch 11/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9963 - loss: 0.0113\n",
      "Epoch 12/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9948 - loss: 0.0152\n",
      "Epoch 13/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9963 - loss: 0.0116\n",
      "Epoch 14/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9965 - loss: 0.0104\n",
      "Epoch 15/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9962 - loss: 0.0115\n",
      "Epoch 16/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9972 - loss: 0.0087\n",
      "Epoch 17/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9974 - loss: 0.0090\n",
      "Epoch 18/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9962 - loss: 0.0123\n",
      "Epoch 19/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9971 - loss: 0.0088\n",
      "Epoch 20/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9974 - loss: 0.0086\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9756 - loss: 0.1495\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9975055552673363\n",
      "Recall Score: 0.9975\n",
      "F1 Score 0.9975002830095122\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.980152777285692\n",
      "Recall Score: 0.9801\n",
      "F1 Score 0.9801007605089596\n",
      "Epoch 1/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9342 - loss: 0.2173\n",
      "Epoch 2/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9735 - loss: 0.0874\n",
      "Epoch 3/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9820 - loss: 0.0582\n",
      "Epoch 4/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9860 - loss: 0.0452\n",
      "Epoch 5/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9887 - loss: 0.0346\n",
      "Epoch 6/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9920 - loss: 0.0257\n",
      "Epoch 7/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9937 - loss: 0.0204\n",
      "Epoch 8/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9947 - loss: 0.0179\n",
      "Epoch 9/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9953 - loss: 0.0149\n",
      "Epoch 10/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9962 - loss: 0.0118\n",
      "Epoch 11/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9969 - loss: 0.0093\n",
      "Epoch 12/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9973 - loss: 0.0082\n",
      "Epoch 13/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9977 - loss: 0.0067\n",
      "Epoch 14/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9985 - loss: 0.0048\n",
      "Epoch 15/20\n",
      "938/938 - 5s - 6ms/step - acc: 0.9986 - loss: 0.0044\n",
      "Epoch 16/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9989 - loss: 0.0033\n",
      "Epoch 17/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9992 - loss: 0.0025\n",
      "Epoch 18/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9990 - loss: 0.0030\n",
      "Epoch 19/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9995 - loss: 0.0017\n",
      "Epoch 20/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9995 - loss: 0.0016\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9816 - loss: 0.1496\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9996338775964162\n",
      "Recall Score: 0.9996333333333334\n",
      "F1 Score 0.9996332502003415\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9842426085666444\n",
      "Recall Score: 0.9842\n",
      "F1 Score 0.9842024327206309\n",
      "Epoch 1/20\n",
      "469/469 - 3s - 7ms/step - acc: 0.9275 - loss: 0.2552\n",
      "Epoch 2/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9708 - loss: 0.0964\n",
      "Epoch 3/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9808 - loss: 0.0619\n",
      "Epoch 4/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9858 - loss: 0.0451\n",
      "Epoch 5/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9899 - loss: 0.0325\n",
      "Epoch 6/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9916 - loss: 0.0260\n",
      "Epoch 7/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9929 - loss: 0.0219\n",
      "Epoch 8/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9941 - loss: 0.0180\n",
      "Epoch 9/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9955 - loss: 0.0142\n",
      "Epoch 10/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9946 - loss: 0.0151\n",
      "Epoch 11/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9960 - loss: 0.0121\n",
      "Epoch 12/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9957 - loss: 0.0132\n",
      "Epoch 13/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9970 - loss: 0.0082\n",
      "Epoch 14/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9965 - loss: 0.0105\n",
      "Epoch 15/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9977 - loss: 0.0066\n",
      "Epoch 16/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9966 - loss: 0.0103\n",
      "Epoch 17/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9976 - loss: 0.0066\n",
      "Epoch 18/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9966 - loss: 0.0109\n",
      "Epoch 19/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9985 - loss: 0.0048\n",
      "Epoch 20/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9977 - loss: 0.0073\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9792 - loss: 0.0956\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9987183189031273\n",
      "Recall Score: 0.9987166666666667\n",
      "F1 Score 0.9987167428741588\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9816594501754571\n",
      "Recall Score: 0.9816\n",
      "F1 Score 0.98160245008979\n",
      "Epoch 1/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9215 - loss: 0.2605\n",
      "Epoch 2/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9699 - loss: 0.0974\n",
      "Epoch 3/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9796 - loss: 0.0656\n",
      "Epoch 4/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9850 - loss: 0.0469\n",
      "Epoch 5/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9890 - loss: 0.0352\n",
      "Epoch 6/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9915 - loss: 0.0275\n",
      "Epoch 7/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9934 - loss: 0.0214\n",
      "Epoch 8/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9948 - loss: 0.0158\n",
      "Epoch 9/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9958 - loss: 0.0132\n",
      "Epoch 10/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9964 - loss: 0.0111\n",
      "Epoch 11/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9971 - loss: 0.0092\n",
      "Epoch 12/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9980 - loss: 0.0064\n",
      "Epoch 13/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9982 - loss: 0.0053\n",
      "Epoch 14/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9986 - loss: 0.0040\n",
      "Epoch 15/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9987 - loss: 0.0038\n",
      "Epoch 16/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9991 - loss: 0.0028\n",
      "Epoch 17/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9992 - loss: 0.0023\n",
      "Epoch 18/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9995 - loss: 0.0014\n",
      "Epoch 19/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9997 - loss: 9.6283e-04\n",
      "Epoch 20/20\n",
      "469/469 - 2s - 3ms/step - acc: 0.9999 - loss: 3.6442e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9809 - loss: 0.1086\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9999833361491243\n",
      "Recall Score: 0.9999833333333333\n",
      "F1 Score 0.9999833334192783\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.983432213731576\n",
      "Recall Score: 0.9834\n",
      "F1 Score 0.9834027563469337\n",
      "Epoch 1/20\n",
      "235/235 - 2s - 10ms/step - acc: 0.9112 - loss: 0.3215\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9657 - loss: 0.1168\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9773 - loss: 0.0753\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9835 - loss: 0.0547\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9871 - loss: 0.0417\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9908 - loss: 0.0300\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9932 - loss: 0.0230\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9938 - loss: 0.0202\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9955 - loss: 0.0155\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9967 - loss: 0.0114\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9967 - loss: 0.0106\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9972 - loss: 0.0093\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9973 - loss: 0.0089\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9974 - loss: 0.0077\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9995 - loss: 0.0027\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9975 - loss: 0.0073\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9957 - loss: 0.0134\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9978 - loss: 0.0071\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9992 - loss: 0.0030\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9983 - loss: 0.0056\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9744 - loss: 0.1237\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9978701134120801\n",
      "Recall Score: 0.9978666666666667\n",
      "F1 Score 0.9978669569604036\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9790295380620774\n",
      "Recall Score: 0.979\n",
      "F1 Score 0.9789948764306222\n",
      "Epoch 1/20\n",
      "235/235 - 2s - 8ms/step - acc: 0.9035 - loss: 0.3278\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9627 - loss: 0.1246\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9755 - loss: 0.0809\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9820 - loss: 0.0583\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9866 - loss: 0.0431\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9887 - loss: 0.0344\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9923 - loss: 0.0253\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9938 - loss: 0.0207\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9953 - loss: 0.0157\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9961 - loss: 0.0128\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9968 - loss: 0.0098\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9977 - loss: 0.0074\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9977 - loss: 0.0072\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9984 - loss: 0.0047\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9990 - loss: 0.0036\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9991 - loss: 0.0030\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9994 - loss: 0.0021\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9998 - loss: 9.5730e-04\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9997 - loss: 0.0013\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 5ms/step - acc: 1.0000 - loss: 4.2243e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9807 - loss: 0.1009\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點256, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9837127243982746\n",
      "Recall Score: 0.9837\n",
      "F1 Score 0.9837022751040387\n",
      "Epoch 1/5\n",
      "1875/1875 - 18s - 9ms/step - acc: 0.9443 - loss: 0.1829\n",
      "Epoch 2/5\n",
      "1875/1875 - 14s - 7ms/step - acc: 0.9746 - loss: 0.0812\n",
      "Epoch 3/5\n",
      "1875/1875 - 14s - 7ms/step - acc: 0.9817 - loss: 0.0564\n",
      "Epoch 4/5\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9870 - loss: 0.0417\n",
      "Epoch 5/5\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9886 - loss: 0.0350\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9735 - loss: 0.1065\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9909704139920938\n",
      "Recall Score: 0.9909166666666667\n",
      "F1 Score 0.9909258797001748\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9774770543538918\n",
      "Recall Score: 0.9772\n",
      "F1 Score 0.977255052648568\n",
      "Epoch 1/5\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9430 - loss: 0.1885\n",
      "Epoch 2/5\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9754 - loss: 0.0860\n",
      "Epoch 3/5\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9823 - loss: 0.0626\n",
      "Epoch 4/5\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9862 - loss: 0.0484\n",
      "Epoch 5/5\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9900 - loss: 0.0374\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9799 - loss: 0.1046\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9941683178902946\n",
      "Recall Score: 0.99415\n",
      "F1 Score 0.9941496852638101\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9833000208800642\n",
      "Recall Score: 0.9832\n",
      "F1 Score 0.9832043517343518\n",
      "Epoch 1/5\n",
      "938/938 - 10s - 11ms/step - acc: 0.9432 - loss: 0.1899\n",
      "Epoch 2/5\n",
      "938/938 - 9s - 10ms/step - acc: 0.9761 - loss: 0.0760\n",
      "Epoch 3/5\n",
      "938/938 - 9s - 9ms/step - acc: 0.9834 - loss: 0.0503\n",
      "Epoch 4/5\n",
      "938/938 - 8s - 8ms/step - acc: 0.9876 - loss: 0.0367\n",
      "Epoch 5/5\n",
      "938/938 - 11s - 11ms/step - acc: 0.9896 - loss: 0.0310\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9769 - loss: 0.0927\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9950713961110792\n",
      "Recall Score: 0.9950666666666667\n",
      "F1 Score 0.995064653743663\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9812767695496606\n",
      "Recall Score: 0.9812\n",
      "F1 Score 0.9811958140382371\n",
      "Epoch 1/5\n",
      "938/938 - 8s - 9ms/step - acc: 0.9383 - loss: 0.2003\n",
      "Epoch 2/5\n",
      "938/938 - 7s - 8ms/step - acc: 0.9755 - loss: 0.0803\n",
      "Epoch 3/5\n",
      "938/938 - 7s - 7ms/step - acc: 0.9833 - loss: 0.0526\n",
      "Epoch 4/5\n",
      "938/938 - 6s - 7ms/step - acc: 0.9882 - loss: 0.0388\n",
      "Epoch 5/5\n",
      "938/938 - 6s - 6ms/step - acc: 0.9919 - loss: 0.0269\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9716 - loss: 0.1246\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9902548920139218\n",
      "Recall Score: 0.9901\n",
      "F1 Score 0.9900993923460196\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9771626916408426\n",
      "Recall Score: 0.9766\n",
      "F1 Score 0.9766135915175819\n",
      "Epoch 1/5\n",
      "469/469 - 6s - 14ms/step - acc: 0.9351 - loss: 0.2181\n",
      "Epoch 2/5\n",
      "469/469 - 10s - 22ms/step - acc: 0.9746 - loss: 0.0812\n",
      "Epoch 3/5\n",
      "469/469 - 5s - 10ms/step - acc: 0.9836 - loss: 0.0520\n",
      "Epoch 4/5\n",
      "469/469 - 4s - 9ms/step - acc: 0.9879 - loss: 0.0373\n",
      "Epoch 5/5\n",
      "469/469 - 4s - 9ms/step - acc: 0.9923 - loss: 0.0252\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9774 - loss: 0.0843\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9937208124021704\n",
      "Recall Score: 0.9937\n",
      "F1 Score 0.9936963842083294\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9803629845852915\n",
      "Recall Score: 0.9803\n",
      "F1 Score 0.98027912142726\n",
      "Epoch 1/5\n",
      "469/469 - 5s - 11ms/step - acc: 0.9291 - loss: 0.2297\n",
      "Epoch 2/5\n",
      "469/469 - 4s - 9ms/step - acc: 0.9743 - loss: 0.0827\n",
      "Epoch 3/5\n",
      "469/469 - 4s - 9ms/step - acc: 0.9828 - loss: 0.0547\n",
      "Epoch 4/5\n",
      "469/469 - 4s - 9ms/step - acc: 0.9882 - loss: 0.0374\n",
      "Epoch 5/5\n",
      "469/469 - 4s - 8ms/step - acc: 0.9915 - loss: 0.0268\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9764 - loss: 0.0906\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9941487377616128\n",
      "Recall Score: 0.9941166666666666\n",
      "F1 Score 0.9941172239826642\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9799540483330899\n",
      "Recall Score: 0.9798\n",
      "F1 Score 0.9798104518745012\n",
      "Epoch 1/5\n",
      "235/235 - 4s - 19ms/step - acc: 0.9242 - loss: 0.2606\n",
      "Epoch 2/5\n",
      "235/235 - 3s - 14ms/step - acc: 0.9728 - loss: 0.0905\n",
      "Epoch 3/5\n",
      "235/235 - 3s - 13ms/step - acc: 0.9824 - loss: 0.0571\n",
      "Epoch 4/5\n",
      "235/235 - 3s - 14ms/step - acc: 0.9877 - loss: 0.0399\n",
      "Epoch 5/5\n",
      "235/235 - 3s - 13ms/step - acc: 0.9919 - loss: 0.0263\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9773 - loss: 0.0883\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9920651615802206\n",
      "Recall Score: 0.992\n",
      "F1 Score 0.991993113240206\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9786641201579425\n",
      "Recall Score: 0.9785\n",
      "F1 Score 0.978492027121763\n",
      "Epoch 1/5\n",
      "235/235 - 4s - 16ms/step - acc: 0.9080 - loss: 0.2978\n",
      "Epoch 2/5\n",
      "235/235 - 3s - 13ms/step - acc: 0.9698 - loss: 0.0990\n",
      "Epoch 3/5\n",
      "235/235 - 3s - 13ms/step - acc: 0.9801 - loss: 0.0644\n",
      "Epoch 4/5\n",
      "235/235 - 5s - 22ms/step - acc: 0.9859 - loss: 0.0435\n",
      "Epoch 5/5\n",
      "235/235 - 3s - 12ms/step - acc: 0.9900 - loss: 0.0310\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9759 - loss: 0.0828\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9907913838004486\n",
      "Recall Score: 0.99055\n",
      "F1 Score 0.9905843758416226\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9783144124664165\n",
      "Recall Score: 0.9777\n",
      "F1 Score 0.9778041345757472\n",
      "Epoch 1/10\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9437 - loss: 0.1838\n",
      "Epoch 2/10\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9755 - loss: 0.0792\n",
      "Epoch 3/10\n",
      "1875/1875 - 14s - 7ms/step - acc: 0.9823 - loss: 0.0574\n",
      "Epoch 4/10\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9871 - loss: 0.0410\n",
      "Epoch 5/10\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9887 - loss: 0.0353\n",
      "Epoch 6/10\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9911 - loss: 0.0289\n",
      "Epoch 7/10\n",
      "1875/1875 - 14s - 7ms/step - acc: 0.9923 - loss: 0.0250\n",
      "Epoch 8/10\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9931 - loss: 0.0214\n",
      "Epoch 9/10\n",
      "1875/1875 - 14s - 7ms/step - acc: 0.9935 - loss: 0.0209\n",
      "Epoch 10/10\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9943 - loss: 0.0186\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9726 - loss: 0.1397\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9913534940323443\n",
      "Recall Score: 0.99115\n",
      "F1 Score 0.9911618040966993\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9740641041954635\n",
      "Recall Score: 0.9737\n",
      "F1 Score 0.973691835207439\n",
      "Epoch 1/10\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9426 - loss: 0.1869\n",
      "Epoch 2/10\n",
      "1875/1875 - 12s - 7ms/step - acc: 0.9756 - loss: 0.0841\n",
      "Epoch 3/10\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9829 - loss: 0.0602\n",
      "Epoch 4/10\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9874 - loss: 0.0452\n",
      "Epoch 5/10\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9899 - loss: 0.0367\n",
      "Epoch 6/10\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9924 - loss: 0.0307\n",
      "Epoch 7/10\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9935 - loss: 0.0232\n",
      "Epoch 8/10\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9949 - loss: 0.0188\n",
      "Epoch 9/10\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9955 - loss: 0.0166\n",
      "Epoch 10/10\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9966 - loss: 0.0127\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9775 - loss: 0.1631\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9968052966721425\n",
      "Recall Score: 0.9968\n",
      "F1 Score 0.9968003681252365\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9818409546566065\n",
      "Recall Score: 0.9818\n",
      "F1 Score 0.9818059632818548\n",
      "Epoch 1/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9418 - loss: 0.1940\n",
      "Epoch 2/10\n",
      "938/938 - 9s - 10ms/step - acc: 0.9768 - loss: 0.0760\n",
      "Epoch 3/10\n",
      "938/938 - 8s - 8ms/step - acc: 0.9834 - loss: 0.0525\n",
      "Epoch 4/10\n",
      "938/938 - 7s - 8ms/step - acc: 0.9879 - loss: 0.0379\n",
      "Epoch 5/10\n",
      "938/938 - 7s - 8ms/step - acc: 0.9892 - loss: 0.0330\n",
      "Epoch 6/10\n",
      "938/938 - 7s - 8ms/step - acc: 0.9914 - loss: 0.0253\n",
      "Epoch 7/10\n",
      "938/938 - 7s - 8ms/step - acc: 0.9934 - loss: 0.0199\n",
      "Epoch 8/10\n",
      "938/938 - 8s - 8ms/step - acc: 0.9934 - loss: 0.0196\n",
      "Epoch 9/10\n",
      "938/938 - 8s - 8ms/step - acc: 0.9943 - loss: 0.0184\n",
      "Epoch 10/10\n",
      "938/938 - 8s - 8ms/step - acc: 0.9960 - loss: 0.0134\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9745 - loss: 0.1436\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9960446103035576\n",
      "Recall Score: 0.9960333333333333\n",
      "F1 Score 0.996034064030169\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9786915752084385\n",
      "Recall Score: 0.9786\n",
      "F1 Score 0.9786019475076945\n",
      "Epoch 1/10\n",
      "938/938 - 8s - 8ms/step - acc: 0.9392 - loss: 0.1972\n",
      "Epoch 2/10\n",
      "938/938 - 7s - 8ms/step - acc: 0.9758 - loss: 0.0793\n",
      "Epoch 3/10\n",
      "938/938 - 7s - 7ms/step - acc: 0.9838 - loss: 0.0539\n",
      "Epoch 4/10\n",
      "938/938 - 6s - 6ms/step - acc: 0.9889 - loss: 0.0371\n",
      "Epoch 5/10\n",
      "938/938 - 6s - 6ms/step - acc: 0.9912 - loss: 0.0287\n",
      "Epoch 6/10\n",
      "938/938 - 6s - 7ms/step - acc: 0.9931 - loss: 0.0218\n",
      "Epoch 7/10\n",
      "938/938 - 6s - 6ms/step - acc: 0.9949 - loss: 0.0174\n",
      "Epoch 8/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9959 - loss: 0.0140\n",
      "Epoch 9/10\n",
      "938/938 - 6s - 7ms/step - acc: 0.9964 - loss: 0.0111\n",
      "Epoch 10/10\n",
      "938/938 - 6s - 6ms/step - acc: 0.9970 - loss: 0.0096\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9813 - loss: 0.1034\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9982040391085341\n",
      "Recall Score: 0.9982\n",
      "F1 Score 0.9982002054306897\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9852231728124574\n",
      "Recall Score: 0.9852\n",
      "F1 Score 0.9852007147758053\n",
      "Epoch 1/10\n",
      "469/469 - 6s - 13ms/step - acc: 0.9364 - loss: 0.2154\n",
      "Epoch 2/10\n",
      "469/469 - 10s - 22ms/step - acc: 0.9754 - loss: 0.0791\n",
      "Epoch 3/10\n",
      "469/469 - 5s - 10ms/step - acc: 0.9830 - loss: 0.0529\n",
      "Epoch 4/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9884 - loss: 0.0357\n",
      "Epoch 5/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9904 - loss: 0.0284\n",
      "Epoch 6/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9926 - loss: 0.0216\n",
      "Epoch 7/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9935 - loss: 0.0197\n",
      "Epoch 8/10\n",
      "469/469 - 5s - 11ms/step - acc: 0.9942 - loss: 0.0171\n",
      "Epoch 9/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9961 - loss: 0.0124\n",
      "Epoch 10/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9951 - loss: 0.0141\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9807 - loss: 0.0832\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9979689982048424\n",
      "Recall Score: 0.9979666666666667\n",
      "F1 Score 0.9979669171049297\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9831392590807781\n",
      "Recall Score: 0.9831\n",
      "F1 Score 0.9831057788536965\n",
      "Epoch 1/10\n",
      "469/469 - 5s - 11ms/step - acc: 0.9299 - loss: 0.2280\n",
      "Epoch 2/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9733 - loss: 0.0839\n",
      "Epoch 3/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9834 - loss: 0.0542\n",
      "Epoch 4/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9890 - loss: 0.0367\n",
      "Epoch 5/10\n",
      "469/469 - 4s - 9ms/step - acc: 0.9910 - loss: 0.0280\n",
      "Epoch 6/10\n",
      "469/469 - 3s - 7ms/step - acc: 0.9937 - loss: 0.0189\n",
      "Epoch 7/10\n",
      "469/469 - 3s - 7ms/step - acc: 0.9953 - loss: 0.0154\n",
      "Epoch 8/10\n",
      "469/469 - 3s - 7ms/step - acc: 0.9961 - loss: 0.0117\n",
      "Epoch 9/10\n",
      "469/469 - 3s - 7ms/step - acc: 0.9968 - loss: 0.0094\n",
      "Epoch 10/10\n",
      "469/469 - 4s - 8ms/step - acc: 0.9976 - loss: 0.0078\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9799 - loss: 0.1032\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9982869409971377\n",
      "Recall Score: 0.9982833333333333\n",
      "F1 Score 0.9982832600407908\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9823573825542284\n",
      "Recall Score: 0.9823\n",
      "F1 Score 0.9822905216824729\n",
      "Epoch 1/10\n",
      "235/235 - 4s - 18ms/step - acc: 0.9258 - loss: 0.2583\n",
      "Epoch 2/10\n",
      "235/235 - 3s - 14ms/step - acc: 0.9731 - loss: 0.0903\n",
      "Epoch 3/10\n",
      "235/235 - 3s - 13ms/step - acc: 0.9818 - loss: 0.0595\n",
      "Epoch 4/10\n",
      "235/235 - 3s - 14ms/step - acc: 0.9872 - loss: 0.0391\n",
      "Epoch 5/10\n",
      "235/235 - 3s - 13ms/step - acc: 0.9919 - loss: 0.0256\n",
      "Epoch 6/10\n",
      "235/235 - 3s - 13ms/step - acc: 0.9939 - loss: 0.0189\n",
      "Epoch 7/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9957 - loss: 0.0137\n",
      "Epoch 8/10\n",
      "235/235 - 3s - 12ms/step - acc: 0.9953 - loss: 0.0142\n",
      "Epoch 9/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9949 - loss: 0.0141\n",
      "Epoch 10/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9969 - loss: 0.0093\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9745 - loss: 0.1065\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.997230447777931\n",
      "Recall Score: 0.9972166666666666\n",
      "F1 Score 0.9972168015426072\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9785743752624818\n",
      "Recall Score: 0.9784\n",
      "F1 Score 0.9784011201488299\n",
      "Epoch 1/10\n",
      "235/235 - 4s - 15ms/step - acc: 0.9107 - loss: 0.2941\n",
      "Epoch 2/10\n",
      "235/235 - 3s - 12ms/step - acc: 0.9689 - loss: 0.1006\n",
      "Epoch 3/10\n",
      "235/235 - 3s - 12ms/step - acc: 0.9798 - loss: 0.0645\n",
      "Epoch 4/10\n",
      "235/235 - 3s - 12ms/step - acc: 0.9848 - loss: 0.0458\n",
      "Epoch 5/10\n",
      "235/235 - 3s - 12ms/step - acc: 0.9899 - loss: 0.0320\n",
      "Epoch 6/10\n",
      "235/235 - 3s - 14ms/step - acc: 0.9926 - loss: 0.0234\n",
      "Epoch 7/10\n",
      "235/235 - 4s - 17ms/step - acc: 0.9944 - loss: 0.0180\n",
      "Epoch 8/10\n",
      "235/235 - 2s - 9ms/step - acc: 0.9962 - loss: 0.0117\n",
      "Epoch 9/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9965 - loss: 0.0108\n",
      "Epoch 10/10\n",
      "235/235 - 2s - 10ms/step - acc: 0.9980 - loss: 0.0066\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9771 - loss: 0.0894\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9983566046916613\n",
      "Recall Score: 0.99835\n",
      "F1 Score 0.9983501400076584\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9825941527043497\n",
      "Recall Score: 0.9825\n",
      "F1 Score 0.9825023319871034\n",
      "Epoch 1/20\n",
      "1875/1875 - 18s - 10ms/step - acc: 0.9452 - loss: 0.1824\n",
      "Epoch 2/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9757 - loss: 0.0780\n",
      "Epoch 3/20\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9821 - loss: 0.0563\n",
      "Epoch 4/20\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9872 - loss: 0.0416\n",
      "Epoch 5/20\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9889 - loss: 0.0353\n",
      "Epoch 6/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9908 - loss: 0.0290\n",
      "Epoch 7/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9919 - loss: 0.0268\n",
      "Epoch 8/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9935 - loss: 0.0214\n",
      "Epoch 9/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9939 - loss: 0.0203\n",
      "Epoch 10/20\n",
      "1875/1875 - 14s - 8ms/step - acc: 0.9934 - loss: 0.0217\n",
      "Epoch 11/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9949 - loss: 0.0173\n",
      "Epoch 12/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9952 - loss: 0.0165\n",
      "Epoch 13/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9956 - loss: 0.0164\n",
      "Epoch 14/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9956 - loss: 0.0144\n",
      "Epoch 15/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9962 - loss: 0.0138\n",
      "Epoch 16/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9961 - loss: 0.0130\n",
      "Epoch 17/20\n",
      "1875/1875 - 20s - 11ms/step - acc: 0.9967 - loss: 0.0133\n",
      "Epoch 18/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9960 - loss: 0.0145\n",
      "Epoch 19/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9966 - loss: 0.0125\n",
      "Epoch 20/20\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9970 - loss: 0.0116\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9774 - loss: 0.1936\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9977196761370051\n",
      "Recall Score: 0.9977166666666667\n",
      "F1 Score 0.997716767169853\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9810835087224854\n",
      "Recall Score: 0.981\n",
      "F1 Score 0.9810064639098993\n",
      "Epoch 1/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9426 - loss: 0.1896\n",
      "Epoch 2/20\n",
      "1875/1875 - 13s - 7ms/step - acc: 0.9761 - loss: 0.0854\n",
      "Epoch 3/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9826 - loss: 0.0640\n",
      "Epoch 4/20\n",
      "1875/1875 - 11s - 6ms/step - acc: 0.9873 - loss: 0.0479\n",
      "Epoch 5/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9899 - loss: 0.0360\n",
      "Epoch 6/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9920 - loss: 0.0296\n",
      "Epoch 7/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9936 - loss: 0.0228\n",
      "Epoch 8/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9944 - loss: 0.0200\n",
      "Epoch 9/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9955 - loss: 0.0172\n",
      "Epoch 10/20\n",
      "1875/1875 - 20s - 11ms/step - acc: 0.9969 - loss: 0.0123\n",
      "Epoch 11/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9979 - loss: 0.0079\n",
      "Epoch 12/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9981 - loss: 0.0075\n",
      "Epoch 13/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9982 - loss: 0.0060\n",
      "Epoch 14/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9981 - loss: 0.0065\n",
      "Epoch 15/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9987 - loss: 0.0046\n",
      "Epoch 16/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9989 - loss: 0.0029\n",
      "Epoch 17/20\n",
      "1875/1875 - 12s - 6ms/step - acc: 0.9989 - loss: 0.0035\n",
      "Epoch 18/20\n",
      "1875/1875 - 20s - 11ms/step - acc: 0.9993 - loss: 0.0027\n",
      "Epoch 19/20\n",
      "1875/1875 - 20s - 11ms/step - acc: 0.9994 - loss: 0.0021\n",
      "Epoch 20/20\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9997 - loss: 6.8987e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9828 - loss: 0.1610\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9997335302690555\n",
      "Recall Score: 0.9997333333333334\n",
      "F1 Score 0.9997333287211976\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9859057354041056\n",
      "Recall Score: 0.9859\n",
      "F1 Score 0.9858952997825092\n",
      "Epoch 1/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9420 - loss: 0.1939\n",
      "Epoch 2/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9752 - loss: 0.0768\n",
      "Epoch 3/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9826 - loss: 0.0534\n",
      "Epoch 4/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9876 - loss: 0.0370\n",
      "Epoch 5/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9906 - loss: 0.0284\n",
      "Epoch 6/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9912 - loss: 0.0259\n",
      "Epoch 7/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9926 - loss: 0.0216\n",
      "Epoch 8/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9940 - loss: 0.0175\n",
      "Epoch 9/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9947 - loss: 0.0173\n",
      "Epoch 10/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9950 - loss: 0.0153\n",
      "Epoch 11/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9956 - loss: 0.0138\n",
      "Epoch 12/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9968 - loss: 0.0091\n",
      "Epoch 13/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9954 - loss: 0.0162\n",
      "Epoch 14/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9967 - loss: 0.0102\n",
      "Epoch 15/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9966 - loss: 0.0121\n",
      "Epoch 16/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9965 - loss: 0.0102\n",
      "Epoch 17/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9964 - loss: 0.0119\n",
      "Epoch 18/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9978 - loss: 0.0077\n",
      "Epoch 19/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9973 - loss: 0.0091\n",
      "Epoch 20/20\n",
      "938/938 - 8s - 8ms/step - acc: 0.9963 - loss: 0.0119\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9777 - loss: 0.1409\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9975737756232032\n",
      "Recall Score: 0.9975666666666667\n",
      "F1 Score 0.9975671028718839\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9813264973548894\n",
      "Recall Score: 0.9812\n",
      "F1 Score 0.9812216061607122\n",
      "Epoch 1/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9390 - loss: 0.1975\n",
      "Epoch 2/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9755 - loss: 0.0802\n",
      "Epoch 3/20\n",
      "938/938 - 7s - 8ms/step - acc: 0.9835 - loss: 0.0533\n",
      "Epoch 4/20\n",
      "938/938 - 6s - 6ms/step - acc: 0.9880 - loss: 0.0387\n",
      "Epoch 5/20\n",
      "938/938 - 6s - 7ms/step - acc: 0.9918 - loss: 0.0287\n",
      "Epoch 6/20\n",
      "938/938 - 6s - 6ms/step - acc: 0.9929 - loss: 0.0224\n",
      "Epoch 7/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9949 - loss: 0.0165\n",
      "Epoch 8/20\n",
      "938/938 - 6s - 7ms/step - acc: 0.9961 - loss: 0.0136\n",
      "Epoch 9/20\n",
      "938/938 - 6s - 7ms/step - acc: 0.9969 - loss: 0.0102\n",
      "Epoch 10/20\n",
      "938/938 - 6s - 6ms/step - acc: 0.9975 - loss: 0.0079\n",
      "Epoch 11/20\n",
      "938/938 - 6s - 7ms/step - acc: 0.9981 - loss: 0.0062\n",
      "Epoch 12/20\n",
      "938/938 - 6s - 6ms/step - acc: 0.9984 - loss: 0.0052\n",
      "Epoch 13/20\n",
      "938/938 - 6s - 6ms/step - acc: 0.9989 - loss: 0.0033\n",
      "Epoch 14/20\n",
      "938/938 - 6s - 7ms/step - acc: 0.9992 - loss: 0.0023\n",
      "Epoch 15/20\n",
      "938/938 - 6s - 6ms/step - acc: 0.9995 - loss: 0.0016\n",
      "Epoch 16/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9991 - loss: 0.0026\n",
      "Epoch 17/20\n",
      "938/938 - 6s - 7ms/step - acc: 0.9999 - loss: 3.0579e-04\n",
      "Epoch 18/20\n",
      "938/938 - 6s - 7ms/step - acc: 1.0000 - loss: 1.7481e-05\n",
      "Epoch 19/20\n",
      "938/938 - 6s - 7ms/step - acc: 1.0000 - loss: 7.5960e-06\n",
      "Epoch 20/20\n",
      "938/938 - 6s - 7ms/step - acc: 1.0000 - loss: 5.1019e-06\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9832 - loss: 0.1370\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9864052351225712\n",
      "Recall Score: 0.9864\n",
      "F1 Score 0.9863993839006051\n",
      "Epoch 1/20\n",
      "469/469 - 6s - 14ms/step - acc: 0.9363 - loss: 0.2147\n",
      "Epoch 2/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9760 - loss: 0.0796\n",
      "Epoch 3/20\n",
      "469/469 - 5s - 12ms/step - acc: 0.9841 - loss: 0.0504\n",
      "Epoch 4/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9882 - loss: 0.0370\n",
      "Epoch 5/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9914 - loss: 0.0272\n",
      "Epoch 6/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9933 - loss: 0.0210\n",
      "Epoch 7/20\n",
      "469/469 - 5s - 10ms/step - acc: 0.9931 - loss: 0.0199\n",
      "Epoch 8/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9943 - loss: 0.0176\n",
      "Epoch 9/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9955 - loss: 0.0134\n",
      "Epoch 10/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9958 - loss: 0.0131\n",
      "Epoch 11/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9959 - loss: 0.0125\n",
      "Epoch 12/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9965 - loss: 0.0103\n",
      "Epoch 13/20\n",
      "469/469 - 4s - 10ms/step - acc: 0.9962 - loss: 0.0110\n",
      "Epoch 14/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9966 - loss: 0.0100\n",
      "Epoch 15/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9969 - loss: 0.0097\n",
      "Epoch 16/20\n",
      "469/469 - 5s - 10ms/step - acc: 0.9974 - loss: 0.0078\n",
      "Epoch 17/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9974 - loss: 0.0085\n",
      "Epoch 18/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9972 - loss: 0.0081\n",
      "Epoch 19/20\n",
      "469/469 - 5s - 10ms/step - acc: 0.9976 - loss: 0.0078\n",
      "Epoch 20/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9978 - loss: 0.0089\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9784 - loss: 0.1211\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9986559445691361\n",
      "Recall Score: 0.99865\n",
      "F1 Score 0.9986509106986572\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9818725320455923\n",
      "Recall Score: 0.9817\n",
      "F1 Score 0.9817176969676177\n",
      "Epoch 1/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9295 - loss: 0.2285\n",
      "Epoch 2/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9744 - loss: 0.0830\n",
      "Epoch 3/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9834 - loss: 0.0520\n",
      "Epoch 4/20\n",
      "469/469 - 4s - 9ms/step - acc: 0.9887 - loss: 0.0355\n",
      "Epoch 5/20\n",
      "469/469 - 4s - 8ms/step - acc: 0.9918 - loss: 0.0257\n",
      "Epoch 6/20\n",
      "469/469 - 4s - 8ms/step - acc: 0.9938 - loss: 0.0182\n",
      "Epoch 7/20\n",
      "469/469 - 4s - 7ms/step - acc: 0.9955 - loss: 0.0143\n",
      "Epoch 8/20\n",
      "469/469 - 3s - 7ms/step - acc: 0.9965 - loss: 0.0112\n",
      "Epoch 9/20\n",
      "469/469 - 4s - 8ms/step - acc: 0.9976 - loss: 0.0071\n",
      "Epoch 10/20\n",
      "469/469 - 4s - 7ms/step - acc: 0.9978 - loss: 0.0062\n",
      "Epoch 11/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9984 - loss: 0.0052\n",
      "Epoch 12/20\n",
      "469/469 - 4s - 8ms/step - acc: 0.9984 - loss: 0.0050\n",
      "Epoch 13/20\n",
      "469/469 - 4s - 7ms/step - acc: 0.9991 - loss: 0.0026\n",
      "Epoch 14/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9993 - loss: 0.0027\n",
      "Epoch 15/20\n",
      "469/469 - 4s - 8ms/step - acc: 0.9995 - loss: 0.0016\n",
      "Epoch 16/20\n",
      "469/469 - 3s - 7ms/step - acc: 0.9998 - loss: 5.2210e-04\n",
      "Epoch 17/20\n",
      "469/469 - 3s - 7ms/step - acc: 1.0000 - loss: 6.5897e-05\n",
      "Epoch 18/20\n",
      "469/469 - 4s - 8ms/step - acc: 1.0000 - loss: 4.0764e-05\n",
      "Epoch 19/20\n",
      "469/469 - 3s - 7ms/step - acc: 1.0000 - loss: 1.4522e-05\n",
      "Epoch 20/20\n",
      "469/469 - 5s - 11ms/step - acc: 1.0000 - loss: 1.2121e-05\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9836 - loss: 0.1164\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9864004923447078\n",
      "Recall Score: 0.9864\n",
      "F1 Score 0.9863985911563463\n",
      "Epoch 1/20\n",
      "235/235 - 4s - 18ms/step - acc: 0.9244 - loss: 0.2624\n",
      "Epoch 2/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9720 - loss: 0.0928\n",
      "Epoch 3/20\n",
      "235/235 - 5s - 22ms/step - acc: 0.9824 - loss: 0.0568\n",
      "Epoch 4/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9883 - loss: 0.0371\n",
      "Epoch 5/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9921 - loss: 0.0255\n",
      "Epoch 6/20\n",
      "235/235 - 3s - 12ms/step - acc: 0.9938 - loss: 0.0191\n",
      "Epoch 7/20\n",
      "235/235 - 5s - 21ms/step - acc: 0.9956 - loss: 0.0143\n",
      "Epoch 8/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9959 - loss: 0.0130\n",
      "Epoch 9/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9965 - loss: 0.0101\n",
      "Epoch 10/20\n",
      "235/235 - 3s - 12ms/step - acc: 0.9962 - loss: 0.0116\n",
      "Epoch 11/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9974 - loss: 0.0084\n",
      "Epoch 12/20\n",
      "235/235 - 5s - 22ms/step - acc: 0.9977 - loss: 0.0073\n",
      "Epoch 13/20\n",
      "235/235 - 3s - 12ms/step - acc: 0.9947 - loss: 0.0151\n",
      "Epoch 14/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9972 - loss: 0.0085\n",
      "Epoch 15/20\n",
      "235/235 - 5s - 22ms/step - acc: 0.9979 - loss: 0.0067\n",
      "Epoch 16/20\n",
      "235/235 - 3s - 12ms/step - acc: 0.9980 - loss: 0.0065\n",
      "Epoch 17/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9987 - loss: 0.0044\n",
      "Epoch 18/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9970 - loss: 0.0089\n",
      "Epoch 19/20\n",
      "235/235 - 3s - 12ms/step - acc: 0.9980 - loss: 0.0063\n",
      "Epoch 20/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9987 - loss: 0.0041\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9776 - loss: 0.1206\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9985693184346943\n",
      "Recall Score: 0.9985666666666667\n",
      "F1 Score 0.9985660966252159\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9807783719177432\n",
      "Recall Score: 0.9807\n",
      "F1 Score 0.9807003275455938\n",
      "Epoch 1/20\n",
      "235/235 - 4s - 16ms/step - acc: 0.9109 - loss: 0.2908\n",
      "Epoch 2/20\n",
      "235/235 - 5s - 22ms/step - acc: 0.9696 - loss: 0.0998\n",
      "Epoch 3/20\n",
      "235/235 - 3s - 13ms/step - acc: 0.9802 - loss: 0.0631\n",
      "Epoch 4/20\n",
      "235/235 - 3s - 12ms/step - acc: 0.9860 - loss: 0.0439\n",
      "Epoch 5/20\n",
      "235/235 - 3s - 13ms/step - acc: 0.9905 - loss: 0.0314\n",
      "Epoch 6/20\n",
      "235/235 - 3s - 12ms/step - acc: 0.9928 - loss: 0.0230\n",
      "Epoch 7/20\n",
      "235/235 - 2s - 9ms/step - acc: 0.9949 - loss: 0.0167\n",
      "Epoch 8/20\n",
      "235/235 - 2s - 9ms/step - acc: 0.9958 - loss: 0.0129\n",
      "Epoch 9/20\n",
      "235/235 - 2s - 10ms/step - acc: 0.9967 - loss: 0.0102\n",
      "Epoch 10/20\n",
      "235/235 - 2s - 9ms/step - acc: 0.9976 - loss: 0.0076\n",
      "Epoch 11/20\n",
      "235/235 - 2s - 9ms/step - acc: 0.9984 - loss: 0.0055\n",
      "Epoch 12/20\n",
      "235/235 - 2s - 10ms/step - acc: 0.9989 - loss: 0.0036\n",
      "Epoch 13/20\n",
      "235/235 - 2s - 9ms/step - acc: 0.9991 - loss: 0.0028\n",
      "Epoch 14/20\n",
      "235/235 - 2s - 9ms/step - acc: 0.9993 - loss: 0.0024\n",
      "Epoch 15/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9998 - loss: 7.7949e-04\n",
      "Epoch 16/20\n",
      "235/235 - 2s - 9ms/step - acc: 1.0000 - loss: 2.3272e-04\n",
      "Epoch 17/20\n",
      "235/235 - 2s - 9ms/step - acc: 1.0000 - loss: 9.5674e-05\n",
      "Epoch 18/20\n",
      "235/235 - 2s - 10ms/step - acc: 1.0000 - loss: 6.4460e-05\n",
      "Epoch 19/20\n",
      "235/235 - 2s - 9ms/step - acc: 1.0000 - loss: 5.4636e-05\n",
      "Epoch 20/20\n",
      "235/235 - 2s - 9ms/step - acc: 1.0000 - loss: 4.3918e-05\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9819 - loss: 0.0945\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點512, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9853079262876843\n",
      "Recall Score: 0.9853\n",
      "F1 Score 0.9852973864293668\n",
      "Epoch 1/5\n",
      "1875/1875 - 45s - 24ms/step - acc: 0.9457 - loss: 0.1786\n",
      "Epoch 2/5\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9744 - loss: 0.0835\n",
      "Epoch 3/5\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9819 - loss: 0.0567\n",
      "Epoch 4/5\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9862 - loss: 0.0449\n",
      "Epoch 5/5\n",
      "1875/1875 - 39s - 21ms/step - acc: 0.9886 - loss: 0.0374\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9697 - loss: 0.1274\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9877620929356461\n",
      "Recall Score: 0.9875333333333334\n",
      "F1 Score 0.987531454056248\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9751003351228197\n",
      "Recall Score: 0.9746\n",
      "F1 Score 0.9746165544561248\n",
      "Epoch 1/5\n",
      "1875/1875 - 38s - 20ms/step - acc: 0.9428 - loss: 0.1899\n",
      "Epoch 2/5\n",
      "1875/1875 - 35s - 19ms/step - acc: 0.9758 - loss: 0.0844\n",
      "Epoch 3/5\n",
      "1875/1875 - 31s - 17ms/step - acc: 0.9840 - loss: 0.0604\n",
      "Epoch 4/5\n",
      "1875/1875 - 31s - 17ms/step - acc: 0.9872 - loss: 0.0454\n",
      "Epoch 5/5\n",
      "1875/1875 - 31s - 17ms/step - acc: 0.9904 - loss: 0.0361\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9759 - loss: 0.1361\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9934013312694238\n",
      "Recall Score: 0.9933333333333333\n",
      "F1 Score 0.9933425912551604\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9793511507587411\n",
      "Recall Score: 0.9791\n",
      "F1 Score 0.9791234825592932\n",
      "Epoch 1/5\n",
      "938/938 - 27s - 29ms/step - acc: 0.9456 - loss: 0.1777\n",
      "Epoch 2/5\n",
      "938/938 - 24s - 26ms/step - acc: 0.9765 - loss: 0.0764\n",
      "Epoch 3/5\n",
      "938/938 - 21s - 22ms/step - acc: 0.9836 - loss: 0.0522\n",
      "Epoch 4/5\n",
      "938/938 - 21s - 23ms/step - acc: 0.9878 - loss: 0.0386\n",
      "Epoch 5/5\n",
      "938/938 - 21s - 23ms/step - acc: 0.9903 - loss: 0.0300\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9743 - loss: 0.1029\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9923851252314625\n",
      "Recall Score: 0.9923333333333333\n",
      "F1 Score 0.9923334836120353\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9780013348552548\n",
      "Recall Score: 0.9778\n",
      "F1 Score 0.9777899248898099\n",
      "Epoch 1/5\n",
      "938/938 - 21s - 23ms/step - acc: 0.9405 - loss: 0.1916\n",
      "Epoch 2/5\n",
      "938/938 - 21s - 22ms/step - acc: 0.9771 - loss: 0.0757\n",
      "Epoch 3/5\n",
      "938/938 - 17s - 18ms/step - acc: 0.9848 - loss: 0.0504\n",
      "Epoch 4/5\n",
      "938/938 - 21s - 22ms/step - acc: 0.9893 - loss: 0.0352\n",
      "Epoch 5/5\n",
      "938/938 - 17s - 19ms/step - acc: 0.9918 - loss: 0.0267\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9745 - loss: 0.1010\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9927685117881839\n",
      "Recall Score: 0.9927333333333334\n",
      "F1 Score 0.9927287194391167\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9784880466756154\n",
      "Recall Score: 0.9784\n",
      "F1 Score 0.9783815378588517\n",
      "Epoch 1/5\n",
      "469/469 - 16s - 34ms/step - acc: 0.9426 - loss: 0.1897\n",
      "Epoch 2/5\n",
      "469/469 - 15s - 32ms/step - acc: 0.9773 - loss: 0.0720\n",
      "Epoch 3/5\n",
      "469/469 - 14s - 30ms/step - acc: 0.9854 - loss: 0.0452\n",
      "Epoch 4/5\n",
      "469/469 - 12s - 26ms/step - acc: 0.9886 - loss: 0.0359\n",
      "Epoch 5/5\n",
      "469/469 - 12s - 25ms/step - acc: 0.9908 - loss: 0.0277\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9779 - loss: 0.0885\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9943615749377899\n",
      "Recall Score: 0.99435\n",
      "F1 Score 0.9943495561028981\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9813478925742023\n",
      "Recall Score: 0.9813\n",
      "F1 Score 0.9813037603913739\n",
      "Epoch 1/5\n",
      "469/469 - 13s - 27ms/step - acc: 0.9315 - loss: 0.2186\n",
      "Epoch 2/5\n",
      "469/469 - 12s - 26ms/step - acc: 0.9761 - loss: 0.0782\n",
      "Epoch 3/5\n",
      "469/469 - 12s - 26ms/step - acc: 0.9843 - loss: 0.0482\n",
      "Epoch 4/5\n",
      "469/469 - 10s - 21ms/step - acc: 0.9901 - loss: 0.0318\n",
      "Epoch 5/5\n",
      "469/469 - 10s - 21ms/step - acc: 0.9929 - loss: 0.0217\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9721 - loss: 0.1020\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9919677839579996\n",
      "Recall Score: 0.9918333333333333\n",
      "F1 Score 0.9918470947921146\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9778158080132636\n",
      "Recall Score: 0.9775\n",
      "F1 Score 0.9775399468605661\n",
      "Epoch 1/5\n",
      "235/235 - 11s - 45ms/step - acc: 0.9330 - loss: 0.2210\n",
      "Epoch 2/5\n",
      "235/235 - 9s - 40ms/step - acc: 0.9770 - loss: 0.0752\n",
      "Epoch 3/5\n",
      "235/235 - 9s - 40ms/step - acc: 0.9843 - loss: 0.0477\n",
      "Epoch 4/5\n",
      "235/235 - 9s - 39ms/step - acc: 0.9905 - loss: 0.0293\n",
      "Epoch 5/5\n",
      "235/235 - 7s - 32ms/step - acc: 0.9921 - loss: 0.0235\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9788 - loss: 0.0730\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9951556805603768\n",
      "Recall Score: 0.9951333333333333\n",
      "F1 Score 0.9951354017746437\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9811418010614571\n",
      "Recall Score: 0.981\n",
      "F1 Score 0.981012069542681\n",
      "Epoch 1/5\n",
      "235/235 - 9s - 37ms/step - acc: 0.9150 - loss: 0.2718\n",
      "Epoch 2/5\n",
      "235/235 - 8s - 34ms/step - acc: 0.9729 - loss: 0.0887\n",
      "Epoch 3/5\n",
      "235/235 - 8s - 33ms/step - acc: 0.9830 - loss: 0.0554\n",
      "Epoch 4/5\n",
      "235/235 - 8s - 34ms/step - acc: 0.9885 - loss: 0.0360\n",
      "Epoch 5/5\n",
      "235/235 - 7s - 29ms/step - acc: 0.9914 - loss: 0.0269\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9575 - loss: 0.1357\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.983406195916407\n",
      "Recall Score: 0.9817166666666667\n",
      "F1 Score 0.9819420665354168\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9682776268878405\n",
      "Recall Score: 0.9654\n",
      "F1 Score 0.9657762720633436\n",
      "Epoch 1/10\n",
      "1875/1875 - 48s - 26ms/step - acc: 0.9457 - loss: 0.1784\n",
      "Epoch 2/10\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9741 - loss: 0.0814\n",
      "Epoch 3/10\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9811 - loss: 0.0583\n",
      "Epoch 4/10\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9856 - loss: 0.0465\n",
      "Epoch 5/10\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9886 - loss: 0.0369\n",
      "Epoch 6/10\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9903 - loss: 0.0312\n",
      "Epoch 7/10\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9913 - loss: 0.0288\n",
      "Epoch 8/10\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9924 - loss: 0.0261\n",
      "Epoch 9/10\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9939 - loss: 0.0204\n",
      "Epoch 10/10\n",
      "1875/1875 - 40s - 21ms/step - acc: 0.9934 - loss: 0.0243\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9742 - loss: 0.1331\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9956362785719967\n",
      "Recall Score: 0.9956166666666667\n",
      "F1 Score 0.9956135953084434\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9792235435504877\n",
      "Recall Score: 0.9791\n",
      "F1 Score 0.9790765220964754\n",
      "Epoch 1/10\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9437 - loss: 0.1855\n",
      "Epoch 2/10\n",
      "1875/1875 - 33s - 18ms/step - acc: 0.9766 - loss: 0.0858\n",
      "Epoch 3/10\n",
      "1875/1875 - 31s - 17ms/step - acc: 0.9837 - loss: 0.0598\n",
      "Epoch 4/10\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9877 - loss: 0.0462\n",
      "Epoch 5/10\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9911 - loss: 0.0349\n",
      "Epoch 6/10\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9922 - loss: 0.0268\n",
      "Epoch 7/10\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9945 - loss: 0.0204\n",
      "Epoch 8/10\n",
      "1875/1875 - 36s - 19ms/step - acc: 0.9957 - loss: 0.0159\n",
      "Epoch 9/10\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9962 - loss: 0.0138\n",
      "Epoch 10/10\n",
      "1875/1875 - 74s - 40ms/step - acc: 0.9973 - loss: 0.0097\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9782 - loss: 0.1419\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.998485147366653\n",
      "Recall Score: 0.9984833333333333\n",
      "F1 Score 0.9984832146562024\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9815189847428303\n",
      "Recall Score: 0.9815\n",
      "F1 Score 0.9814964801993381\n",
      "Epoch 1/10\n",
      "938/938 - 26s - 27ms/step - acc: 0.9446 - loss: 0.1793\n",
      "Epoch 2/10\n",
      "938/938 - 24s - 26ms/step - acc: 0.9763 - loss: 0.0751\n",
      "Epoch 3/10\n",
      "938/938 - 26s - 28ms/step - acc: 0.9839 - loss: 0.0507\n",
      "Epoch 4/10\n",
      "938/938 - 28s - 30ms/step - acc: 0.9873 - loss: 0.0385\n",
      "Epoch 5/10\n",
      "938/938 - 33s - 36ms/step - acc: 0.9893 - loss: 0.0327\n",
      "Epoch 6/10\n",
      "938/938 - 35s - 37ms/step - acc: 0.9919 - loss: 0.0240\n",
      "Epoch 7/10\n",
      "938/938 - 30s - 32ms/step - acc: 0.9927 - loss: 0.0232\n",
      "Epoch 8/10\n",
      "938/938 - 39s - 42ms/step - acc: 0.9933 - loss: 0.0216\n",
      "Epoch 9/10\n",
      "938/938 - 23s - 24ms/step - acc: 0.9938 - loss: 0.0199\n",
      "Epoch 10/10\n",
      "938/938 - 22s - 24ms/step - acc: 0.9955 - loss: 0.0148\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9724 - loss: 0.1589\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9921013098844335\n",
      "Recall Score: 0.99185\n",
      "F1 Score 0.9918352812855921\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9769974888919897\n",
      "Recall Score: 0.9763\n",
      "F1 Score 0.9762851553835122\n",
      "Epoch 1/10\n",
      "938/938 - 22s - 23ms/step - acc: 0.9418 - loss: 0.1880\n",
      "Epoch 2/10\n",
      "938/938 - 21s - 23ms/step - acc: 0.9774 - loss: 0.0761\n",
      "Epoch 3/10\n",
      "938/938 - 18s - 19ms/step - acc: 0.9853 - loss: 0.0495\n",
      "Epoch 4/10\n",
      "938/938 - 17s - 18ms/step - acc: 0.9893 - loss: 0.0354\n",
      "Epoch 5/10\n",
      "938/938 - 17s - 19ms/step - acc: 0.9920 - loss: 0.0266\n",
      "Epoch 6/10\n",
      "938/938 - 17s - 18ms/step - acc: 0.9941 - loss: 0.0197\n",
      "Epoch 7/10\n",
      "938/938 - 18s - 19ms/step - acc: 0.9959 - loss: 0.0136\n",
      "Epoch 8/10\n",
      "938/938 - 17s - 19ms/step - acc: 0.9965 - loss: 0.0110\n",
      "Epoch 9/10\n",
      "938/938 - 18s - 19ms/step - acc: 0.9976 - loss: 0.0081\n",
      "Epoch 10/10\n",
      "938/938 - 17s - 19ms/step - acc: 0.9979 - loss: 0.0072\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9779 - loss: 0.1217\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9988038451330445\n",
      "Recall Score: 0.9988\n",
      "F1 Score 0.9988004062546648\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9828429471586276\n",
      "Recall Score: 0.9827\n",
      "F1 Score 0.9827112539947066\n",
      "Epoch 1/10\n",
      "469/469 - 16s - 34ms/step - acc: 0.9425 - loss: 0.1940\n",
      "Epoch 2/10\n",
      "469/469 - 15s - 32ms/step - acc: 0.9768 - loss: 0.0733\n",
      "Epoch 3/10\n",
      "469/469 - 14s - 31ms/step - acc: 0.9852 - loss: 0.0460\n",
      "Epoch 4/10\n",
      "469/469 - 12s - 26ms/step - acc: 0.9893 - loss: 0.0345\n",
      "Epoch 5/10\n",
      "469/469 - 12s - 25ms/step - acc: 0.9905 - loss: 0.0278\n",
      "Epoch 6/10\n",
      "469/469 - 12s - 26ms/step - acc: 0.9925 - loss: 0.0213\n",
      "Epoch 7/10\n",
      "469/469 - 12s - 26ms/step - acc: 0.9922 - loss: 0.0238\n",
      "Epoch 8/10\n",
      "469/469 - 12s - 26ms/step - acc: 0.9951 - loss: 0.0155\n",
      "Epoch 9/10\n",
      "469/469 - 12s - 26ms/step - acc: 0.9955 - loss: 0.0138\n",
      "Epoch 10/10\n",
      "469/469 - 12s - 26ms/step - acc: 0.9951 - loss: 0.0152\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9780 - loss: 0.0932\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9945179586940771\n",
      "Recall Score: 0.9944666666666667\n",
      "F1 Score 0.9944578260104199\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9809958552654146\n",
      "Recall Score: 0.9809\n",
      "F1 Score 0.9808587076854789\n",
      "Epoch 1/10\n",
      "469/469 - 13s - 27ms/step - acc: 0.9312 - loss: 0.2178\n",
      "Epoch 2/10\n",
      "469/469 - 12s - 26ms/step - acc: 0.9753 - loss: 0.0787\n",
      "Epoch 3/10\n",
      "469/469 - 12s - 25ms/step - acc: 0.9841 - loss: 0.0489\n",
      "Epoch 4/10\n",
      "469/469 - 10s - 22ms/step - acc: 0.9891 - loss: 0.0333\n",
      "Epoch 5/10\n",
      "469/469 - 10s - 21ms/step - acc: 0.9923 - loss: 0.0235\n",
      "Epoch 6/10\n",
      "469/469 - 10s - 21ms/step - acc: 0.9947 - loss: 0.0162\n",
      "Epoch 7/10\n",
      "469/469 - 10s - 21ms/step - acc: 0.9964 - loss: 0.0121\n",
      "Epoch 8/10\n",
      "469/469 - 10s - 21ms/step - acc: 0.9966 - loss: 0.0100\n",
      "Epoch 9/10\n",
      "469/469 - 10s - 21ms/step - acc: 0.9976 - loss: 0.0072\n",
      "Epoch 10/10\n",
      "469/469 - 10s - 21ms/step - acc: 0.9979 - loss: 0.0065\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9792 - loss: 0.1110\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.998505777369287\n",
      "Recall Score: 0.9985\n",
      "F1 Score 0.998500818197995\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9830118184571256\n",
      "Recall Score: 0.9829\n",
      "F1 Score 0.9829163948055537\n",
      "Epoch 1/10\n",
      "235/235 - 11s - 45ms/step - acc: 0.9343 - loss: 0.2200\n",
      "Epoch 2/10\n",
      "235/235 - 9s - 40ms/step - acc: 0.9758 - loss: 0.0778\n",
      "Epoch 3/10\n",
      "235/235 - 9s - 40ms/step - acc: 0.9851 - loss: 0.0465\n",
      "Epoch 4/10\n",
      "235/235 - 10s - 40ms/step - acc: 0.9897 - loss: 0.0330\n",
      "Epoch 5/10\n",
      "235/235 - 8s - 32ms/step - acc: 0.9931 - loss: 0.0219\n",
      "Epoch 6/10\n",
      "235/235 - 8s - 33ms/step - acc: 0.9948 - loss: 0.0163\n",
      "Epoch 7/10\n",
      "235/235 - 7s - 32ms/step - acc: 0.9941 - loss: 0.0176\n",
      "Epoch 8/10\n",
      "235/235 - 7s - 32ms/step - acc: 0.9956 - loss: 0.0138\n",
      "Epoch 9/10\n",
      "235/235 - 8s - 32ms/step - acc: 0.9958 - loss: 0.0128\n",
      "Epoch 10/10\n",
      "235/235 - 8s - 33ms/step - acc: 0.9961 - loss: 0.0125\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9805 - loss: 0.0815\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9982515377186076\n",
      "Recall Score: 0.99825\n",
      "F1 Score 0.9982502152504672\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9842341250316715\n",
      "Recall Score: 0.9842\n",
      "F1 Score 0.9842012138086073\n",
      "Epoch 1/10\n",
      "235/235 - 9s - 37ms/step - acc: 0.9151 - loss: 0.2738\n",
      "Epoch 2/10\n",
      "235/235 - 8s - 34ms/step - acc: 0.9728 - loss: 0.0891\n",
      "Epoch 3/10\n",
      "235/235 - 8s - 33ms/step - acc: 0.9824 - loss: 0.0566\n",
      "Epoch 4/10\n",
      "235/235 - 8s - 34ms/step - acc: 0.9876 - loss: 0.0377\n",
      "Epoch 5/10\n",
      "235/235 - 7s - 31ms/step - acc: 0.9916 - loss: 0.0259\n",
      "Epoch 6/10\n",
      "235/235 - 6s - 27ms/step - acc: 0.9940 - loss: 0.0192\n",
      "Epoch 7/10\n",
      "235/235 - 6s - 27ms/step - acc: 0.9953 - loss: 0.0145\n",
      "Epoch 8/10\n",
      "235/235 - 6s - 28ms/step - acc: 0.9975 - loss: 0.0087\n",
      "Epoch 9/10\n",
      "235/235 - 6s - 26ms/step - acc: 0.9979 - loss: 0.0069\n",
      "Epoch 10/10\n",
      "235/235 - 6s - 27ms/step - acc: 0.9983 - loss: 0.0053\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9792 - loss: 0.0833\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.998654572551217\n",
      "Recall Score: 0.99865\n",
      "F1 Score 0.9986502393969766\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.983953165655569\n",
      "Recall Score: 0.9839\n",
      "F1 Score 0.9839032145634881\n",
      "Epoch 1/20\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9449 - loss: 0.1796\n",
      "Epoch 2/20\n",
      "1875/1875 - 75s - 40ms/step - acc: 0.9736 - loss: 0.0836\n",
      "Epoch 3/20\n",
      "1875/1875 - 40s - 22ms/step - acc: 0.9818 - loss: 0.0587\n",
      "Epoch 4/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9859 - loss: 0.0451\n",
      "Epoch 5/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9888 - loss: 0.0358\n",
      "Epoch 6/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9897 - loss: 0.0346\n",
      "Epoch 7/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9915 - loss: 0.0270\n",
      "Epoch 8/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9929 - loss: 0.0248\n",
      "Epoch 9/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9926 - loss: 0.0263\n",
      "Epoch 10/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9946 - loss: 0.0185\n",
      "Epoch 11/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9941 - loss: 0.0224\n",
      "Epoch 12/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9941 - loss: 0.0220\n",
      "Epoch 13/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9954 - loss: 0.0181\n",
      "Epoch 14/20\n",
      "1875/1875 - 42s - 22ms/step - acc: 0.9949 - loss: 0.0223\n",
      "Epoch 15/20\n",
      "1875/1875 - 42s - 22ms/step - acc: 0.9956 - loss: 0.0191\n",
      "Epoch 16/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9966 - loss: 0.0136\n",
      "Epoch 17/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9967 - loss: 0.0138\n",
      "Epoch 18/20\n",
      "1875/1875 - 42s - 22ms/step - acc: 0.9957 - loss: 0.0193\n",
      "Epoch 19/20\n",
      "1875/1875 - 42s - 22ms/step - acc: 0.9965 - loss: 0.0165\n",
      "Epoch 20/20\n",
      "1875/1875 - 42s - 22ms/step - acc: 0.9966 - loss: 0.0157\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9801 - loss: 0.1598\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9983847595941063\n",
      "Recall Score: 0.9983833333333333\n",
      "F1 Score 0.9983833072626787\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9834363984412731\n",
      "Recall Score: 0.9834\n",
      "F1 Score 0.98339855268655\n",
      "Epoch 1/20\n",
      "1875/1875 - 39s - 21ms/step - acc: 0.9436 - loss: 0.1866\n",
      "Epoch 2/20\n",
      "1875/1875 - 35s - 19ms/step - acc: 0.9768 - loss: 0.0843\n",
      "Epoch 3/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9836 - loss: 0.0596\n",
      "Epoch 4/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9879 - loss: 0.0454\n",
      "Epoch 5/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9906 - loss: 0.0352\n",
      "Epoch 6/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9927 - loss: 0.0273\n",
      "Epoch 7/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9944 - loss: 0.0203\n",
      "Epoch 8/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9955 - loss: 0.0150\n",
      "Epoch 9/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9963 - loss: 0.0135\n",
      "Epoch 10/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 0.9975 - loss: 0.0095\n",
      "Epoch 11/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9980 - loss: 0.0069\n",
      "Epoch 12/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9987 - loss: 0.0047\n",
      "Epoch 13/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9990 - loss: 0.0034\n",
      "Epoch 14/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9991 - loss: 0.0031\n",
      "Epoch 15/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9992 - loss: 0.0021\n",
      "Epoch 16/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9997 - loss: 0.0015\n",
      "Epoch 17/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9998 - loss: 5.4800e-04\n",
      "Epoch 18/20\n",
      "1875/1875 - 33s - 17ms/step - acc: 0.9998 - loss: 5.4246e-04\n",
      "Epoch 19/20\n",
      "1875/1875 - 32s - 17ms/step - acc: 0.9999 - loss: 5.0398e-04\n",
      "Epoch 20/20\n",
      "1875/1875 - 41s - 22ms/step - acc: 1.0000 - loss: 1.3598e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9828 - loss: 0.1504\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9861019445649656\n",
      "Recall Score: 0.9861\n",
      "F1 Score 0.9860944082624646\n",
      "Epoch 1/20\n",
      "938/938 - 28s - 30ms/step - acc: 0.9451 - loss: 0.1784\n",
      "Epoch 2/20\n",
      "938/938 - 36s - 39ms/step - acc: 0.9760 - loss: 0.0760\n",
      "Epoch 3/20\n",
      "938/938 - 22s - 23ms/step - acc: 0.9830 - loss: 0.0513\n",
      "Epoch 4/20\n",
      "938/938 - 22s - 24ms/step - acc: 0.9880 - loss: 0.0387\n",
      "Epoch 5/20\n",
      "938/938 - 22s - 23ms/step - acc: 0.9897 - loss: 0.0311\n",
      "Epoch 6/20\n",
      "938/938 - 22s - 24ms/step - acc: 0.9907 - loss: 0.0304\n",
      "Epoch 7/20\n",
      "938/938 - 22s - 24ms/step - acc: 0.9933 - loss: 0.0210\n",
      "Epoch 8/20\n",
      "938/938 - 24s - 26ms/step - acc: 0.9934 - loss: 0.0213\n",
      "Epoch 9/20\n",
      "938/938 - 22s - 24ms/step - acc: 0.9945 - loss: 0.0176\n",
      "Epoch 10/20\n",
      "938/938 - 21s - 23ms/step - acc: 0.9934 - loss: 0.0214\n",
      "Epoch 11/20\n",
      "938/938 - 22s - 23ms/step - acc: 0.9957 - loss: 0.0137\n",
      "Epoch 12/20\n",
      "938/938 - 21s - 23ms/step - acc: 0.9949 - loss: 0.0178\n",
      "Epoch 13/20\n",
      "938/938 - 21s - 23ms/step - acc: 0.9968 - loss: 0.0111\n",
      "Epoch 14/20\n",
      "938/938 - 22s - 23ms/step - acc: 0.9959 - loss: 0.0145\n",
      "Epoch 15/20\n",
      "938/938 - 21s - 23ms/step - acc: 0.9977 - loss: 0.0085\n",
      "Epoch 16/20\n",
      "938/938 - 22s - 23ms/step - acc: 0.9956 - loss: 0.0169\n",
      "Epoch 17/20\n",
      "938/938 - 22s - 24ms/step - acc: 0.9970 - loss: 0.0113\n",
      "Epoch 18/20\n",
      "938/938 - 22s - 23ms/step - acc: 0.9968 - loss: 0.0113\n",
      "Epoch 19/20\n",
      "938/938 - 21s - 23ms/step - acc: 0.9971 - loss: 0.0112\n",
      "Epoch 20/20\n",
      "938/938 - 22s - 23ms/step - acc: 0.9972 - loss: 0.0104\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9762 - loss: 0.1906\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9971565283494397\n",
      "Recall Score: 0.99715\n",
      "F1 Score 0.9971505602915081\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9806935090174619\n",
      "Recall Score: 0.9806\n",
      "F1 Score 0.9806077513591246\n",
      "Epoch 1/20\n",
      "938/938 - 22s - 23ms/step - acc: 0.9421 - loss: 0.1903\n",
      "Epoch 2/20\n",
      "938/938 - 21s - 22ms/step - acc: 0.9773 - loss: 0.0748\n",
      "Epoch 3/20\n",
      "938/938 - 18s - 19ms/step - acc: 0.9852 - loss: 0.0493\n",
      "Epoch 4/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9890 - loss: 0.0362\n",
      "Epoch 5/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9920 - loss: 0.0256\n",
      "Epoch 6/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9941 - loss: 0.0197\n",
      "Epoch 7/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9953 - loss: 0.0138\n",
      "Epoch 8/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9969 - loss: 0.0105\n",
      "Epoch 9/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9976 - loss: 0.0078\n",
      "Epoch 10/20\n",
      "938/938 - 17s - 19ms/step - acc: 0.9982 - loss: 0.0063\n",
      "Epoch 11/20\n",
      "938/938 - 20s - 22ms/step - acc: 0.9986 - loss: 0.0044\n",
      "Epoch 12/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9991 - loss: 0.0031\n",
      "Epoch 13/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9993 - loss: 0.0026\n",
      "Epoch 14/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9997 - loss: 0.0011\n",
      "Epoch 15/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9996 - loss: 0.0012\n",
      "Epoch 16/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9997 - loss: 8.5098e-04\n",
      "Epoch 17/20\n",
      "938/938 - 17s - 19ms/step - acc: 0.9999 - loss: 4.7082e-04\n",
      "Epoch 18/20\n",
      "938/938 - 17s - 18ms/step - acc: 0.9998 - loss: 6.6625e-04\n",
      "Epoch 19/20\n",
      "938/938 - 17s - 19ms/step - acc: 1.0000 - loss: 2.7357e-05\n",
      "Epoch 20/20\n",
      "938/938 - 20s - 22ms/step - acc: 1.0000 - loss: 5.6315e-06\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9838 - loss: 0.1308\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9871092653360467\n",
      "Recall Score: 0.9871\n",
      "F1 Score 0.98709887882925\n",
      "Epoch 1/20\n",
      "469/469 - 16s - 34ms/step - acc: 0.9423 - loss: 0.1898\n",
      "Epoch 2/20\n",
      "469/469 - 15s - 32ms/step - acc: 0.9768 - loss: 0.0723\n",
      "Epoch 3/20\n",
      "469/469 - 15s - 31ms/step - acc: 0.9853 - loss: 0.0456\n",
      "Epoch 4/20\n",
      "469/469 - 12s - 25ms/step - acc: 0.9889 - loss: 0.0350\n",
      "Epoch 5/20\n",
      "469/469 - 12s - 25ms/step - acc: 0.9906 - loss: 0.0276\n",
      "Epoch 6/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9926 - loss: 0.0227\n",
      "Epoch 7/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9940 - loss: 0.0179\n",
      "Epoch 8/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9948 - loss: 0.0158\n",
      "Epoch 9/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9944 - loss: 0.0168\n",
      "Epoch 10/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9952 - loss: 0.0148\n",
      "Epoch 11/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9958 - loss: 0.0132\n",
      "Epoch 12/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9960 - loss: 0.0122\n",
      "Epoch 13/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9964 - loss: 0.0115\n",
      "Epoch 14/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9961 - loss: 0.0121\n",
      "Epoch 15/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9965 - loss: 0.0118\n",
      "Epoch 16/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9973 - loss: 0.0084\n",
      "Epoch 17/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9978 - loss: 0.0076\n",
      "Epoch 18/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9969 - loss: 0.0108\n",
      "Epoch 19/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9975 - loss: 0.0085\n",
      "Epoch 20/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9963 - loss: 0.0121\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9765 - loss: 0.1365\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9978564289648839\n",
      "Recall Score: 0.99785\n",
      "F1 Score 0.9978509421960287\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.98176299941136\n",
      "Recall Score: 0.9817\n",
      "F1 Score 0.9817139216213995\n",
      "Epoch 1/20\n",
      "469/469 - 13s - 28ms/step - acc: 0.9332 - loss: 0.2160\n",
      "Epoch 2/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9764 - loss: 0.0766\n",
      "Epoch 3/20\n",
      "469/469 - 12s - 26ms/step - acc: 0.9846 - loss: 0.0475\n",
      "Epoch 4/20\n",
      "469/469 - 11s - 23ms/step - acc: 0.9890 - loss: 0.0334\n",
      "Epoch 5/20\n",
      "469/469 - 10s - 22ms/step - acc: 0.9923 - loss: 0.0235\n",
      "Epoch 6/20\n",
      "469/469 - 10s - 22ms/step - acc: 0.9944 - loss: 0.0175\n",
      "Epoch 7/20\n",
      "469/469 - 10s - 22ms/step - acc: 0.9963 - loss: 0.0119\n",
      "Epoch 8/20\n",
      "469/469 - 10s - 22ms/step - acc: 0.9969 - loss: 0.0098\n",
      "Epoch 9/20\n",
      "469/469 - 10s - 22ms/step - acc: 0.9978 - loss: 0.0072\n",
      "Epoch 10/20\n",
      "469/469 - 10s - 21ms/step - acc: 0.9984 - loss: 0.0054\n",
      "Epoch 11/20\n",
      "469/469 - 10s - 21ms/step - acc: 0.9990 - loss: 0.0034\n",
      "Epoch 12/20\n",
      "469/469 - 10s - 21ms/step - acc: 0.9992 - loss: 0.0027\n",
      "Epoch 13/20\n",
      "469/469 - 10s - 21ms/step - acc: 0.9994 - loss: 0.0015\n",
      "Epoch 14/20\n",
      "469/469 - 10s - 21ms/step - acc: 0.9995 - loss: 0.0014\n",
      "Epoch 15/20\n",
      "469/469 - 10s - 21ms/step - acc: 0.9999 - loss: 3.0952e-04\n",
      "Epoch 16/20\n",
      "469/469 - 10s - 22ms/step - acc: 1.0000 - loss: 8.4471e-05\n",
      "Epoch 17/20\n",
      "469/469 - 10s - 21ms/step - acc: 1.0000 - loss: 6.2483e-05\n",
      "Epoch 18/20\n",
      "469/469 - 10s - 21ms/step - acc: 1.0000 - loss: 1.6669e-05\n",
      "Epoch 19/20\n",
      "469/469 - 10s - 21ms/step - acc: 1.0000 - loss: 1.1926e-05\n",
      "Epoch 20/20\n",
      "469/469 - 10s - 21ms/step - acc: 1.0000 - loss: 1.0306e-05\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9829 - loss: 0.0988\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9860001524189133\n",
      "Recall Score: 0.986\n",
      "F1 Score 0.985997909561115\n",
      "Epoch 1/20\n",
      "235/235 - 11s - 45ms/step - acc: 0.9349 - loss: 0.2204\n",
      "Epoch 2/20\n",
      "235/235 - 10s - 44ms/step - acc: 0.9767 - loss: 0.0752\n",
      "Epoch 3/20\n",
      "235/235 - 9s - 40ms/step - acc: 0.9851 - loss: 0.0470\n",
      "Epoch 4/20\n",
      "235/235 - 9s - 40ms/step - acc: 0.9897 - loss: 0.0314\n",
      "Epoch 5/20\n",
      "235/235 - 8s - 33ms/step - acc: 0.9920 - loss: 0.0238\n",
      "Epoch 6/20\n",
      "235/235 - 7s - 32ms/step - acc: 0.9943 - loss: 0.0173\n",
      "Epoch 7/20\n",
      "235/235 - 7s - 32ms/step - acc: 0.9951 - loss: 0.0155\n",
      "Epoch 8/20\n",
      "235/235 - 8s - 32ms/step - acc: 0.9948 - loss: 0.0151\n",
      "Epoch 9/20\n",
      "235/235 - 7s - 32ms/step - acc: 0.9937 - loss: 0.0179\n",
      "Epoch 10/20\n",
      "235/235 - 8s - 32ms/step - acc: 0.9969 - loss: 0.0091\n",
      "Epoch 11/20\n",
      "235/235 - 8s - 32ms/step - acc: 0.9975 - loss: 0.0080\n",
      "Epoch 12/20\n",
      "235/235 - 7s - 32ms/step - acc: 0.9962 - loss: 0.0116\n",
      "Epoch 13/20\n",
      "235/235 - 7s - 32ms/step - acc: 0.9965 - loss: 0.0100\n",
      "Epoch 14/20\n",
      "235/235 - 8s - 32ms/step - acc: 0.9969 - loss: 0.0092\n",
      "Epoch 15/20\n",
      "235/235 - 7s - 32ms/step - acc: 0.9969 - loss: 0.0105\n",
      "Epoch 16/20\n",
      "235/235 - 7s - 32ms/step - acc: 0.9982 - loss: 0.0054\n",
      "Epoch 17/20\n",
      "235/235 - 8s - 32ms/step - acc: 0.9976 - loss: 0.0075\n",
      "Epoch 18/20\n",
      "235/235 - 8s - 32ms/step - acc: 0.9970 - loss: 0.0093\n",
      "Epoch 19/20\n",
      "235/235 - 8s - 32ms/step - acc: 0.9969 - loss: 0.0098\n",
      "Epoch 20/20\n",
      "235/235 - 8s - 32ms/step - acc: 0.9981 - loss: 0.0056\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9803 - loss: 0.1062\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9984533286741312\n",
      "Recall Score: 0.99845\n",
      "F1 Score 0.9984501319211901\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9828810423225092\n",
      "Recall Score: 0.9828\n",
      "F1 Score 0.9828057520434484\n",
      "Epoch 1/20\n",
      "235/235 - 9s - 38ms/step - acc: 0.9149 - loss: 0.2713\n",
      "Epoch 2/20\n",
      "235/235 - 9s - 37ms/step - acc: 0.9720 - loss: 0.0904\n",
      "Epoch 3/20\n",
      "235/235 - 8s - 35ms/step - acc: 0.9827 - loss: 0.0560\n",
      "Epoch 4/20\n",
      "235/235 - 9s - 38ms/step - acc: 0.9877 - loss: 0.0380\n",
      "Epoch 5/20\n",
      "235/235 - 8s - 32ms/step - acc: 0.9919 - loss: 0.0254\n",
      "Epoch 6/20\n",
      "235/235 - 6s - 26ms/step - acc: 0.9944 - loss: 0.0180\n",
      "Epoch 7/20\n",
      "235/235 - 6s - 27ms/step - acc: 0.9963 - loss: 0.0119\n",
      "Epoch 8/20\n",
      "235/235 - 6s - 27ms/step - acc: 0.9969 - loss: 0.0105\n",
      "Epoch 9/20\n",
      "235/235 - 7s - 28ms/step - acc: 0.9973 - loss: 0.0075\n",
      "Epoch 10/20\n",
      "235/235 - 6s - 27ms/step - acc: 0.9982 - loss: 0.0056\n",
      "Epoch 11/20\n",
      "235/235 - 6s - 27ms/step - acc: 0.9988 - loss: 0.0037\n",
      "Epoch 12/20\n",
      "235/235 - 6s - 27ms/step - acc: 0.9991 - loss: 0.0030\n",
      "Epoch 13/20\n",
      "235/235 - 6s - 27ms/step - acc: 0.9998 - loss: 0.0011\n",
      "Epoch 14/20\n",
      "235/235 - 6s - 27ms/step - acc: 1.0000 - loss: 1.5926e-04\n",
      "Epoch 15/20\n",
      "235/235 - 7s - 28ms/step - acc: 1.0000 - loss: 9.5384e-05\n",
      "Epoch 16/20\n",
      "235/235 - 6s - 27ms/step - acc: 1.0000 - loss: 6.1830e-05\n",
      "Epoch 17/20\n",
      "235/235 - 6s - 26ms/step - acc: 1.0000 - loss: 5.1866e-05\n",
      "Epoch 18/20\n",
      "235/235 - 6s - 27ms/step - acc: 1.0000 - loss: 4.5602e-05\n",
      "Epoch 19/20\n",
      "235/235 - 6s - 27ms/step - acc: 1.0000 - loss: 4.0929e-05\n",
      "Epoch 20/20\n",
      "235/235 - 6s - 26ms/step - acc: 1.0000 - loss: 3.6826e-05\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9849 - loss: 0.0844\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score 1.0\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層2, 節點1024, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9870082449261932\n",
      "Recall Score: 0.987\n",
      "F1 Score 0.9870004904608126\n",
      "==================================================\n",
      "Completed batch for layer 2\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9157 - loss: 0.2779\n",
      "Epoch 2/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9631 - loss: 0.1223\n",
      "Epoch 3/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9715 - loss: 0.0910\n",
      "Epoch 4/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9772 - loss: 0.0719\n",
      "Epoch 5/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9807 - loss: 0.0600\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9651 - loss: 0.1172\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9846123587596161\n",
      "Recall Score: 0.9845333333333334\n",
      "F1 Score 0.9845313245158348\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9701734222710764\n",
      "Recall Score: 0.97\n",
      "F1 Score 0.9699950709396605\n",
      "Epoch 1/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9173 - loss: 0.2840\n",
      "Epoch 2/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9606 - loss: 0.1339\n",
      "Epoch 3/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9694 - loss: 0.1021\n",
      "Epoch 4/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9747 - loss: 0.0845\n",
      "Epoch 5/5\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9787 - loss: 0.0728\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9616 - loss: 0.1372\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9796215307389874\n",
      "Recall Score: 0.9789833333333333\n",
      "F1 Score 0.9790967878810233\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9681070458217428\n",
      "Recall Score: 0.9674\n",
      "F1 Score 0.9675012669343281\n",
      "Epoch 1/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9093 - loss: 0.3096\n",
      "Epoch 2/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9603 - loss: 0.1325\n",
      "Epoch 3/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9714 - loss: 0.0958\n",
      "Epoch 4/5\n",
      "938/938 - 2s - 3ms/step - acc: 0.9761 - loss: 0.0762\n",
      "Epoch 5/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9791 - loss: 0.0646\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9680 - loss: 0.1033\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9861052716888283\n",
      "Recall Score: 0.9860666666666666\n",
      "F1 Score 0.9860560189949917\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9727003212145068\n",
      "Recall Score: 0.9725\n",
      "F1 Score 0.9724789103601763\n",
      "Epoch 1/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9099 - loss: 0.3087\n",
      "Epoch 2/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9574 - loss: 0.1397\n",
      "Epoch 3/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9687 - loss: 0.1042\n",
      "Epoch 4/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9747 - loss: 0.0839\n",
      "Epoch 5/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9791 - loss: 0.0695\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9678 - loss: 0.1160\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9851863987746575\n",
      "Recall Score: 0.98505\n",
      "F1 Score 0.9850573325930561\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9724396746736022\n",
      "Recall Score: 0.9723\n",
      "F1 Score 0.9722823323351251\n",
      "Epoch 1/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.8895 - loss: 0.3769\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9540 - loss: 0.1566\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9663 - loss: 0.1137\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9735 - loss: 0.0904\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9762 - loss: 0.0771\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9663 - loss: 0.1088\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9821968328489593\n",
      "Recall Score: 0.9820833333333333\n",
      "F1 Score 0.9820621821023697\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9696490305651443\n",
      "Recall Score: 0.9696\n",
      "F1 Score 0.9695594892444689\n",
      "Epoch 1/5\n",
      "469/469 - 2s - 4ms/step - acc: 0.8921 - loss: 0.3704\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9518 - loss: 0.1614\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9651 - loss: 0.1164\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9720 - loss: 0.0930\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 2ms/step - acc: 0.9766 - loss: 0.0756\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9702 - loss: 0.0991\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.983724830805587\n",
      "Recall Score: 0.98365\n",
      "F1 Score 0.9836501461345639\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9748351957515828\n",
      "Recall Score: 0.9748\n",
      "F1 Score 0.974790062915218\n",
      "Epoch 1/5\n",
      "235/235 - 2s - 9ms/step - acc: 0.8653 - loss: 0.4729\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9465 - loss: 0.1836\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9584 - loss: 0.1418\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9643 - loss: 0.1181\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9694 - loss: 0.1011\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9653 - loss: 0.1158\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9758449549896315\n",
      "Recall Score: 0.9757333333333333\n",
      "F1 Score 0.9757355903239471\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.968857230497941\n",
      "Recall Score: 0.9688\n",
      "F1 Score 0.968794742443524\n",
      "Epoch 1/5\n",
      "235/235 - 2s - 6ms/step - acc: 0.8666 - loss: 0.4599\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9430 - loss: 0.1920\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9578 - loss: 0.1421\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 2ms/step - acc: 0.9658 - loss: 0.1136\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9713 - loss: 0.0950\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9594 - loss: 0.1311\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9725933436362494\n",
      "Recall Score: 0.9719666666666666\n",
      "F1 Score 0.9719791874899785\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9655653503786478\n",
      "Recall Score: 0.9647\n",
      "F1 Score 0.9647350235563387\n",
      "Epoch 1/10\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9168 - loss: 0.2736\n",
      "Epoch 2/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9629 - loss: 0.1230\n",
      "Epoch 3/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9708 - loss: 0.0933\n",
      "Epoch 4/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9767 - loss: 0.0752\n",
      "Epoch 5/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9812 - loss: 0.0614\n",
      "Epoch 6/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9825 - loss: 0.0534\n",
      "Epoch 7/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9854 - loss: 0.0468\n",
      "Epoch 8/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9869 - loss: 0.0402\n",
      "Epoch 9/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9880 - loss: 0.0359\n",
      "Epoch 10/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9891 - loss: 0.0342\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9679 - loss: 0.1331\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9896337300528214\n",
      "Recall Score: 0.9895166666666667\n",
      "F1 Score 0.9895129442579814\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9720167105276103\n",
      "Recall Score: 0.9718\n",
      "F1 Score 0.971754584035386\n",
      "Epoch 1/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9194 - loss: 0.2705\n",
      "Epoch 2/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9626 - loss: 0.1245\n",
      "Epoch 3/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9720 - loss: 0.0946\n",
      "Epoch 4/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9767 - loss: 0.0800\n",
      "Epoch 5/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9798 - loss: 0.0688\n",
      "Epoch 6/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9824 - loss: 0.0613\n",
      "Epoch 7/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9842 - loss: 0.0582\n",
      "Epoch 8/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9850 - loss: 0.0527\n",
      "Epoch 9/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9872 - loss: 0.0486\n",
      "Epoch 10/10\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9881 - loss: 0.0452\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9728 - loss: 0.1396\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9916404179374683\n",
      "Recall Score: 0.9915833333333334\n",
      "F1 Score 0.9915910666636117\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9767793303526674\n",
      "Recall Score: 0.9766\n",
      "F1 Score 0.976615418112073\n",
      "Epoch 1/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9080 - loss: 0.3179\n",
      "Epoch 2/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9602 - loss: 0.1318\n",
      "Epoch 3/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9706 - loss: 0.0966\n",
      "Epoch 4/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9767 - loss: 0.0758\n",
      "Epoch 5/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9804 - loss: 0.0628\n",
      "Epoch 6/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9826 - loss: 0.0542\n",
      "Epoch 7/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9851 - loss: 0.0465\n",
      "Epoch 8/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9874 - loss: 0.0394\n",
      "Epoch 9/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9886 - loss: 0.0348\n",
      "Epoch 10/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9902 - loss: 0.0304\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9720 - loss: 0.1114\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9924708143269215\n",
      "Recall Score: 0.9924166666666666\n",
      "F1 Score 0.9924117017012885\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9746138142083223\n",
      "Recall Score: 0.9745\n",
      "F1 Score 0.9744715681450634\n",
      "Epoch 1/10\n",
      "938/938 - 2s - 3ms/step - acc: 0.9100 - loss: 0.3057\n",
      "Epoch 2/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9587 - loss: 0.1361\n",
      "Epoch 3/10\n",
      "938/938 - 2s - 3ms/step - acc: 0.9696 - loss: 0.0993\n",
      "Epoch 4/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9758 - loss: 0.0803\n",
      "Epoch 5/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9798 - loss: 0.0677\n",
      "Epoch 6/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9826 - loss: 0.0572\n",
      "Epoch 7/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9845 - loss: 0.0500\n",
      "Epoch 8/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9863 - loss: 0.0450\n",
      "Epoch 9/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9876 - loss: 0.0396\n",
      "Epoch 10/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9893 - loss: 0.0346\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9706 - loss: 0.1241\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9920900396515482\n",
      "Recall Score: 0.9920333333333333\n",
      "F1 Score 0.9920438050844369\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9753328008844697\n",
      "Recall Score: 0.9752\n",
      "F1 Score 0.9752211512568679\n",
      "Epoch 1/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.8850 - loss: 0.3917\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9536 - loss: 0.1586\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9646 - loss: 0.1190\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9716 - loss: 0.0948\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9768 - loss: 0.0767\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9800 - loss: 0.0646\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9829 - loss: 0.0541\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9849 - loss: 0.0478\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9872 - loss: 0.0404\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9891 - loss: 0.0353\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9702 - loss: 0.1030\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9920820260109171\n",
      "Recall Score: 0.9920666666666667\n",
      "F1 Score 0.9920647964248914\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9737652344009472\n",
      "Recall Score: 0.9737\n",
      "F1 Score 0.973686686835666\n",
      "Epoch 1/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.8925 - loss: 0.3710\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9510 - loss: 0.1644\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9643 - loss: 0.1180\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9720 - loss: 0.0925\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9763 - loss: 0.0773\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9797 - loss: 0.0658\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9827 - loss: 0.0563\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9850 - loss: 0.0499\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9862 - loss: 0.0435\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 2ms/step - acc: 0.9880 - loss: 0.0384\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9715 - loss: 0.1001\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9915216294820612\n",
      "Recall Score: 0.9914833333333334\n",
      "F1 Score 0.9914852992781895\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9756409674640462\n",
      "Recall Score: 0.9755\n",
      "F1 Score 0.9754993495087528\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 8ms/step - acc: 0.8566 - loss: 0.4988\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9417 - loss: 0.1991\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9544 - loss: 0.1534\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9640 - loss: 0.1224\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9689 - loss: 0.1028\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9734 - loss: 0.0879\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9768 - loss: 0.0777\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9790 - loss: 0.0688\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9805 - loss: 0.0617\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9839 - loss: 0.0528\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9686 - loss: 0.1030\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9869315628030373\n",
      "Recall Score: 0.9868666666666667\n",
      "F1 Score 0.9868541304860157\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9728070352673538\n",
      "Recall Score: 0.9727\n",
      "F1 Score 0.972689691489526\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 7ms/step - acc: 0.8736 - loss: 0.4583\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9415 - loss: 0.2012\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9560 - loss: 0.1480\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9642 - loss: 0.1183\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9709 - loss: 0.0978\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9746 - loss: 0.0847\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9782 - loss: 0.0734\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 2ms/step - acc: 0.9801 - loss: 0.0652\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9823 - loss: 0.0575\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9841 - loss: 0.0505\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9698 - loss: 0.1120\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.986166327179171\n",
      "Recall Score: 0.9860666666666666\n",
      "F1 Score 0.986037319648329\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9738376667065293\n",
      "Recall Score: 0.9737\n",
      "F1 Score 0.9736769711672423\n",
      "Epoch 1/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9209 - loss: 0.2641\n",
      "Epoch 2/20\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9638 - loss: 0.1184\n",
      "Epoch 3/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9735 - loss: 0.0870\n",
      "Epoch 4/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9776 - loss: 0.0704\n",
      "Epoch 5/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9816 - loss: 0.0582\n",
      "Epoch 6/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9842 - loss: 0.0497\n",
      "Epoch 7/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9857 - loss: 0.0438\n",
      "Epoch 8/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9873 - loss: 0.0388\n",
      "Epoch 9/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9886 - loss: 0.0345\n",
      "Epoch 10/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9900 - loss: 0.0299\n",
      "Epoch 11/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9905 - loss: 0.0293\n",
      "Epoch 12/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9920 - loss: 0.0244\n",
      "Epoch 13/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9922 - loss: 0.0225\n",
      "Epoch 14/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9925 - loss: 0.0216\n",
      "Epoch 15/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9930 - loss: 0.0206\n",
      "Epoch 16/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9938 - loss: 0.0186\n",
      "Epoch 17/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9942 - loss: 0.0177\n",
      "Epoch 18/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9940 - loss: 0.0189\n",
      "Epoch 19/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9954 - loss: 0.0139\n",
      "Epoch 20/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9942 - loss: 0.0183\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9752 - loss: 0.1247\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9967763270802311\n",
      "Recall Score: 0.9967666666666667\n",
      "F1 Score 0.9967674620866296\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9797780129104962\n",
      "Recall Score: 0.9797\n",
      "F1 Score 0.9797168732727021\n",
      "Epoch 1/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9187 - loss: 0.2781\n",
      "Epoch 2/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9603 - loss: 0.1310\n",
      "Epoch 3/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9697 - loss: 0.1015\n",
      "Epoch 4/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9748 - loss: 0.0851\n",
      "Epoch 5/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9791 - loss: 0.0719\n",
      "Epoch 6/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9810 - loss: 0.0655\n",
      "Epoch 7/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9830 - loss: 0.0581\n",
      "Epoch 8/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9848 - loss: 0.0530\n",
      "Epoch 9/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9865 - loss: 0.0479\n",
      "Epoch 10/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9868 - loss: 0.0462\n",
      "Epoch 11/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9886 - loss: 0.0428\n",
      "Epoch 12/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9885 - loss: 0.0417\n",
      "Epoch 13/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9892 - loss: 0.0381\n",
      "Epoch 14/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9908 - loss: 0.0349\n",
      "Epoch 15/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9913 - loss: 0.0339\n",
      "Epoch 16/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9913 - loss: 0.0314\n",
      "Epoch 17/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9920 - loss: 0.0307\n",
      "Epoch 18/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9918 - loss: 0.0300\n",
      "Epoch 19/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9930 - loss: 0.0264\n",
      "Epoch 20/20\n",
      "1875/1875 - 3s - 1ms/step - acc: 0.9928 - loss: 0.0284\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9721 - loss: 0.2428\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.993824469608022\n",
      "Recall Score: 0.9937666666666667\n",
      "F1 Score 0.9937701529842796\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9767841828323395\n",
      "Recall Score: 0.9766\n",
      "F1 Score 0.9766123710511265\n",
      "Epoch 1/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9111 - loss: 0.3083\n",
      "Epoch 2/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9591 - loss: 0.1325\n",
      "Epoch 3/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9709 - loss: 0.0944\n",
      "Epoch 4/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9759 - loss: 0.0767\n",
      "Epoch 5/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9802 - loss: 0.0632\n",
      "Epoch 6/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9834 - loss: 0.0513\n",
      "Epoch 7/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9851 - loss: 0.0458\n",
      "Epoch 8/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9876 - loss: 0.0368\n",
      "Epoch 9/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9887 - loss: 0.0335\n",
      "Epoch 10/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9908 - loss: 0.0286\n",
      "Epoch 11/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9908 - loss: 0.0278\n",
      "Epoch 12/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9926 - loss: 0.0221\n",
      "Epoch 13/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9923 - loss: 0.0228\n",
      "Epoch 14/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9936 - loss: 0.0193\n",
      "Epoch 15/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9940 - loss: 0.0178\n",
      "Epoch 16/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9944 - loss: 0.0171\n",
      "Epoch 17/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9948 - loss: 0.0160\n",
      "Epoch 18/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9934 - loss: 0.0186\n",
      "Epoch 19/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9956 - loss: 0.0135\n",
      "Epoch 20/20\n",
      "938/938 - 1s - 2ms/step - acc: 0.9951 - loss: 0.0148\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9700 - loss: 0.1444\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9949063877101935\n",
      "Recall Score: 0.9948833333333333\n",
      "F1 Score 0.9948807107072917\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9739373960023924\n",
      "Recall Score: 0.9738\n",
      "F1 Score 0.9737996456405692\n",
      "Epoch 1/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9087 - loss: 0.3128\n",
      "Epoch 2/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9588 - loss: 0.1393\n",
      "Epoch 3/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9687 - loss: 0.1034\n",
      "Epoch 4/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9746 - loss: 0.0832\n",
      "Epoch 5/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9783 - loss: 0.0701\n",
      "Epoch 6/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9811 - loss: 0.0607\n",
      "Epoch 7/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9832 - loss: 0.0537\n",
      "Epoch 8/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9848 - loss: 0.0465\n",
      "Epoch 9/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9869 - loss: 0.0416\n",
      "Epoch 10/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9885 - loss: 0.0373\n",
      "Epoch 11/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9894 - loss: 0.0343\n",
      "Epoch 12/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9902 - loss: 0.0306\n",
      "Epoch 13/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9907 - loss: 0.0283\n",
      "Epoch 14/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9922 - loss: 0.0254\n",
      "Epoch 15/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9926 - loss: 0.0236\n",
      "Epoch 16/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9935 - loss: 0.0203\n",
      "Epoch 17/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9939 - loss: 0.0195\n",
      "Epoch 18/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9940 - loss: 0.0194\n",
      "Epoch 19/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9940 - loss: 0.0188\n",
      "Epoch 20/20\n",
      "938/938 - 1s - 1ms/step - acc: 0.9954 - loss: 0.0154\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9694 - loss: 0.1892\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9965296848550586\n",
      "Recall Score: 0.9965166666666667\n",
      "F1 Score 0.9965184605367134\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9747648753805952\n",
      "Recall Score: 0.9746\n",
      "F1 Score 0.9746141544354597\n",
      "Epoch 1/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.8924 - loss: 0.3751\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9538 - loss: 0.1555\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9645 - loss: 0.1160\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9724 - loss: 0.0921\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9771 - loss: 0.0753\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9804 - loss: 0.0644\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9820 - loss: 0.0575\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9843 - loss: 0.0503\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9861 - loss: 0.0440\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9882 - loss: 0.0372\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9895 - loss: 0.0322\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9906 - loss: 0.0294\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9909 - loss: 0.0270\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9916 - loss: 0.0245\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9926 - loss: 0.0228\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9936 - loss: 0.0188\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9944 - loss: 0.0177\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9939 - loss: 0.0172\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9950 - loss: 0.0149\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9937 - loss: 0.0185\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9682 - loss: 0.1350\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9918743452164916\n",
      "Recall Score: 0.9918\n",
      "F1 Score 0.9917856047515297\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9711510975413761\n",
      "Recall Score: 0.9708\n",
      "F1 Score 0.9707780841064813\n",
      "Epoch 1/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.8914 - loss: 0.3788\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9500 - loss: 0.1669\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9642 - loss: 0.1210\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9705 - loss: 0.0965\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9756 - loss: 0.0797\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9791 - loss: 0.0667\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9819 - loss: 0.0581\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9844 - loss: 0.0505\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9862 - loss: 0.0444\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9877 - loss: 0.0391\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9887 - loss: 0.0343\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9900 - loss: 0.0314\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9911 - loss: 0.0280\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9923 - loss: 0.0246\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9931 - loss: 0.0225\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9937 - loss: 0.0196\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9940 - loss: 0.0186\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9944 - loss: 0.0165\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9954 - loss: 0.0156\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9959 - loss: 0.0135\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9683 - loss: 0.1503\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9963294560869592\n",
      "Recall Score: 0.9962833333333333\n",
      "F1 Score 0.996289821126011\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9747252476484022\n",
      "Recall Score: 0.9744\n",
      "F1 Score 0.9744294127190407\n",
      "Epoch 1/20\n",
      "235/235 - 2s - 8ms/step - acc: 0.8583 - loss: 0.4909\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9453 - loss: 0.1873\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9589 - loss: 0.1403\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9657 - loss: 0.1147\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9713 - loss: 0.0946\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9762 - loss: 0.0802\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9791 - loss: 0.0684\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9817 - loss: 0.0605\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9840 - loss: 0.0529\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9860 - loss: 0.0460\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9879 - loss: 0.0409\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9892 - loss: 0.0367\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9892 - loss: 0.0348\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9911 - loss: 0.0291\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9918 - loss: 0.0263\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9933 - loss: 0.0221\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9936 - loss: 0.0205\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9937 - loss: 0.0193\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9941 - loss: 0.0183\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9958 - loss: 0.0142\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.9694 - loss: 0.1227 \n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9968051120378735\n",
      "Recall Score: 0.9968\n",
      "F1 Score 0.9967986364472031\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9738720602453809\n",
      "Recall Score: 0.9738\n",
      "F1 Score 0.9737974140566091\n",
      "Epoch 1/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.8612 - loss: 0.4780\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9403 - loss: 0.1987\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9566 - loss: 0.1447\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9658 - loss: 0.1162\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9713 - loss: 0.0968\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9750 - loss: 0.0835\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9778 - loss: 0.0715\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9811 - loss: 0.0626\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9831 - loss: 0.0560\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9848 - loss: 0.0487\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9867 - loss: 0.0431\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9884 - loss: 0.0387\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9893 - loss: 0.0347\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9905 - loss: 0.0307\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9914 - loss: 0.0278\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9926 - loss: 0.0247\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9933 - loss: 0.0224\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9940 - loss: 0.0204\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 2ms/step - acc: 0.9945 - loss: 0.0184\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9949 - loss: 0.0168\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9671 - loss: 0.1395\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9924241296938098\n",
      "Recall Score: 0.99225\n",
      "F1 Score 0.9922599810994166\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點64, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9734779794140739\n",
      "Recall Score: 0.9729\n",
      "F1 Score 0.9729828923389566\n",
      "Epoch 1/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9312 - loss: 0.2291\n",
      "Epoch 2/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9689 - loss: 0.1005\n",
      "Epoch 3/5\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9764 - loss: 0.0753\n",
      "Epoch 4/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9818 - loss: 0.0571\n",
      "Epoch 5/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9851 - loss: 0.0470\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9714 - loss: 0.0910\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9901413949634154\n",
      "Recall Score: 0.9901\n",
      "F1 Score 0.9900935235803717\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9754732689716357\n",
      "Recall Score: 0.9754\n",
      "F1 Score 0.9753898956922827\n",
      "Epoch 1/5\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9301 - loss: 0.2327\n",
      "Epoch 2/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9697 - loss: 0.1048\n",
      "Epoch 3/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9772 - loss: 0.0795\n",
      "Epoch 4/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9802 - loss: 0.0674\n",
      "Epoch 5/5\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9844 - loss: 0.0563\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9678 - loss: 0.1348\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.984617713241524\n",
      "Recall Score: 0.9843833333333334\n",
      "F1 Score 0.9843130348850632\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9728923832977429\n",
      "Recall Score: 0.9725\n",
      "F1 Score 0.9724121766778518\n",
      "Epoch 1/5\n",
      "938/938 - 4s - 4ms/step - acc: 0.9253 - loss: 0.2538\n",
      "Epoch 2/5\n",
      "938/938 - 2s - 3ms/step - acc: 0.9685 - loss: 0.1050\n",
      "Epoch 3/5\n",
      "938/938 - 2s - 3ms/step - acc: 0.9771 - loss: 0.0728\n",
      "Epoch 4/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9825 - loss: 0.0559\n",
      "Epoch 5/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9847 - loss: 0.0467\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9713 - loss: 0.0944\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9896275103340029\n",
      "Recall Score: 0.98955\n",
      "F1 Score 0.9895478752513002\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9768152613787315\n",
      "Recall Score: 0.9766\n",
      "F1 Score 0.9766021026833686\n",
      "Epoch 1/5\n",
      "938/938 - 3s - 3ms/step - acc: 0.9237 - loss: 0.2530\n",
      "Epoch 2/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9677 - loss: 0.1043\n",
      "Epoch 3/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9772 - loss: 0.0750\n",
      "Epoch 4/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9827 - loss: 0.0578\n",
      "Epoch 5/5\n",
      "938/938 - 2s - 2ms/step - acc: 0.9859 - loss: 0.0471\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9653 - loss: 0.1274\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9850378294317836\n",
      "Recall Score: 0.9846666666666667\n",
      "F1 Score 0.9847224866255138\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9720512186455893\n",
      "Recall Score: 0.9712\n",
      "F1 Score 0.9713344354987549\n",
      "Epoch 1/5\n",
      "469/469 - 3s - 6ms/step - acc: 0.9099 - loss: 0.3072\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9657 - loss: 0.1147\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9754 - loss: 0.0797\n",
      "Epoch 4/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9809 - loss: 0.0614\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9854 - loss: 0.0468\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9740 - loss: 0.0901\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9903044506601176\n",
      "Recall Score: 0.9902666666666666\n",
      "F1 Score 0.9902656033929207\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9776690165686519\n",
      "Recall Score: 0.9776\n",
      "F1 Score 0.9775971291621485\n",
      "Epoch 1/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.9105 - loss: 0.2995\n",
      "Epoch 2/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9642 - loss: 0.1181\n",
      "Epoch 3/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9753 - loss: 0.0797\n",
      "Epoch 4/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.9817 - loss: 0.0595\n",
      "Epoch 5/5\n",
      "469/469 - 1s - 3ms/step - acc: 0.9856 - loss: 0.0468\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9719 - loss: 0.0976\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9893878872309458\n",
      "Recall Score: 0.98935\n",
      "F1 Score 0.9893390844617435\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9765655967306786\n",
      "Recall Score: 0.9764\n",
      "F1 Score 0.976377282226321\n",
      "Epoch 1/5\n",
      "235/235 - 2s - 10ms/step - acc: 0.8905 - loss: 0.3888\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9585 - loss: 0.1408\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9716 - loss: 0.0948\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9784 - loss: 0.0707\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9829 - loss: 0.0556\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9635 - loss: 0.1136\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9833834712031266\n",
      "Recall Score: 0.9830166666666666\n",
      "F1 Score 0.983032880220837\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9707562975017965\n",
      "Recall Score: 0.9701\n",
      "F1 Score 0.9701326353780408\n",
      "Epoch 1/5\n",
      "235/235 - 2s - 8ms/step - acc: 0.8891 - loss: 0.3822\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9539 - loss: 0.1525\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 3ms/step - acc: 0.9684 - loss: 0.1017\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 4ms/step - acc: 0.9763 - loss: 0.0777\n",
      "Epoch 5/5\n",
      "235/235 - 1s - 5ms/step - acc: 0.9809 - loss: 0.0612\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9659 - loss: 0.1037\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9842637542142465\n",
      "Recall Score: 0.9841166666666666\n",
      "F1 Score 0.9840847372566246\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9724414159946536\n",
      "Recall Score: 0.972\n",
      "F1 Score 0.9719616275345161\n",
      "Epoch 1/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9305 - loss: 0.2308\n",
      "Epoch 2/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9688 - loss: 0.1009\n",
      "Epoch 3/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9775 - loss: 0.0717\n",
      "Epoch 4/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9815 - loss: 0.0583\n",
      "Epoch 5/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9857 - loss: 0.0446\n",
      "Epoch 6/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9868 - loss: 0.0412\n",
      "Epoch 7/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9890 - loss: 0.0329\n",
      "Epoch 8/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9906 - loss: 0.0294\n",
      "Epoch 9/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9918 - loss: 0.0267\n",
      "Epoch 10/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9919 - loss: 0.0244\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9768 - loss: 0.1078\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9938063377085967\n",
      "Recall Score: 0.9937666666666667\n",
      "F1 Score 0.9937639182762446\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.98095681725592\n",
      "Recall Score: 0.9809\n",
      "F1 Score 0.9809003789121805\n",
      "Epoch 1/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9289 - loss: 0.2315\n",
      "Epoch 2/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9685 - loss: 0.1076\n",
      "Epoch 3/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9753 - loss: 0.0846\n",
      "Epoch 4/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9808 - loss: 0.0680\n",
      "Epoch 5/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9839 - loss: 0.0586\n",
      "Epoch 6/10\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9859 - loss: 0.0512\n",
      "Epoch 7/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9873 - loss: 0.0481\n",
      "Epoch 8/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9890 - loss: 0.0430\n",
      "Epoch 9/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9897 - loss: 0.0412\n",
      "Epoch 10/10\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9900 - loss: 0.0398\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9730 - loss: 0.1821\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9938640502689959\n",
      "Recall Score: 0.9938333333333333\n",
      "F1 Score 0.9938352823064985\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9774782061499278\n",
      "Recall Score: 0.9774\n",
      "F1 Score 0.9773993288651832\n",
      "Epoch 1/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9217 - loss: 0.2621\n",
      "Epoch 2/10\n",
      "938/938 - 2s - 3ms/step - acc: 0.9683 - loss: 0.1045\n",
      "Epoch 3/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9773 - loss: 0.0728\n",
      "Epoch 4/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9827 - loss: 0.0549\n",
      "Epoch 5/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9849 - loss: 0.0464\n",
      "Epoch 6/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9879 - loss: 0.0358\n",
      "Epoch 7/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9888 - loss: 0.0324\n",
      "Epoch 8/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9907 - loss: 0.0276\n",
      "Epoch 9/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9926 - loss: 0.0230\n",
      "Epoch 10/10\n",
      "938/938 - 2s - 3ms/step - acc: 0.9926 - loss: 0.0217\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9732 - loss: 0.1111\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.99388117017439\n",
      "Recall Score: 0.9938666666666667\n",
      "F1 Score 0.9938631468231127\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9768719692237408\n",
      "Recall Score: 0.9768\n",
      "F1 Score 0.9767859850485429\n",
      "Epoch 1/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9247 - loss: 0.2515\n",
      "Epoch 2/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9678 - loss: 0.1061\n",
      "Epoch 3/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9767 - loss: 0.0749\n",
      "Epoch 4/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9823 - loss: 0.0587\n",
      "Epoch 5/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9859 - loss: 0.0467\n",
      "Epoch 6/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9876 - loss: 0.0400\n",
      "Epoch 7/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9894 - loss: 0.0341\n",
      "Epoch 8/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9909 - loss: 0.0297\n",
      "Epoch 9/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9926 - loss: 0.0234\n",
      "Epoch 10/10\n",
      "938/938 - 2s - 2ms/step - acc: 0.9933 - loss: 0.0213\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9712 - loss: 0.1453\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9928582921608298\n",
      "Recall Score: 0.9927333333333334\n",
      "F1 Score 0.9927289120205995\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.976656282150752\n",
      "Recall Score: 0.9764\n",
      "F1 Score 0.9763788122836546\n",
      "Epoch 1/10\n",
      "469/469 - 3s - 6ms/step - acc: 0.9096 - loss: 0.3105\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9633 - loss: 0.1213\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9747 - loss: 0.0827\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9793 - loss: 0.0669\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9847 - loss: 0.0498\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9871 - loss: 0.0411\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9895 - loss: 0.0336\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9909 - loss: 0.0281\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9926 - loss: 0.0229\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9919 - loss: 0.0232\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9765 - loss: 0.0900\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.994154596136558\n",
      "Recall Score: 0.9941166666666666\n",
      "F1 Score 0.9941145103297027\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9787313372851119\n",
      "Recall Score: 0.9786\n",
      "F1 Score 0.9786075500379615\n",
      "Epoch 1/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9084 - loss: 0.3058\n",
      "Epoch 2/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9610 - loss: 0.1256\n",
      "Epoch 3/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9735 - loss: 0.0855\n",
      "Epoch 4/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9796 - loss: 0.0649\n",
      "Epoch 5/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9847 - loss: 0.0503\n",
      "Epoch 6/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9868 - loss: 0.0409\n",
      "Epoch 7/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9897 - loss: 0.0320\n",
      "Epoch 8/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9918 - loss: 0.0260\n",
      "Epoch 9/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9927 - loss: 0.0237\n",
      "Epoch 10/10\n",
      "469/469 - 1s - 3ms/step - acc: 0.9941 - loss: 0.0190\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9738 - loss: 0.1150\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9931739685957469\n",
      "Recall Score: 0.9931166666666666\n",
      "F1 Score 0.9931017369381687\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9774275534738559\n",
      "Recall Score: 0.9773\n",
      "F1 Score 0.9772640230237466\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 10ms/step - acc: 0.8899 - loss: 0.3897\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9578 - loss: 0.1412\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9711 - loss: 0.0964\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9778 - loss: 0.0734\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9820 - loss: 0.0584\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9848 - loss: 0.0476\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9880 - loss: 0.0379\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9905 - loss: 0.0299\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9924 - loss: 0.0247\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9929 - loss: 0.0218\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9760 - loss: 0.0894\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9946388284476431\n",
      "Recall Score: 0.9946166666666667\n",
      "F1 Score 0.9946159540579561\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.979127341972101\n",
      "Recall Score: 0.979\n",
      "F1 Score 0.9789955329766673\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 8ms/step - acc: 0.8858 - loss: 0.3842\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9554 - loss: 0.1470\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9693 - loss: 0.1002\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9766 - loss: 0.0759\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9817 - loss: 0.0587\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9851 - loss: 0.0478\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9875 - loss: 0.0391\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9896 - loss: 0.0323\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 3ms/step - acc: 0.9914 - loss: 0.0274\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 4ms/step - acc: 0.9928 - loss: 0.0227\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9717 - loss: 0.1100\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9938039931144715\n",
      "Recall Score: 0.99375\n",
      "F1 Score 0.9937439916929182\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.976070020783395\n",
      "Recall Score: 0.9758\n",
      "F1 Score 0.9757842699094353\n",
      "Epoch 1/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9325 - loss: 0.2267\n",
      "Epoch 2/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9695 - loss: 0.0992\n",
      "Epoch 3/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9777 - loss: 0.0715\n",
      "Epoch 4/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9819 - loss: 0.0565\n",
      "Epoch 5/20\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9860 - loss: 0.0452\n",
      "Epoch 6/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9879 - loss: 0.0387\n",
      "Epoch 7/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9892 - loss: 0.0332\n",
      "Epoch 8/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9903 - loss: 0.0303\n",
      "Epoch 9/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9916 - loss: 0.0262\n",
      "Epoch 10/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9928 - loss: 0.0224\n",
      "Epoch 11/20\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9933 - loss: 0.0218\n",
      "Epoch 12/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9937 - loss: 0.0198\n",
      "Epoch 13/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9938 - loss: 0.0196\n",
      "Epoch 14/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9954 - loss: 0.0155\n",
      "Epoch 15/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9955 - loss: 0.0147\n",
      "Epoch 16/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9953 - loss: 0.0147\n",
      "Epoch 17/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9955 - loss: 0.0144\n",
      "Epoch 18/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9961 - loss: 0.0137\n",
      "Epoch 19/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9958 - loss: 0.0132\n",
      "Epoch 20/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9963 - loss: 0.0124\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9721 - loss: 0.1630\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9966484125247009\n",
      "Recall Score: 0.9966166666666667\n",
      "F1 Score 0.9966187163924459\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9784987362193742\n",
      "Recall Score: 0.9783\n",
      "F1 Score 0.9783017950118101\n",
      "Epoch 1/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9312 - loss: 0.2281\n",
      "Epoch 2/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9692 - loss: 0.1064\n",
      "Epoch 3/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9772 - loss: 0.0795\n",
      "Epoch 4/20\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9808 - loss: 0.0670\n",
      "Epoch 5/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9837 - loss: 0.0574\n",
      "Epoch 6/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9864 - loss: 0.0493\n",
      "Epoch 7/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9883 - loss: 0.0441\n",
      "Epoch 8/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9891 - loss: 0.0411\n",
      "Epoch 9/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9904 - loss: 0.0367\n",
      "Epoch 10/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9909 - loss: 0.0348\n",
      "Epoch 11/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9923 - loss: 0.0311\n",
      "Epoch 12/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9926 - loss: 0.0303\n",
      "Epoch 13/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9934 - loss: 0.0288\n",
      "Epoch 14/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9936 - loss: 0.0274\n",
      "Epoch 15/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9939 - loss: 0.0238\n",
      "Epoch 16/20\n",
      "1875/1875 - 5s - 2ms/step - acc: 0.9944 - loss: 0.0228\n",
      "Epoch 17/20\n",
      "1875/1875 - 4s - 2ms/step - acc: 0.9949 - loss: 0.0246\n",
      "Epoch 18/20\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9952 - loss: 0.0204\n",
      "Epoch 19/20\n",
      "1875/1875 - 3s - 2ms/step - acc: 0.9955 - loss: 0.0211\n",
      "Epoch 20/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9962 - loss: 0.0161\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9754 - loss: 0.2712\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9972211863934363\n",
      "Recall Score: 0.9972166666666666\n",
      "F1 Score 0.9972165912649507\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9793561743498703\n",
      "Recall Score: 0.9793\n",
      "F1 Score 0.9792923145436544\n",
      "Epoch 1/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9245 - loss: 0.2535\n",
      "Epoch 2/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9673 - loss: 0.1043\n",
      "Epoch 3/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9780 - loss: 0.0718\n",
      "Epoch 4/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9833 - loss: 0.0535\n",
      "Epoch 5/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9866 - loss: 0.0413\n",
      "Epoch 6/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9888 - loss: 0.0350\n",
      "Epoch 7/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9893 - loss: 0.0328\n",
      "Epoch 8/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9918 - loss: 0.0254\n",
      "Epoch 9/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9916 - loss: 0.0245\n",
      "Epoch 10/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9931 - loss: 0.0202\n",
      "Epoch 11/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9936 - loss: 0.0192\n",
      "Epoch 12/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9948 - loss: 0.0151\n",
      "Epoch 13/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9940 - loss: 0.0188\n",
      "Epoch 14/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9950 - loss: 0.0152\n",
      "Epoch 15/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9952 - loss: 0.0144\n",
      "Epoch 16/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9952 - loss: 0.0140\n",
      "Epoch 17/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9958 - loss: 0.0131\n",
      "Epoch 18/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9965 - loss: 0.0112\n",
      "Epoch 19/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9967 - loss: 0.0104\n",
      "Epoch 20/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9953 - loss: 0.0143\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9749 - loss: 0.1293\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9972069598744058\n",
      "Recall Score: 0.9972\n",
      "F1 Score 0.9972003390548001\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9792763079852531\n",
      "Recall Score: 0.9792\n",
      "F1 Score 0.9792020423790156\n",
      "Epoch 1/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9248 - loss: 0.2522\n",
      "Epoch 2/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9672 - loss: 0.1064\n",
      "Epoch 3/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9765 - loss: 0.0765\n",
      "Epoch 4/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9826 - loss: 0.0570\n",
      "Epoch 5/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9855 - loss: 0.0479\n",
      "Epoch 6/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9879 - loss: 0.0398\n",
      "Epoch 7/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9901 - loss: 0.0325\n",
      "Epoch 8/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9912 - loss: 0.0284\n",
      "Epoch 9/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9926 - loss: 0.0248\n",
      "Epoch 10/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9931 - loss: 0.0219\n",
      "Epoch 11/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9940 - loss: 0.0196\n",
      "Epoch 12/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9943 - loss: 0.0190\n",
      "Epoch 13/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9954 - loss: 0.0160\n",
      "Epoch 14/20\n",
      "938/938 - 2s - 3ms/step - acc: 0.9960 - loss: 0.0136\n",
      "Epoch 15/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9962 - loss: 0.0133\n",
      "Epoch 16/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9962 - loss: 0.0128\n",
      "Epoch 17/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9967 - loss: 0.0118\n",
      "Epoch 18/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9969 - loss: 0.0108\n",
      "Epoch 19/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9971 - loss: 0.0103\n",
      "Epoch 20/20\n",
      "938/938 - 2s - 2ms/step - acc: 0.9972 - loss: 0.0102\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9709 - loss: 0.2237\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9966457354546207\n",
      "Recall Score: 0.9966166666666667\n",
      "F1 Score 0.9966123165055447\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9766896281701567\n",
      "Recall Score: 0.9765\n",
      "F1 Score 0.9765105965657451\n",
      "Epoch 1/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9096 - loss: 0.3075\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9656 - loss: 0.1162\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9758 - loss: 0.0786\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9811 - loss: 0.0598\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9853 - loss: 0.0469\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9882 - loss: 0.0375\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9900 - loss: 0.0311\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9906 - loss: 0.0281\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9920 - loss: 0.0220\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9932 - loss: 0.0201\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9939 - loss: 0.0187\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9949 - loss: 0.0160\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9953 - loss: 0.0134\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9946 - loss: 0.0163\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9962 - loss: 0.0124\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9952 - loss: 0.0135\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9958 - loss: 0.0117\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9965 - loss: 0.0099\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9965 - loss: 0.0102\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9957 - loss: 0.0126\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9770 - loss: 0.1105\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9985026492076899\n",
      "Recall Score: 0.9985\n",
      "F1 Score 0.9985001424231845\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9813634649416841\n",
      "Recall Score: 0.9813\n",
      "F1 Score 0.9813094932199224\n",
      "Epoch 1/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9088 - loss: 0.3044\n",
      "Epoch 2/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9629 - loss: 0.1217\n",
      "Epoch 3/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9740 - loss: 0.0849\n",
      "Epoch 4/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9794 - loss: 0.0652\n",
      "Epoch 5/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9840 - loss: 0.0514\n",
      "Epoch 6/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9871 - loss: 0.0413\n",
      "Epoch 7/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9885 - loss: 0.0338\n",
      "Epoch 8/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9912 - loss: 0.0283\n",
      "Epoch 9/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9924 - loss: 0.0226\n",
      "Epoch 10/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9935 - loss: 0.0205\n",
      "Epoch 11/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9944 - loss: 0.0175\n",
      "Epoch 12/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9954 - loss: 0.0151\n",
      "Epoch 13/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9958 - loss: 0.0132\n",
      "Epoch 14/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9965 - loss: 0.0108\n",
      "Epoch 15/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9966 - loss: 0.0105\n",
      "Epoch 16/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9971 - loss: 0.0098\n",
      "Epoch 17/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9973 - loss: 0.0089\n",
      "Epoch 18/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9975 - loss: 0.0089\n",
      "Epoch 19/20\n",
      "469/469 - 1s - 2ms/step - acc: 0.9978 - loss: 0.0075\n",
      "Epoch 20/20\n",
      "469/469 - 1s - 3ms/step - acc: 0.9980 - loss: 0.0061\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9751 - loss: 0.1667\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9986511188781151\n",
      "Recall Score: 0.99865\n",
      "F1 Score 0.9986501378212522\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9787739074423891\n",
      "Recall Score: 0.9787\n",
      "F1 Score 0.9787021999573342\n",
      "Epoch 1/20\n",
      "235/235 - 2s - 9ms/step - acc: 0.8928 - loss: 0.3768\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9590 - loss: 0.1383\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9701 - loss: 0.0983\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9785 - loss: 0.0705\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9828 - loss: 0.0555\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9864 - loss: 0.0437\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9896 - loss: 0.0347\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9906 - loss: 0.0297\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9929 - loss: 0.0245\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9944 - loss: 0.0184\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9946 - loss: 0.0177\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9952 - loss: 0.0147\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9959 - loss: 0.0126\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9959 - loss: 0.0125\n",
      "Epoch 15/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9958 - loss: 0.0115\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9973 - loss: 0.0086\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9959 - loss: 0.0123\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9958 - loss: 0.0119\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9970 - loss: 0.0087\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9972 - loss: 0.0081\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9709 - loss: 0.1395\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9963134906960022\n",
      "Recall Score: 0.9963\n",
      "F1 Score 0.9962991300362546\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9762509824433279\n",
      "Recall Score: 0.976\n",
      "F1 Score 0.9759986135917151\n",
      "Epoch 1/20\n",
      "235/235 - 2s - 8ms/step - acc: 0.8885 - loss: 0.3784\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9543 - loss: 0.1494\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9690 - loss: 0.1022\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9763 - loss: 0.0773\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9807 - loss: 0.0616\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9850 - loss: 0.0494\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9868 - loss: 0.0401\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9890 - loss: 0.0331\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9910 - loss: 0.0273\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9930 - loss: 0.0223\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9940 - loss: 0.0193\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9951 - loss: 0.0152\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9955 - loss: 0.0138\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9956 - loss: 0.0136\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9963 - loss: 0.0112\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9969 - loss: 0.0094\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9973 - loss: 0.0089\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9974 - loss: 0.0075\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 3ms/step - acc: 0.9978 - loss: 0.0067\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 4ms/step - acc: 0.9980 - loss: 0.0063\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9793 - loss: 0.1074\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9989344031722814\n",
      "Recall Score: 0.9989333333333333\n",
      "F1 Score 0.9989334864507361\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點128, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9814327776524037\n",
      "Recall Score: 0.9814\n",
      "F1 Score 0.9813973475826264\n",
      "Epoch 1/5\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9383 - loss: 0.2029\n",
      "Epoch 2/5\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9725 - loss: 0.0913\n",
      "Epoch 3/5\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9784 - loss: 0.0672\n",
      "Epoch 4/5\n",
      "1875/1875 - 10s - 6ms/step - acc: 0.9832 - loss: 0.0528\n",
      "Epoch 5/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9863 - loss: 0.0437\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9736 - loss: 0.0969\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.989656928791026\n",
      "Recall Score: 0.9896333333333334\n",
      "F1 Score 0.9896273037715553\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9775488995960623\n",
      "Recall Score: 0.9774\n",
      "F1 Score 0.9774000440033965\n",
      "Epoch 1/5\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9350 - loss: 0.2125\n",
      "Epoch 2/5\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9714 - loss: 0.0998\n",
      "Epoch 3/5\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9790 - loss: 0.0766\n",
      "Epoch 4/5\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9822 - loss: 0.0671\n",
      "Epoch 5/5\n",
      "1875/1875 - 5s - 3ms/step - acc: 0.9859 - loss: 0.0564\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9650 - loss: 0.1638\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9848015533685838\n",
      "Recall Score: 0.9842666666666666\n",
      "F1 Score 0.9843399924741425\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9714153224230527\n",
      "Recall Score: 0.9704\n",
      "F1 Score 0.9705484784747672\n",
      "Epoch 1/5\n",
      "938/938 - 5s - 6ms/step - acc: 0.9345 - loss: 0.2163\n",
      "Epoch 2/5\n",
      "938/938 - 5s - 5ms/step - acc: 0.9727 - loss: 0.0881\n",
      "Epoch 3/5\n",
      "938/938 - 4s - 4ms/step - acc: 0.9806 - loss: 0.0612\n",
      "Epoch 4/5\n",
      "938/938 - 4s - 4ms/step - acc: 0.9854 - loss: 0.0466\n",
      "Epoch 5/5\n",
      "938/938 - 4s - 4ms/step - acc: 0.9870 - loss: 0.0400\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9730 - loss: 0.0968\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9900047874067763\n",
      "Recall Score: 0.9897666666666667\n",
      "F1 Score 0.9897822948942802\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9774523435420902\n",
      "Recall Score: 0.9771\n",
      "F1 Score 0.9771182738150886\n",
      "Epoch 1/5\n",
      "938/938 - 5s - 5ms/step - acc: 0.9320 - loss: 0.2234\n",
      "Epoch 2/5\n",
      "938/938 - 5s - 6ms/step - acc: 0.9725 - loss: 0.0923\n",
      "Epoch 3/5\n",
      "938/938 - 4s - 4ms/step - acc: 0.9804 - loss: 0.0648\n",
      "Epoch 4/5\n",
      "938/938 - 4s - 4ms/step - acc: 0.9854 - loss: 0.0484\n",
      "Epoch 5/5\n",
      "938/938 - 4s - 4ms/step - acc: 0.9881 - loss: 0.0400\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9709 - loss: 0.1118\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9872018741639729\n",
      "Recall Score: 0.9869666666666667\n",
      "F1 Score 0.9869653328433755\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9745452201948845\n",
      "Recall Score: 0.9741\n",
      "F1 Score 0.974106359900817\n",
      "Epoch 1/5\n",
      "469/469 - 4s - 8ms/step - acc: 0.9281 - loss: 0.2443\n",
      "Epoch 2/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.9717 - loss: 0.0913\n",
      "Epoch 3/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.9808 - loss: 0.0613\n",
      "Epoch 4/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.9854 - loss: 0.0455\n",
      "Epoch 5/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.9891 - loss: 0.0339\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9725 - loss: 0.0900\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.991867675307225\n",
      "Recall Score: 0.9918\n",
      "F1 Score 0.9918054244749163\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9769867375408441\n",
      "Recall Score: 0.9766\n",
      "F1 Score 0.9766441284126146\n",
      "Epoch 1/5\n",
      "469/469 - 3s - 7ms/step - acc: 0.9184 - loss: 0.2606\n",
      "Epoch 2/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.9698 - loss: 0.0984\n",
      "Epoch 3/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.9791 - loss: 0.0662\n",
      "Epoch 4/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.9855 - loss: 0.0467\n",
      "Epoch 5/5\n",
      "469/469 - 2s - 5ms/step - acc: 0.9884 - loss: 0.0363\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9766 - loss: 0.0831\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.992505878817484\n",
      "Recall Score: 0.99245\n",
      "F1 Score 0.9924554279101343\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9792795240645014\n",
      "Recall Score: 0.9791\n",
      "F1 Score 0.9791136383926973\n",
      "Epoch 1/5\n",
      "235/235 - 3s - 12ms/step - acc: 0.9121 - loss: 0.3047\n",
      "Epoch 2/5\n",
      "235/235 - 2s - 7ms/step - acc: 0.9689 - loss: 0.1042\n",
      "Epoch 3/5\n",
      "235/235 - 1s - 6ms/step - acc: 0.9792 - loss: 0.0672\n",
      "Epoch 4/5\n",
      "235/235 - 1s - 6ms/step - acc: 0.9851 - loss: 0.0491\n",
      "Epoch 5/5\n",
      "235/235 - 2s - 6ms/step - acc: 0.9886 - loss: 0.0357\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9765 - loss: 0.0851\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9920883841331453\n",
      "Recall Score: 0.99205\n",
      "F1 Score 0.9920525758416876\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.980013820219748\n",
      "Recall Score: 0.9799\n",
      "F1 Score 0.9799155580335515\n",
      "Epoch 1/5\n",
      "235/235 - 2s - 10ms/step - acc: 0.8975 - loss: 0.3338\n",
      "Epoch 2/5\n",
      "235/235 - 1s - 6ms/step - acc: 0.9632 - loss: 0.1197\n",
      "Epoch 3/5\n",
      "235/235 - 2s - 6ms/step - acc: 0.9758 - loss: 0.0777\n",
      "Epoch 4/5\n",
      "235/235 - 2s - 6ms/step - acc: 0.9825 - loss: 0.0544\n",
      "Epoch 5/5\n",
      "235/235 - 3s - 11ms/step - acc: 0.9868 - loss: 0.0403\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9758 - loss: 0.0842\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9920626071200553\n",
      "Recall Score: 0.9920333333333333\n",
      "F1 Score 0.9920342992133429\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9794077487123449\n",
      "Recall Score: 0.9793\n",
      "F1 Score 0.9793018318343282\n",
      "Epoch 1/10\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9366 - loss: 0.2071\n",
      "Epoch 2/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9715 - loss: 0.0925\n",
      "Epoch 3/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9792 - loss: 0.0660\n",
      "Epoch 4/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9834 - loss: 0.0521\n",
      "Epoch 5/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9868 - loss: 0.0413\n",
      "Epoch 6/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9892 - loss: 0.0338\n",
      "Epoch 7/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9902 - loss: 0.0322\n",
      "Epoch 8/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9921 - loss: 0.0267\n",
      "Epoch 9/10\n",
      "1875/1875 - 7s - 3ms/step - acc: 0.9928 - loss: 0.0235\n",
      "Epoch 10/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9932 - loss: 0.0217\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9731 - loss: 0.1287\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9935819960819465\n",
      "Recall Score: 0.99355\n",
      "F1 Score 0.9935397332307366\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9757578788376439\n",
      "Recall Score: 0.9756\n",
      "F1 Score 0.9755992028462495\n",
      "Epoch 1/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9364 - loss: 0.2100\n",
      "Epoch 2/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9722 - loss: 0.0985\n",
      "Epoch 3/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9801 - loss: 0.0753\n",
      "Epoch 4/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9838 - loss: 0.0639\n",
      "Epoch 5/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9860 - loss: 0.0551\n",
      "Epoch 6/10\n",
      "1875/1875 - 9s - 5ms/step - acc: 0.9880 - loss: 0.0497\n",
      "Epoch 7/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9887 - loss: 0.0469\n",
      "Epoch 8/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9904 - loss: 0.0416\n",
      "Epoch 9/10\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9921 - loss: 0.0369\n",
      "Epoch 10/10\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9924 - loss: 0.0334\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9744 - loss: 0.1992\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9949703831540428\n",
      "Recall Score: 0.99495\n",
      "F1 Score 0.9949509467112246\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9777081957541893\n",
      "Recall Score: 0.9776\n",
      "F1 Score 0.9775978526227739\n",
      "Epoch 1/10\n",
      "938/938 - 5s - 6ms/step - acc: 0.9315 - loss: 0.2206\n",
      "Epoch 2/10\n",
      "938/938 - 5s - 6ms/step - acc: 0.9735 - loss: 0.0860\n",
      "Epoch 3/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9812 - loss: 0.0594\n",
      "Epoch 4/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9845 - loss: 0.0487\n",
      "Epoch 5/10\n",
      "938/938 - 6s - 6ms/step - acc: 0.9883 - loss: 0.0356\n",
      "Epoch 6/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9901 - loss: 0.0313\n",
      "Epoch 7/10\n",
      "938/938 - 3s - 4ms/step - acc: 0.9910 - loss: 0.0271\n",
      "Epoch 8/10\n",
      "938/938 - 3s - 4ms/step - acc: 0.9919 - loss: 0.0248\n",
      "Epoch 9/10\n",
      "938/938 - 3s - 4ms/step - acc: 0.9927 - loss: 0.0213\n",
      "Epoch 10/10\n",
      "938/938 - 3s - 4ms/step - acc: 0.9936 - loss: 0.0192\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9743 - loss: 0.1140\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9927752690948003\n",
      "Recall Score: 0.9927\n",
      "F1 Score 0.9926991279207151\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.979689104718744\n",
      "Recall Score: 0.9796\n",
      "F1 Score 0.9795839784429067\n",
      "Epoch 1/10\n",
      "938/938 - 4s - 5ms/step - acc: 0.9306 - loss: 0.2289\n",
      "Epoch 2/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9729 - loss: 0.0920\n",
      "Epoch 3/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9814 - loss: 0.0656\n",
      "Epoch 4/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9845 - loss: 0.0488\n",
      "Epoch 5/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9880 - loss: 0.0413\n",
      "Epoch 6/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9906 - loss: 0.0320\n",
      "Epoch 7/10\n",
      "938/938 - 4s - 4ms/step - acc: 0.9918 - loss: 0.0282\n",
      "Epoch 8/10\n",
      "938/938 - 3s - 4ms/step - acc: 0.9932 - loss: 0.0240\n",
      "Epoch 9/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9943 - loss: 0.0203\n",
      "Epoch 10/10\n",
      "938/938 - 3s - 3ms/step - acc: 0.9948 - loss: 0.0191\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9779 - loss: 0.1375\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9971695396944843\n",
      "Recall Score: 0.9971666666666666\n",
      "F1 Score 0.9971665173676905\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9827269547610121\n",
      "Recall Score: 0.9827\n",
      "F1 Score 0.9827023575099701\n",
      "Epoch 1/10\n",
      "469/469 - 4s - 8ms/step - acc: 0.9286 - loss: 0.2440\n",
      "Epoch 2/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9717 - loss: 0.0903\n",
      "Epoch 3/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9801 - loss: 0.0613\n",
      "Epoch 4/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9859 - loss: 0.0461\n",
      "Epoch 5/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9893 - loss: 0.0322\n",
      "Epoch 6/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9914 - loss: 0.0259\n",
      "Epoch 7/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9919 - loss: 0.0241\n",
      "Epoch 8/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9926 - loss: 0.0230\n",
      "Epoch 9/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9934 - loss: 0.0187\n",
      "Epoch 10/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9943 - loss: 0.0184\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9755 - loss: 0.1072\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9956831200836098\n",
      "Recall Score: 0.9956666666666667\n",
      "F1 Score 0.9956675504430782\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9789877816917153\n",
      "Recall Score: 0.9789\n",
      "F1 Score 0.9788942775720205\n",
      "Epoch 1/10\n",
      "469/469 - 3s - 7ms/step - acc: 0.9198 - loss: 0.2600\n",
      "Epoch 2/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9703 - loss: 0.0967\n",
      "Epoch 3/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9801 - loss: 0.0643\n",
      "Epoch 4/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9853 - loss: 0.0464\n",
      "Epoch 5/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9888 - loss: 0.0355\n",
      "Epoch 6/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9914 - loss: 0.0278\n",
      "Epoch 7/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9929 - loss: 0.0220\n",
      "Epoch 8/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9937 - loss: 0.0183\n",
      "Epoch 9/10\n",
      "469/469 - 2s - 4ms/step - acc: 0.9950 - loss: 0.0156\n",
      "Epoch 10/10\n",
      "469/469 - 2s - 5ms/step - acc: 0.9955 - loss: 0.0138\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9793 - loss: 0.1188\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9970286469575543\n",
      "Recall Score: 0.9970166666666667\n",
      "F1 Score 0.9970161691588906\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9823932260317697\n",
      "Recall Score: 0.9823\n",
      "F1 Score 0.9823055181130772\n",
      "Epoch 1/10\n",
      "235/235 - 3s - 11ms/step - acc: 0.9104 - loss: 0.3089\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9671 - loss: 0.1070\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9775 - loss: 0.0708\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9843 - loss: 0.0506\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9886 - loss: 0.0358\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9903 - loss: 0.0300\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9922 - loss: 0.0239\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9950 - loss: 0.0153\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9942 - loss: 0.0168\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9955 - loss: 0.0139\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9734 - loss: 0.1042\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9939401722095791\n",
      "Recall Score: 0.9938166666666667\n",
      "F1 Score 0.9938291100918303\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9785654681799093\n",
      "Recall Score: 0.9782\n",
      "F1 Score 0.978248349146434\n",
      "Epoch 1/10\n",
      "235/235 - 2s - 9ms/step - acc: 0.8967 - loss: 0.3347\n",
      "Epoch 2/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9644 - loss: 0.1164\n",
      "Epoch 3/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9762 - loss: 0.0748\n",
      "Epoch 4/10\n",
      "235/235 - 1s - 5ms/step - acc: 0.9829 - loss: 0.0524\n",
      "Epoch 5/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9872 - loss: 0.0394\n",
      "Epoch 6/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9905 - loss: 0.0297\n",
      "Epoch 7/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9919 - loss: 0.0234\n",
      "Epoch 8/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9940 - loss: 0.0184\n",
      "Epoch 9/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9948 - loss: 0.0152\n",
      "Epoch 10/10\n",
      "235/235 - 1s - 6ms/step - acc: 0.9959 - loss: 0.0122\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9782 - loss: 0.0953\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9967869520453303\n",
      "Recall Score: 0.99675\n",
      "F1 Score 0.9967558298423376\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9807542070147445\n",
      "Recall Score: 0.9806\n",
      "F1 Score 0.9806037244683532\n",
      "Epoch 1/20\n",
      "1875/1875 - 8s - 4ms/step - acc: 0.9373 - loss: 0.2053\n",
      "Epoch 2/20\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9726 - loss: 0.0903\n",
      "Epoch 3/20\n",
      "1875/1875 - 10s - 5ms/step - acc: 0.9795 - loss: 0.0653\n",
      "Epoch 4/20\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9841 - loss: 0.0511\n",
      "Epoch 5/20\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9872 - loss: 0.0416\n",
      "Epoch 6/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9890 - loss: 0.0350\n",
      "Epoch 7/20\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9902 - loss: 0.0313\n",
      "Epoch 8/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9920 - loss: 0.0261\n",
      "Epoch 9/20\n",
      "1875/1875 - 7s - 3ms/step - acc: 0.9921 - loss: 0.0251\n",
      "Epoch 10/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9938 - loss: 0.0207\n",
      "Epoch 11/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9933 - loss: 0.0212\n",
      "Epoch 12/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9941 - loss: 0.0196\n",
      "Epoch 13/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9944 - loss: 0.0195\n",
      "Epoch 14/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9951 - loss: 0.0170\n",
      "Epoch 15/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9951 - loss: 0.0172\n",
      "Epoch 16/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9958 - loss: 0.0147\n",
      "Epoch 17/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9951 - loss: 0.0181\n",
      "Epoch 18/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9953 - loss: 0.0157\n",
      "Epoch 19/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9963 - loss: 0.0134\n",
      "Epoch 20/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9964 - loss: 0.0138\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9748 - loss: 0.1972\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9936339871423899\n",
      "Recall Score: 0.9935833333333334\n",
      "F1 Score 0.993578182209077\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9802053377264046\n",
      "Recall Score: 0.98\n",
      "F1 Score 0.9800040514070842\n",
      "Epoch 1/20\n",
      "1875/1875 - 7s - 4ms/step - acc: 0.9379 - loss: 0.2092\n",
      "Epoch 2/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9715 - loss: 0.0984\n",
      "Epoch 3/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9790 - loss: 0.0766\n",
      "Epoch 4/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9832 - loss: 0.0611\n",
      "Epoch 5/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9862 - loss: 0.0549\n",
      "Epoch 6/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9882 - loss: 0.0462\n",
      "Epoch 7/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9898 - loss: 0.0414\n",
      "Epoch 8/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9908 - loss: 0.0366\n",
      "Epoch 9/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9918 - loss: 0.0334\n",
      "Epoch 10/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9933 - loss: 0.0294\n",
      "Epoch 11/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9937 - loss: 0.0269\n",
      "Epoch 12/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9944 - loss: 0.0238\n",
      "Epoch 13/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9946 - loss: 0.0241\n",
      "Epoch 14/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9952 - loss: 0.0221\n",
      "Epoch 15/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9953 - loss: 0.0228\n",
      "Epoch 16/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9961 - loss: 0.0179\n",
      "Epoch 17/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9964 - loss: 0.0179\n",
      "Epoch 18/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9966 - loss: 0.0163\n",
      "Epoch 19/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9973 - loss: 0.0121\n",
      "Epoch 20/20\n",
      "1875/1875 - 6s - 3ms/step - acc: 0.9973 - loss: 0.0143\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9763 - loss: 0.2740\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9954914179725731\n",
      "Recall Score: 0.9954333333333333\n",
      "F1 Score 0.9954365804861602\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9797210782133128\n",
      "Recall Score: 0.9795\n",
      "F1 Score 0.9795167234261487\n",
      "Epoch 1/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9341 - loss: 0.2172\n",
      "Epoch 2/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9735 - loss: 0.0865\n",
      "Epoch 3/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9808 - loss: 0.0619\n",
      "Epoch 4/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9853 - loss: 0.0458\n",
      "Epoch 5/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9875 - loss: 0.0385\n",
      "Epoch 6/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9896 - loss: 0.0320\n",
      "Epoch 7/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9916 - loss: 0.0267\n",
      "Epoch 8/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9923 - loss: 0.0243\n",
      "Epoch 9/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9933 - loss: 0.0208\n",
      "Epoch 10/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9936 - loss: 0.0203\n",
      "Epoch 11/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9938 - loss: 0.0183\n",
      "Epoch 12/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9957 - loss: 0.0138\n",
      "Epoch 13/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9953 - loss: 0.0162\n",
      "Epoch 14/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9959 - loss: 0.0136\n",
      "Epoch 15/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9960 - loss: 0.0131\n",
      "Epoch 16/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9956 - loss: 0.0151\n",
      "Epoch 17/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9972 - loss: 0.0103\n",
      "Epoch 18/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9960 - loss: 0.0120\n",
      "Epoch 19/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9958 - loss: 0.0141\n",
      "Epoch 20/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9976 - loss: 0.0079\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9761 - loss: 0.1192\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.997107821315253\n",
      "Recall Score: 0.9970833333333333\n",
      "F1 Score 0.9970850498766213\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9810440928680233\n",
      "Recall Score: 0.9809\n",
      "F1 Score 0.9809146078806403\n",
      "Epoch 1/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9330 - loss: 0.2213\n",
      "Epoch 2/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9719 - loss: 0.0920\n",
      "Epoch 3/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9811 - loss: 0.0629\n",
      "Epoch 4/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9853 - loss: 0.0498\n",
      "Epoch 5/20\n",
      "938/938 - 3s - 4ms/step - acc: 0.9882 - loss: 0.0403\n",
      "Epoch 6/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9904 - loss: 0.0324\n",
      "Epoch 7/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9925 - loss: 0.0263\n",
      "Epoch 8/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9929 - loss: 0.0235\n",
      "Epoch 9/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9945 - loss: 0.0183\n",
      "Epoch 10/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9957 - loss: 0.0152\n",
      "Epoch 11/20\n",
      "938/938 - 5s - 5ms/step - acc: 0.9959 - loss: 0.0143\n",
      "Epoch 12/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9966 - loss: 0.0125\n",
      "Epoch 13/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9971 - loss: 0.0108\n",
      "Epoch 14/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9972 - loss: 0.0100\n",
      "Epoch 15/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9971 - loss: 0.0105\n",
      "Epoch 16/20\n",
      "938/938 - 4s - 4ms/step - acc: 0.9976 - loss: 0.0083\n",
      "Epoch 17/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9975 - loss: 0.0084\n",
      "Epoch 18/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9983 - loss: 0.0059\n",
      "Epoch 19/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9985 - loss: 0.0054\n",
      "Epoch 20/20\n",
      "938/938 - 3s - 3ms/step - acc: 0.9986 - loss: 0.0051\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9774 - loss: 0.1983\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9991181877267801\n",
      "Recall Score: 0.9991166666666667\n",
      "F1 Score 0.9991167436206118\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.981220512954906\n",
      "Recall Score: 0.9811\n",
      "F1 Score 0.9811046568905284\n",
      "Epoch 1/20\n",
      "469/469 - 3s - 7ms/step - acc: 0.9259 - loss: 0.2518\n",
      "Epoch 2/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9719 - loss: 0.0919\n",
      "Epoch 3/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9807 - loss: 0.0624\n",
      "Epoch 4/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9855 - loss: 0.0454\n",
      "Epoch 5/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9884 - loss: 0.0356\n",
      "Epoch 6/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9905 - loss: 0.0287\n",
      "Epoch 7/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9916 - loss: 0.0254\n",
      "Epoch 8/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9927 - loss: 0.0214\n",
      "Epoch 9/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9940 - loss: 0.0182\n",
      "Epoch 10/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9938 - loss: 0.0178\n",
      "Epoch 11/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9944 - loss: 0.0167\n",
      "Epoch 12/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9951 - loss: 0.0151\n",
      "Epoch 13/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9959 - loss: 0.0131\n",
      "Epoch 14/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9958 - loss: 0.0131\n",
      "Epoch 15/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9959 - loss: 0.0135\n",
      "Epoch 16/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9967 - loss: 0.0102\n",
      "Epoch 17/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9962 - loss: 0.0125\n",
      "Epoch 18/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9977 - loss: 0.0077\n",
      "Epoch 19/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9966 - loss: 0.0102\n",
      "Epoch 20/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9973 - loss: 0.0090\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9812 - loss: 0.0993\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.998983562301712\n",
      "Recall Score: 0.9989833333333333\n",
      "F1 Score 0.9989832754476137\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9827025226681033\n",
      "Recall Score: 0.9827\n",
      "F1 Score 0.9826939837248143\n",
      "Epoch 1/20\n",
      "469/469 - 3s - 7ms/step - acc: 0.9198 - loss: 0.2595\n",
      "Epoch 2/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9700 - loss: 0.0989\n",
      "Epoch 3/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9802 - loss: 0.0644\n",
      "Epoch 4/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9855 - loss: 0.0474\n",
      "Epoch 5/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9878 - loss: 0.0372\n",
      "Epoch 6/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9911 - loss: 0.0280\n",
      "Epoch 7/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9925 - loss: 0.0227\n",
      "Epoch 8/20\n",
      "469/469 - 3s - 6ms/step - acc: 0.9942 - loss: 0.0185\n",
      "Epoch 9/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9948 - loss: 0.0156\n",
      "Epoch 10/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9960 - loss: 0.0125\n",
      "Epoch 11/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9963 - loss: 0.0119\n",
      "Epoch 12/20\n",
      "469/469 - 2s - 5ms/step - acc: 0.9967 - loss: 0.0100\n",
      "Epoch 13/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9977 - loss: 0.0086\n",
      "Epoch 14/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9976 - loss: 0.0075\n",
      "Epoch 15/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9977 - loss: 0.0078\n",
      "Epoch 16/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9975 - loss: 0.0073\n",
      "Epoch 17/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9984 - loss: 0.0058\n",
      "Epoch 18/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9987 - loss: 0.0042\n",
      "Epoch 19/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9984 - loss: 0.0051\n",
      "Epoch 20/20\n",
      "469/469 - 2s - 4ms/step - acc: 0.9987 - loss: 0.0043\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - acc: 0.9804 - loss: 0.1494\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9988381753913107\n",
      "Recall Score: 0.9988333333333334\n",
      "F1 Score 0.9988330056588147\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.983087233168787\n",
      "Recall Score: 0.983\n",
      "F1 Score 0.9830013269451623\n",
      "Epoch 1/20\n",
      "235/235 - 3s - 11ms/step - acc: 0.9115 - loss: 0.3073\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9664 - loss: 0.1094\n",
      "Epoch 3/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9782 - loss: 0.0704\n",
      "Epoch 4/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9833 - loss: 0.0526\n",
      "Epoch 5/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9889 - loss: 0.0355\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9908 - loss: 0.0285\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9929 - loss: 0.0215\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9938 - loss: 0.0186\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9940 - loss: 0.0183\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 5ms/step - acc: 0.9951 - loss: 0.0147\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9954 - loss: 0.0132\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9950 - loss: 0.0143\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9959 - loss: 0.0120\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9967 - loss: 0.0093\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9968 - loss: 0.0097\n",
      "Epoch 16/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9969 - loss: 0.0094\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9956 - loss: 0.0132\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9979 - loss: 0.0065\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9969 - loss: 0.0091\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9976 - loss: 0.0070\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9755 - loss: 0.1211\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9970122175722542\n",
      "Recall Score: 0.997\n",
      "F1 Score 0.9970005740750645\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9785584520945314\n",
      "Recall Score: 0.9784\n",
      "F1 Score 0.9784063834567641\n",
      "Epoch 1/20\n",
      "235/235 - 2s - 10ms/step - acc: 0.8964 - loss: 0.3363\n",
      "Epoch 2/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9632 - loss: 0.1182\n",
      "Epoch 3/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9760 - loss: 0.0760\n",
      "Epoch 4/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9826 - loss: 0.0547\n",
      "Epoch 5/20\n",
      "235/235 - 2s - 10ms/step - acc: 0.9868 - loss: 0.0410\n",
      "Epoch 6/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9904 - loss: 0.0299\n",
      "Epoch 7/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9926 - loss: 0.0226\n",
      "Epoch 8/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9937 - loss: 0.0196\n",
      "Epoch 9/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9952 - loss: 0.0153\n",
      "Epoch 10/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9958 - loss: 0.0135\n",
      "Epoch 11/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9970 - loss: 0.0091\n",
      "Epoch 12/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9969 - loss: 0.0091\n",
      "Epoch 13/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9969 - loss: 0.0089\n",
      "Epoch 14/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9979 - loss: 0.0062\n",
      "Epoch 15/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9981 - loss: 0.0063\n",
      "Epoch 16/20\n",
      "235/235 - 2s - 7ms/step - acc: 0.9984 - loss: 0.0051\n",
      "Epoch 17/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9985 - loss: 0.0053\n",
      "Epoch 18/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9987 - loss: 0.0042\n",
      "Epoch 19/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9983 - loss: 0.0048\n",
      "Epoch 20/20\n",
      "235/235 - 1s - 6ms/step - acc: 0.9987 - loss: 0.0041\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9789 - loss: 0.1253\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9993013337314057\n",
      "Recall Score: 0.9993\n",
      "F1 Score 0.9993001475792828\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點256, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9830191405676841\n",
      "Recall Score: 0.983\n",
      "F1 Score 0.9829927844182476\n",
      "Epoch 1/5\n",
      "1875/1875 - 22s - 12ms/step - acc: 0.9407 - loss: 0.1978\n",
      "Epoch 2/5\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9720 - loss: 0.0934\n",
      "Epoch 3/5\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9796 - loss: 0.0675\n",
      "Epoch 4/5\n",
      "1875/1875 - 20s - 10ms/step - acc: 0.9839 - loss: 0.0547\n",
      "Epoch 5/5\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9871 - loss: 0.0427\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9757 - loss: 0.0906\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9914545701976843\n",
      "Recall Score: 0.9914166666666666\n",
      "F1 Score 0.9914086761732779\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9794753465601455\n",
      "Recall Score: 0.9793\n",
      "F1 Score 0.9792857459047017\n",
      "Epoch 1/5\n",
      "1875/1875 - 20s - 10ms/step - acc: 0.9381 - loss: 0.2084\n",
      "Epoch 2/5\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9722 - loss: 0.1030\n",
      "Epoch 3/5\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9800 - loss: 0.0763\n",
      "Epoch 4/5\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9830 - loss: 0.0681\n",
      "Epoch 5/5\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9865 - loss: 0.0577\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9740 - loss: 0.1375\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9898290889217014\n",
      "Recall Score: 0.9897333333333334\n",
      "F1 Score 0.9897412208571535\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9777023058678118\n",
      "Recall Score: 0.9772\n",
      "F1 Score 0.977251375577823\n",
      "Epoch 1/5\n",
      "938/938 - 14s - 15ms/step - acc: 0.9392 - loss: 0.2016\n",
      "Epoch 2/5\n",
      "938/938 - 12s - 13ms/step - acc: 0.9735 - loss: 0.0871\n",
      "Epoch 3/5\n",
      "938/938 - 12s - 13ms/step - acc: 0.9815 - loss: 0.0592\n",
      "Epoch 4/5\n",
      "938/938 - 12s - 13ms/step - acc: 0.9863 - loss: 0.0444\n",
      "Epoch 5/5\n",
      "938/938 - 10s - 11ms/step - acc: 0.9882 - loss: 0.0385\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9735 - loss: 0.0983\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9936483129220639\n",
      "Recall Score: 0.9936166666666667\n",
      "F1 Score 0.9936193813127294\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9787793753467966\n",
      "Recall Score: 0.9786\n",
      "F1 Score 0.9786100560022194\n",
      "Epoch 1/5\n",
      "938/938 - 12s - 12ms/step - acc: 0.9351 - loss: 0.2140\n",
      "Epoch 2/5\n",
      "938/938 - 10s - 11ms/step - acc: 0.9745 - loss: 0.0881\n",
      "Epoch 3/5\n",
      "938/938 - 10s - 11ms/step - acc: 0.9825 - loss: 0.0613\n",
      "Epoch 4/5\n",
      "938/938 - 10s - 11ms/step - acc: 0.9857 - loss: 0.0463\n",
      "Epoch 5/5\n",
      "938/938 - 9s - 10ms/step - acc: 0.9887 - loss: 0.0374\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9803 - loss: 0.0866\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.9956384548554643\n",
      "Recall Score: 0.9956333333333334\n",
      "F1 Score 0.9956333865547785\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9835467252438642\n",
      "Recall Score: 0.9835\n",
      "F1 Score 0.983502557690528\n",
      "Epoch 1/5\n",
      "469/469 - 9s - 19ms/step - acc: 0.9368 - loss: 0.2100\n",
      "Epoch 2/5\n",
      "469/469 - 7s - 15ms/step - acc: 0.9758 - loss: 0.0805\n",
      "Epoch 3/5\n",
      "469/469 - 7s - 15ms/step - acc: 0.9834 - loss: 0.0532\n",
      "Epoch 4/5\n",
      "469/469 - 7s - 15ms/step - acc: 0.9865 - loss: 0.0415\n",
      "Epoch 5/5\n",
      "469/469 - 7s - 15ms/step - acc: 0.9889 - loss: 0.0328\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9696 - loss: 0.1159\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9901857214286185\n",
      "Recall Score: 0.99005\n",
      "F1 Score 0.9900674577805053\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9741879621940756\n",
      "Recall Score: 0.9738\n",
      "F1 Score 0.9738499650851711\n",
      "Epoch 1/5\n",
      "469/469 - 7s - 15ms/step - acc: 0.9234 - loss: 0.2411\n",
      "Epoch 2/5\n",
      "469/469 - 6s - 13ms/step - acc: 0.9730 - loss: 0.0869\n",
      "Epoch 3/5\n",
      "469/469 - 6s - 13ms/step - acc: 0.9826 - loss: 0.0569\n",
      "Epoch 4/5\n",
      "469/469 - 6s - 13ms/step - acc: 0.9876 - loss: 0.0404\n",
      "Epoch 5/5\n",
      "469/469 - 6s - 13ms/step - acc: 0.9902 - loss: 0.0317\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9770 - loss: 0.0907\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9937968026973533\n",
      "Recall Score: 0.9937666666666667\n",
      "F1 Score 0.9937657088471689\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9811381346819992\n",
      "Recall Score: 0.981\n",
      "F1 Score 0.9809989860222138\n",
      "Epoch 1/5\n",
      "235/235 - 6s - 25ms/step - acc: 0.9251 - loss: 0.2501\n",
      "Epoch 2/5\n",
      "235/235 - 4s - 19ms/step - acc: 0.9729 - loss: 0.0866\n",
      "Epoch 3/5\n",
      "235/235 - 5s - 19ms/step - acc: 0.9828 - loss: 0.0553\n",
      "Epoch 4/5\n",
      "235/235 - 5s - 19ms/step - acc: 0.9876 - loss: 0.0388\n",
      "Epoch 5/5\n",
      "235/235 - 4s - 19ms/step - acc: 0.9907 - loss: 0.0286\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9721 - loss: 0.0960\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9915271051552595\n",
      "Recall Score: 0.9914833333333334\n",
      "F1 Score 0.9914754019811867\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9758376088041605\n",
      "Recall Score: 0.9757\n",
      "F1 Score 0.975688083795519\n",
      "Epoch 1/5\n",
      "235/235 - 5s - 20ms/step - acc: 0.9020 - loss: 0.3110\n",
      "Epoch 2/5\n",
      "235/235 - 4s - 17ms/step - acc: 0.9689 - loss: 0.1008\n",
      "Epoch 3/5\n",
      "235/235 - 4s - 16ms/step - acc: 0.9795 - loss: 0.0652\n",
      "Epoch 4/5\n",
      "235/235 - 4s - 17ms/step - acc: 0.9867 - loss: 0.0426\n",
      "Epoch 5/5\n",
      "235/235 - 4s - 16ms/step - acc: 0.9901 - loss: 0.0314\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9791 - loss: 0.0761\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9954536991610485\n",
      "Recall Score: 0.99545\n",
      "F1 Score 0.9954499879741869\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9824490719859769\n",
      "Recall Score: 0.9824\n",
      "F1 Score 0.9823978515714087\n",
      "Epoch 1/10\n",
      "1875/1875 - 23s - 12ms/step - acc: 0.9410 - loss: 0.1961\n",
      "Epoch 2/10\n",
      "1875/1875 - 22s - 12ms/step - acc: 0.9716 - loss: 0.0926\n",
      "Epoch 3/10\n",
      "1875/1875 - 22s - 12ms/step - acc: 0.9800 - loss: 0.0661\n",
      "Epoch 4/10\n",
      "1875/1875 - 22s - 12ms/step - acc: 0.9834 - loss: 0.0540\n",
      "Epoch 5/10\n",
      "1875/1875 - 22s - 12ms/step - acc: 0.9863 - loss: 0.0444\n",
      "Epoch 6/10\n",
      "1875/1875 - 22s - 12ms/step - acc: 0.9884 - loss: 0.0389\n",
      "Epoch 7/10\n",
      "1875/1875 - 20s - 10ms/step - acc: 0.9904 - loss: 0.0327\n",
      "Epoch 8/10\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9915 - loss: 0.0293\n",
      "Epoch 9/10\n",
      "1875/1875 - 20s - 11ms/step - acc: 0.9915 - loss: 0.0276\n",
      "Epoch 10/10\n",
      "1875/1875 - 18s - 10ms/step - acc: 0.9919 - loss: 0.0266\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9759 - loss: 0.1019\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9933841961849507\n",
      "Recall Score: 0.99335\n",
      "F1 Score 0.9933411557473999\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9788380923367229\n",
      "Recall Score: 0.9787\n",
      "F1 Score 0.9786783170100626\n",
      "Epoch 1/10\n",
      "1875/1875 - 20s - 10ms/step - acc: 0.9384 - loss: 0.2107\n",
      "Epoch 2/10\n",
      "1875/1875 - 20s - 10ms/step - acc: 0.9727 - loss: 0.1006\n",
      "Epoch 3/10\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9791 - loss: 0.0805\n",
      "Epoch 4/10\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9828 - loss: 0.0695\n",
      "Epoch 5/10\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9853 - loss: 0.0597\n",
      "Epoch 6/10\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9884 - loss: 0.0491\n",
      "Epoch 7/10\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9898 - loss: 0.0436\n",
      "Epoch 8/10\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9913 - loss: 0.0378\n",
      "Epoch 9/10\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9928 - loss: 0.0338\n",
      "Epoch 10/10\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9941 - loss: 0.0300\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9775 - loss: 0.1607\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9953052990754789\n",
      "Recall Score: 0.9952666666666666\n",
      "F1 Score 0.9952665511367403\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9812160327067623\n",
      "Recall Score: 0.9811\n",
      "F1 Score 0.9810966030934954\n",
      "Epoch 1/10\n",
      "938/938 - 13s - 14ms/step - acc: 0.9409 - loss: 0.1964\n",
      "Epoch 2/10\n",
      "938/938 - 12s - 13ms/step - acc: 0.9743 - loss: 0.0856\n",
      "Epoch 3/10\n",
      "938/938 - 12s - 13ms/step - acc: 0.9810 - loss: 0.0611\n",
      "Epoch 4/10\n",
      "938/938 - 18s - 20ms/step - acc: 0.9853 - loss: 0.0469\n",
      "Epoch 5/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9886 - loss: 0.0367\n",
      "Epoch 6/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9903 - loss: 0.0315\n",
      "Epoch 7/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9909 - loss: 0.0284\n",
      "Epoch 8/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9920 - loss: 0.0269\n",
      "Epoch 9/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9937 - loss: 0.0210\n",
      "Epoch 10/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9933 - loss: 0.0221\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9761 - loss: 0.1087\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9951331435990856\n",
      "Recall Score: 0.9951166666666666\n",
      "F1 Score 0.9951169770486514\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9798108517761129\n",
      "Recall Score: 0.9797\n",
      "F1 Score 0.9796940847757336\n",
      "Epoch 1/10\n",
      "938/938 - 11s - 12ms/step - acc: 0.9336 - loss: 0.2133\n",
      "Epoch 2/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9740 - loss: 0.0890\n",
      "Epoch 3/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9821 - loss: 0.0602\n",
      "Epoch 4/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9863 - loss: 0.0457\n",
      "Epoch 5/10\n",
      "938/938 - 9s - 10ms/step - acc: 0.9890 - loss: 0.0357\n",
      "Epoch 6/10\n",
      "938/938 - 8s - 9ms/step - acc: 0.9917 - loss: 0.0274\n",
      "Epoch 7/10\n",
      "938/938 - 9s - 9ms/step - acc: 0.9934 - loss: 0.0237\n",
      "Epoch 8/10\n",
      "938/938 - 8s - 9ms/step - acc: 0.9940 - loss: 0.0205\n",
      "Epoch 9/10\n",
      "938/938 - 8s - 9ms/step - acc: 0.9958 - loss: 0.0151\n",
      "Epoch 10/10\n",
      "938/938 - 10s - 11ms/step - acc: 0.9964 - loss: 0.0121\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9796 - loss: 0.1404\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9976893960009802\n",
      "Recall Score: 0.9976833333333334\n",
      "F1 Score 0.9976832917849334\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9824783022461974\n",
      "Recall Score: 0.9824\n",
      "F1 Score 0.982396646051242\n",
      "Epoch 1/10\n",
      "469/469 - 9s - 19ms/step - acc: 0.9366 - loss: 0.2121\n",
      "Epoch 2/10\n",
      "469/469 - 7s - 15ms/step - acc: 0.9737 - loss: 0.0823\n",
      "Epoch 3/10\n",
      "469/469 - 7s - 15ms/step - acc: 0.9833 - loss: 0.0534\n",
      "Epoch 4/10\n",
      "469/469 - 7s - 15ms/step - acc: 0.9869 - loss: 0.0413\n",
      "Epoch 5/10\n",
      "469/469 - 7s - 15ms/step - acc: 0.9895 - loss: 0.0312\n",
      "Epoch 6/10\n",
      "469/469 - 7s - 15ms/step - acc: 0.9901 - loss: 0.0307\n",
      "Epoch 7/10\n",
      "469/469 - 6s - 13ms/step - acc: 0.9929 - loss: 0.0223\n",
      "Epoch 8/10\n",
      "469/469 - 6s - 12ms/step - acc: 0.9930 - loss: 0.0211\n",
      "Epoch 9/10\n",
      "469/469 - 6s - 12ms/step - acc: 0.9938 - loss: 0.0191\n",
      "Epoch 10/10\n",
      "469/469 - 6s - 13ms/step - acc: 0.9950 - loss: 0.0165\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9741 - loss: 0.1143\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9954038564987835\n",
      "Recall Score: 0.9953833333333333\n",
      "F1 Score 0.9953826613163305\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9788611132101219\n",
      "Recall Score: 0.9787\n",
      "F1 Score 0.9787114967010994\n",
      "Epoch 1/10\n",
      "469/469 - 7s - 15ms/step - acc: 0.9247 - loss: 0.2403\n",
      "Epoch 2/10\n",
      "469/469 - 6s - 13ms/step - acc: 0.9739 - loss: 0.0859\n",
      "Epoch 3/10\n",
      "469/469 - 10s - 22ms/step - acc: 0.9824 - loss: 0.0560\n",
      "Epoch 4/10\n",
      "469/469 - 6s - 12ms/step - acc: 0.9871 - loss: 0.0403\n",
      "Epoch 5/10\n",
      "469/469 - 6s - 13ms/step - acc: 0.9906 - loss: 0.0301\n",
      "Epoch 6/10\n",
      "469/469 - 6s - 13ms/step - acc: 0.9928 - loss: 0.0225\n",
      "Epoch 7/10\n",
      "469/469 - 5s - 11ms/step - acc: 0.9941 - loss: 0.0183\n",
      "Epoch 8/10\n",
      "469/469 - 5s - 10ms/step - acc: 0.9954 - loss: 0.0151\n",
      "Epoch 9/10\n",
      "469/469 - 5s - 11ms/step - acc: 0.9963 - loss: 0.0108\n",
      "Epoch 10/10\n",
      "469/469 - 5s - 10ms/step - acc: 0.9964 - loss: 0.0111\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9812 - loss: 0.0989\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9989347646483484\n",
      "Recall Score: 0.9989333333333333\n",
      "F1 Score 0.9989334404506233\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9849600900195166\n",
      "Recall Score: 0.9849\n",
      "F1 Score 0.984906942486785\n",
      "Epoch 1/10\n",
      "235/235 - 6s - 24ms/step - acc: 0.9264 - loss: 0.2529\n",
      "Epoch 2/10\n",
      "235/235 - 4s - 18ms/step - acc: 0.9742 - loss: 0.0850\n",
      "Epoch 3/10\n",
      "235/235 - 4s - 19ms/step - acc: 0.9837 - loss: 0.0529\n",
      "Epoch 4/10\n",
      "235/235 - 4s - 19ms/step - acc: 0.9869 - loss: 0.0409\n",
      "Epoch 5/10\n",
      "235/235 - 5s - 22ms/step - acc: 0.9902 - loss: 0.0288\n",
      "Epoch 6/10\n",
      "235/235 - 4s - 19ms/step - acc: 0.9919 - loss: 0.0241\n",
      "Epoch 7/10\n",
      "235/235 - 4s - 19ms/step - acc: 0.9940 - loss: 0.0184\n",
      "Epoch 8/10\n",
      "235/235 - 4s - 19ms/step - acc: 0.9938 - loss: 0.0180\n",
      "Epoch 9/10\n",
      "235/235 - 5s - 19ms/step - acc: 0.9949 - loss: 0.0150\n",
      "Epoch 10/10\n",
      "235/235 - 4s - 19ms/step - acc: 0.9951 - loss: 0.0150\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9789 - loss: 0.0862\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9980693899544021\n",
      "Recall Score: 0.9980666666666667\n",
      "F1 Score 0.9980665261362475\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.981823598513251\n",
      "Recall Score: 0.9818\n",
      "F1 Score 0.9817979054187393\n",
      "Epoch 1/10\n",
      "235/235 - 5s - 21ms/step - acc: 0.9047 - loss: 0.3057\n",
      "Epoch 2/10\n",
      "235/235 - 4s - 17ms/step - acc: 0.9695 - loss: 0.0998\n",
      "Epoch 3/10\n",
      "235/235 - 4s - 17ms/step - acc: 0.9805 - loss: 0.0620\n",
      "Epoch 4/10\n",
      "235/235 - 5s - 22ms/step - acc: 0.9861 - loss: 0.0436\n",
      "Epoch 5/10\n",
      "235/235 - 4s - 16ms/step - acc: 0.9900 - loss: 0.0309\n",
      "Epoch 6/10\n",
      "235/235 - 4s - 17ms/step - acc: 0.9923 - loss: 0.0237\n",
      "Epoch 7/10\n",
      "235/235 - 5s - 21ms/step - acc: 0.9941 - loss: 0.0179\n",
      "Epoch 8/10\n",
      "235/235 - 4s - 16ms/step - acc: 0.9954 - loss: 0.0135\n",
      "Epoch 9/10\n",
      "235/235 - 4s - 17ms/step - acc: 0.9966 - loss: 0.0111\n",
      "Epoch 10/10\n",
      "235/235 - 4s - 16ms/step - acc: 0.9966 - loss: 0.0105\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9760 - loss: 0.1037\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9984870902299554\n",
      "Recall Score: 0.9984833333333333\n",
      "F1 Score 0.9984836375602358\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9806579766989701\n",
      "Recall Score: 0.9805\n",
      "F1 Score 0.9805185416919047\n",
      "Epoch 1/20\n",
      "1875/1875 - 24s - 13ms/step - acc: 0.9413 - loss: 0.1935\n",
      "Epoch 2/20\n",
      "1875/1875 - 22s - 12ms/step - acc: 0.9715 - loss: 0.0922\n",
      "Epoch 3/20\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9797 - loss: 0.0671\n",
      "Epoch 4/20\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9837 - loss: 0.0516\n",
      "Epoch 5/20\n",
      "1875/1875 - 18s - 10ms/step - acc: 0.9873 - loss: 0.0430\n",
      "Epoch 6/20\n",
      "1875/1875 - 18s - 10ms/step - acc: 0.9890 - loss: 0.0357\n",
      "Epoch 7/20\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9901 - loss: 0.0329\n",
      "Epoch 8/20\n",
      "1875/1875 - 18s - 10ms/step - acc: 0.9912 - loss: 0.0303\n",
      "Epoch 9/20\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9921 - loss: 0.0264\n",
      "Epoch 10/20\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9930 - loss: 0.0260\n",
      "Epoch 11/20\n",
      "1875/1875 - 18s - 10ms/step - acc: 0.9937 - loss: 0.0231\n",
      "Epoch 12/20\n",
      "1875/1875 - 20s - 11ms/step - acc: 0.9947 - loss: 0.0196\n",
      "Epoch 13/20\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9941 - loss: 0.0235\n",
      "Epoch 14/20\n",
      "1875/1875 - 20s - 11ms/step - acc: 0.9946 - loss: 0.0194\n",
      "Epoch 15/20\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9953 - loss: 0.0197\n",
      "Epoch 16/20\n",
      "1875/1875 - 20s - 11ms/step - acc: 0.9945 - loss: 0.0231\n",
      "Epoch 17/20\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9953 - loss: 0.0180\n",
      "Epoch 18/20\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9960 - loss: 0.0160\n",
      "Epoch 19/20\n",
      "1875/1875 - 21s - 11ms/step - acc: 0.9962 - loss: 0.0154\n",
      "Epoch 20/20\n",
      "1875/1875 - 38s - 20ms/step - acc: 0.9962 - loss: 0.0152\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9771 - loss: 0.1821\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9970021059284034\n",
      "Recall Score: 0.997\n",
      "F1 Score 0.9969992988084293\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9812149881695502\n",
      "Recall Score: 0.9812\n",
      "F1 Score 0.9811990438364563\n",
      "Epoch 1/20\n",
      "1875/1875 - 19s - 10ms/step - acc: 0.9386 - loss: 0.2079\n",
      "Epoch 2/20\n",
      "1875/1875 - 18s - 10ms/step - acc: 0.9725 - loss: 0.1013\n",
      "Epoch 3/20\n",
      "1875/1875 - 17s - 9ms/step - acc: 0.9799 - loss: 0.0778\n",
      "Epoch 4/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9838 - loss: 0.0646\n",
      "Epoch 5/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9864 - loss: 0.0571\n",
      "Epoch 6/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9887 - loss: 0.0461\n",
      "Epoch 7/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9909 - loss: 0.0398\n",
      "Epoch 8/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9924 - loss: 0.0331\n",
      "Epoch 9/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9931 - loss: 0.0303\n",
      "Epoch 10/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9947 - loss: 0.0230\n",
      "Epoch 11/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9955 - loss: 0.0203\n",
      "Epoch 12/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9960 - loss: 0.0171\n",
      "Epoch 13/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9964 - loss: 0.0163\n",
      "Epoch 14/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9969 - loss: 0.0138\n",
      "Epoch 15/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9978 - loss: 0.0101\n",
      "Epoch 16/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9975 - loss: 0.0123\n",
      "Epoch 17/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9974 - loss: 0.0103\n",
      "Epoch 18/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9983 - loss: 0.0079\n",
      "Epoch 19/20\n",
      "1875/1875 - 16s - 8ms/step - acc: 0.9989 - loss: 0.0056\n",
      "Epoch 20/20\n",
      "1875/1875 - 15s - 8ms/step - acc: 0.9986 - loss: 0.0066\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9758 - loss: 0.2748\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9983734386655783\n",
      "Recall Score: 0.9983666666666666\n",
      "F1 Score 0.9983662623372116\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9809384875987566\n",
      "Recall Score: 0.9808\n",
      "F1 Score 0.9807979937930712\n",
      "Epoch 1/20\n",
      "938/938 - 13s - 14ms/step - acc: 0.9410 - loss: 0.1963\n",
      "Epoch 2/20\n",
      "938/938 - 12s - 13ms/step - acc: 0.9730 - loss: 0.0865\n",
      "Epoch 3/20\n",
      "938/938 - 12s - 13ms/step - acc: 0.9819 - loss: 0.0583\n",
      "Epoch 4/20\n",
      "938/938 - 18s - 19ms/step - acc: 0.9859 - loss: 0.0454\n",
      "Epoch 5/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9878 - loss: 0.0399\n",
      "Epoch 6/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9904 - loss: 0.0308\n",
      "Epoch 7/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9920 - loss: 0.0266\n",
      "Epoch 8/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9925 - loss: 0.0239\n",
      "Epoch 9/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9932 - loss: 0.0231\n",
      "Epoch 10/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9938 - loss: 0.0207\n",
      "Epoch 11/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9952 - loss: 0.0161\n",
      "Epoch 12/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9950 - loss: 0.0168\n",
      "Epoch 13/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9944 - loss: 0.0191\n",
      "Epoch 14/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9961 - loss: 0.0142\n",
      "Epoch 15/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9954 - loss: 0.0162\n",
      "Epoch 16/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9968 - loss: 0.0124\n",
      "Epoch 17/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9964 - loss: 0.0129\n",
      "Epoch 18/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9966 - loss: 0.0116\n",
      "Epoch 19/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9965 - loss: 0.0121\n",
      "Epoch 20/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9972 - loss: 0.0095\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - acc: 0.9796 - loss: 0.1343\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9972949951990238\n",
      "Recall Score: 0.9972833333333333\n",
      "F1 Score 0.9972835359129898\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9817673033451271\n",
      "Recall Score: 0.9817\n",
      "F1 Score 0.9817081405596028\n",
      "Epoch 1/20\n",
      "938/938 - 11s - 12ms/step - acc: 0.9341 - loss: 0.2133\n",
      "Epoch 2/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9740 - loss: 0.0877\n",
      "Epoch 3/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9819 - loss: 0.0618\n",
      "Epoch 4/20\n",
      "938/938 - 10s - 11ms/step - acc: 0.9869 - loss: 0.0452\n",
      "Epoch 5/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9900 - loss: 0.0360\n",
      "Epoch 6/20\n",
      "938/938 - 9s - 9ms/step - acc: 0.9918 - loss: 0.0280\n",
      "Epoch 7/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9932 - loss: 0.0237\n",
      "Epoch 8/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9945 - loss: 0.0199\n",
      "Epoch 9/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9956 - loss: 0.0143\n",
      "Epoch 10/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9964 - loss: 0.0132\n",
      "Epoch 11/20\n",
      "938/938 - 9s - 10ms/step - acc: 0.9969 - loss: 0.0102\n",
      "Epoch 12/20\n",
      "938/938 - 9s - 9ms/step - acc: 0.9969 - loss: 0.0111\n",
      "Epoch 13/20\n",
      "938/938 - 10s - 10ms/step - acc: 0.9981 - loss: 0.0078\n",
      "Epoch 14/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9982 - loss: 0.0061\n",
      "Epoch 15/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9986 - loss: 0.0054\n",
      "Epoch 16/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9987 - loss: 0.0044\n",
      "Epoch 17/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9991 - loss: 0.0028\n",
      "Epoch 18/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9992 - loss: 0.0030\n",
      "Epoch 19/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9990 - loss: 0.0040\n",
      "Epoch 20/20\n",
      "938/938 - 8s - 9ms/step - acc: 0.9990 - loss: 0.0042\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9778 - loss: 0.2050\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.999251839404567\n",
      "Recall Score: 0.99925\n",
      "F1 Score 0.9992500060687891\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9817948132352214\n",
      "Recall Score: 0.9817\n",
      "F1 Score 0.9817082919306779\n",
      "Epoch 1/20\n",
      "469/469 - 9s - 18ms/step - acc: 0.9358 - loss: 0.2126\n",
      "Epoch 2/20\n",
      "469/469 - 10s - 22ms/step - acc: 0.9749 - loss: 0.0820\n",
      "Epoch 3/20\n",
      "469/469 - 8s - 17ms/step - acc: 0.9824 - loss: 0.0557\n",
      "Epoch 4/20\n",
      "469/469 - 10s - 21ms/step - acc: 0.9876 - loss: 0.0384\n",
      "Epoch 5/20\n",
      "469/469 - 7s - 15ms/step - acc: 0.9906 - loss: 0.0301\n",
      "Epoch 6/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9911 - loss: 0.0282\n",
      "Epoch 7/20\n",
      "469/469 - 6s - 12ms/step - acc: 0.9927 - loss: 0.0229\n",
      "Epoch 8/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9935 - loss: 0.0195\n",
      "Epoch 9/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9935 - loss: 0.0203\n",
      "Epoch 10/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9950 - loss: 0.0160\n",
      "Epoch 11/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9945 - loss: 0.0170\n",
      "Epoch 12/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9962 - loss: 0.0126\n",
      "Epoch 13/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9951 - loss: 0.0159\n",
      "Epoch 14/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9960 - loss: 0.0124\n",
      "Epoch 15/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9955 - loss: 0.0140\n",
      "Epoch 16/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9966 - loss: 0.0097\n",
      "Epoch 17/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9971 - loss: 0.0106\n",
      "Epoch 18/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9970 - loss: 0.0101\n",
      "Epoch 19/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9962 - loss: 0.0124\n",
      "Epoch 20/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9977 - loss: 0.0080\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9788 - loss: 0.1113\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9969455398204131\n",
      "Recall Score: 0.9969333333333333\n",
      "F1 Score 0.9969343078582612\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9819665026178929\n",
      "Recall Score: 0.9818\n",
      "F1 Score 0.9818042021407669\n",
      "Epoch 1/20\n",
      "469/469 - 7s - 15ms/step - acc: 0.9237 - loss: 0.2450\n",
      "Epoch 2/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9727 - loss: 0.0872\n",
      "Epoch 3/20\n",
      "469/469 - 6s - 12ms/step - acc: 0.9822 - loss: 0.0571\n",
      "Epoch 4/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9878 - loss: 0.0397\n",
      "Epoch 5/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9903 - loss: 0.0307\n",
      "Epoch 6/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9930 - loss: 0.0223\n",
      "Epoch 7/20\n",
      "469/469 - 6s - 13ms/step - acc: 0.9944 - loss: 0.0176\n",
      "Epoch 8/20\n",
      "469/469 - 9s - 19ms/step - acc: 0.9956 - loss: 0.0142\n",
      "Epoch 9/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9958 - loss: 0.0131\n",
      "Epoch 10/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9968 - loss: 0.0110\n",
      "Epoch 11/20\n",
      "469/469 - 5s - 10ms/step - acc: 0.9974 - loss: 0.0083\n",
      "Epoch 12/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9976 - loss: 0.0082\n",
      "Epoch 13/20\n",
      "469/469 - 5s - 10ms/step - acc: 0.9977 - loss: 0.0080\n",
      "Epoch 14/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9982 - loss: 0.0055\n",
      "Epoch 15/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9986 - loss: 0.0045\n",
      "Epoch 16/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9984 - loss: 0.0050\n",
      "Epoch 17/20\n",
      "469/469 - 5s - 10ms/step - acc: 0.9990 - loss: 0.0035\n",
      "Epoch 18/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9993 - loss: 0.0022\n",
      "Epoch 19/20\n",
      "469/469 - 5s - 10ms/step - acc: 0.9994 - loss: 0.0022\n",
      "Epoch 20/20\n",
      "469/469 - 5s - 11ms/step - acc: 0.9996 - loss: 0.0016\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9843 - loss: 0.1286\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9998667506976595\n",
      "Recall Score: 0.9998666666666667\n",
      "F1 Score 0.9998666662775028\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9862125624492961\n",
      "Recall Score: 0.9862\n",
      "F1 Score 0.9861995662986194\n",
      "Epoch 1/20\n",
      "235/235 - 6s - 25ms/step - acc: 0.9271 - loss: 0.2490\n",
      "Epoch 2/20\n",
      "235/235 - 5s - 20ms/step - acc: 0.9736 - loss: 0.0850\n",
      "Epoch 3/20\n",
      "235/235 - 4s - 19ms/step - acc: 0.9827 - loss: 0.0549\n",
      "Epoch 4/20\n",
      "235/235 - 5s - 22ms/step - acc: 0.9873 - loss: 0.0399\n",
      "Epoch 5/20\n",
      "235/235 - 5s - 19ms/step - acc: 0.9906 - loss: 0.0287\n",
      "Epoch 6/20\n",
      "235/235 - 4s - 19ms/step - acc: 0.9917 - loss: 0.0268\n",
      "Epoch 7/20\n",
      "235/235 - 5s - 20ms/step - acc: 0.9936 - loss: 0.0196\n",
      "Epoch 8/20\n",
      "235/235 - 5s - 19ms/step - acc: 0.9942 - loss: 0.0173\n",
      "Epoch 9/20\n",
      "235/235 - 4s - 19ms/step - acc: 0.9949 - loss: 0.0159\n",
      "Epoch 10/20\n",
      "235/235 - 5s - 22ms/step - acc: 0.9939 - loss: 0.0182\n",
      "Epoch 11/20\n",
      "235/235 - 4s - 15ms/step - acc: 0.9963 - loss: 0.0112\n",
      "Epoch 12/20\n",
      "235/235 - 4s - 15ms/step - acc: 0.9963 - loss: 0.0115\n",
      "Epoch 13/20\n",
      "235/235 - 4s - 16ms/step - acc: 0.9958 - loss: 0.0136\n",
      "Epoch 14/20\n",
      "235/235 - 4s - 15ms/step - acc: 0.9970 - loss: 0.0084\n",
      "Epoch 15/20\n",
      "235/235 - 4s - 16ms/step - acc: 0.9966 - loss: 0.0104\n",
      "Epoch 16/20\n",
      "235/235 - 4s - 16ms/step - acc: 0.9956 - loss: 0.0128\n",
      "Epoch 17/20\n",
      "235/235 - 4s - 16ms/step - acc: 0.9969 - loss: 0.0093\n",
      "Epoch 18/20\n",
      "235/235 - 4s - 16ms/step - acc: 0.9967 - loss: 0.0109\n",
      "Epoch 19/20\n",
      "235/235 - 4s - 16ms/step - acc: 0.9975 - loss: 0.0081\n",
      "Epoch 20/20\n",
      "235/235 - 4s - 15ms/step - acc: 0.9965 - loss: 0.0106\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9785 - loss: 0.1123\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9983690652100287\n",
      "Recall Score: 0.9983666666666666\n",
      "F1 Score 0.9983664172640064\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9820691819135371\n",
      "Recall Score: 0.982\n",
      "F1 Score 0.9820022462410596\n",
      "Epoch 1/20\n",
      "235/235 - 5s - 21ms/step - acc: 0.9002 - loss: 0.3095\n",
      "Epoch 2/20\n",
      "235/235 - 4s - 17ms/step - acc: 0.9688 - loss: 0.0994\n",
      "Epoch 3/20\n",
      "235/235 - 4s - 16ms/step - acc: 0.9804 - loss: 0.0617\n",
      "Epoch 4/20\n",
      "235/235 - 4s - 17ms/step - acc: 0.9864 - loss: 0.0427\n",
      "Epoch 5/20\n",
      "235/235 - 4s - 17ms/step - acc: 0.9897 - loss: 0.0321\n",
      "Epoch 6/20\n",
      "235/235 - 4s - 17ms/step - acc: 0.9926 - loss: 0.0230\n",
      "Epoch 7/20\n",
      "235/235 - 4s - 17ms/step - acc: 0.9942 - loss: 0.0176\n",
      "Epoch 8/20\n",
      "235/235 - 4s - 17ms/step - acc: 0.9953 - loss: 0.0141\n",
      "Epoch 9/20\n",
      "235/235 - 4s - 17ms/step - acc: 0.9966 - loss: 0.0104\n",
      "Epoch 10/20\n",
      "235/235 - 4s - 17ms/step - acc: 0.9966 - loss: 0.0101\n",
      "Epoch 11/20\n",
      "235/235 - 5s - 22ms/step - acc: 0.9975 - loss: 0.0079\n",
      "Epoch 12/20\n",
      "235/235 - 4s - 17ms/step - acc: 0.9978 - loss: 0.0062\n",
      "Epoch 13/20\n",
      "235/235 - 3s - 13ms/step - acc: 0.9982 - loss: 0.0054\n",
      "Epoch 14/20\n",
      "235/235 - 3s - 13ms/step - acc: 0.9983 - loss: 0.0048\n",
      "Epoch 15/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9985 - loss: 0.0048\n",
      "Epoch 16/20\n",
      "235/235 - 3s - 13ms/step - acc: 0.9987 - loss: 0.0039\n",
      "Epoch 17/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9987 - loss: 0.0045\n",
      "Epoch 18/20\n",
      "235/235 - 3s - 13ms/step - acc: 0.9987 - loss: 0.0039\n",
      "Epoch 19/20\n",
      "235/235 - 3s - 14ms/step - acc: 0.9989 - loss: 0.0036\n",
      "Epoch 20/20\n",
      "235/235 - 3s - 13ms/step - acc: 0.9994 - loss: 0.0020\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - acc: 0.9750 - loss: 0.1561\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.99621841271372\n",
      "Recall Score: 0.9961666666666666\n",
      "F1 Score 0.9961567302134449\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點512, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9800722545458348\n",
      "Recall Score: 0.9798\n",
      "F1 Score 0.9798012144419401\n",
      "Epoch 1/5\n",
      "1875/1875 - 71s - 38ms/step - acc: 0.9392 - loss: 0.2021\n",
      "Epoch 2/5\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9711 - loss: 0.0991\n",
      "Epoch 3/5\n",
      "1875/1875 - 61s - 33ms/step - acc: 0.9793 - loss: 0.0722\n",
      "Epoch 4/5\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9832 - loss: 0.0569\n",
      "Epoch 5/5\n",
      "1875/1875 - 61s - 33ms/step - acc: 0.9858 - loss: 0.0478\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - acc: 0.9711 - loss: 0.1215\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9901373260761146\n",
      "Recall Score: 0.9900833333333333\n",
      "F1 Score 0.9900905090700632\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9753510058270578\n",
      "Recall Score: 0.9752\n",
      "F1 Score 0.9752043569088419\n",
      "Epoch 1/5\n",
      "1875/1875 - 59s - 31ms/step - acc: 0.9379 - loss: 0.2157\n",
      "Epoch 2/5\n",
      "1875/1875 - 55s - 30ms/step - acc: 0.9720 - loss: 0.1085\n",
      "Epoch 3/5\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9788 - loss: 0.0835\n",
      "Epoch 4/5\n",
      "1875/1875 - 50s - 26ms/step - acc: 0.9840 - loss: 0.0669\n",
      "Epoch 5/5\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9872 - loss: 0.0559\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9729 - loss: 0.1312\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch32 訓練集績效:\n",
      "Precision Score: 0.9888373619861549\n",
      "Recall Score: 0.9886833333333334\n",
      "F1 Score 0.9886767201805315\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch32 測試集績效:\n",
      "Precision Score: 0.9770870100247053\n",
      "Recall Score: 0.9768\n",
      "F1 Score 0.976773915752169\n",
      "Epoch 1/5\n",
      "938/938 - 42s - 44ms/step - acc: 0.9411 - loss: 0.1922\n",
      "Epoch 2/5\n",
      "938/938 - 40s - 42ms/step - acc: 0.9732 - loss: 0.0886\n",
      "Epoch 3/5\n",
      "938/938 - 36s - 39ms/step - acc: 0.9811 - loss: 0.0633\n",
      "Epoch 4/5\n",
      "938/938 - 33s - 35ms/step - acc: 0.9849 - loss: 0.0492\n",
      "Epoch 5/5\n",
      "938/938 - 33s - 35ms/step - acc: 0.9871 - loss: 0.0420\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - acc: 0.9720 - loss: 0.1128\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.990638397322713\n",
      "Recall Score: 0.9904666666666667\n",
      "F1 Score 0.9904883484813856\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9766063716339857\n",
      "Recall Score: 0.9762\n",
      "F1 Score 0.9762294031375969\n",
      "Epoch 1/5\n",
      "938/938 - 33s - 36ms/step - acc: 0.9340 - loss: 0.2168\n",
      "Epoch 2/5\n",
      "938/938 - 32s - 34ms/step - acc: 0.9746 - loss: 0.0886\n",
      "Epoch 3/5\n",
      "938/938 - 32s - 34ms/step - acc: 0.9827 - loss: 0.0593\n",
      "Epoch 4/5\n",
      "938/938 - 27s - 28ms/step - acc: 0.9872 - loss: 0.0443\n",
      "Epoch 5/5\n",
      "938/938 - 27s - 29ms/step - acc: 0.9900 - loss: 0.0332\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9775 - loss: 0.1101\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch64 訓練集績效:\n",
      "Precision Score: 0.991407499799136\n",
      "Recall Score: 0.9913166666666666\n",
      "F1 Score 0.9913208367131884\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch64 測試集績效:\n",
      "Precision Score: 0.9806536482206543\n",
      "Recall Score: 0.9805\n",
      "F1 Score 0.9805036992875635\n",
      "Epoch 1/5\n",
      "469/469 - 25s - 54ms/step - acc: 0.9403 - loss: 0.1980\n",
      "Epoch 2/5\n",
      "469/469 - 26s - 54ms/step - acc: 0.9747 - loss: 0.0813\n",
      "Epoch 3/5\n",
      "469/469 - 25s - 54ms/step - acc: 0.9834 - loss: 0.0525\n",
      "Epoch 4/5\n",
      "469/469 - 26s - 55ms/step - acc: 0.9866 - loss: 0.0436\n",
      "Epoch 5/5\n",
      "469/469 - 21s - 44ms/step - acc: 0.9894 - loss: 0.0326\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - acc: 0.9731 - loss: 0.1193\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9901481295808876\n",
      "Recall Score: 0.9899833333333333\n",
      "F1 Score 0.9900015625389574\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9769775355071295\n",
      "Recall Score: 0.9767\n",
      "F1 Score 0.9767175700374723\n",
      "Epoch 1/5\n",
      "469/469 - 20s - 43ms/step - acc: 0.9244 - loss: 0.2422\n",
      "Epoch 2/5\n",
      "469/469 - 19s - 40ms/step - acc: 0.9739 - loss: 0.0854\n",
      "Epoch 3/5\n",
      "469/469 - 19s - 40ms/step - acc: 0.9830 - loss: 0.0555\n",
      "Epoch 4/5\n",
      "469/469 - 19s - 40ms/step - acc: 0.9876 - loss: 0.0386\n",
      "Epoch 5/5\n",
      "469/469 - 19s - 40ms/step - acc: 0.9911 - loss: 0.0288\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - acc: 0.9790 - loss: 0.0871\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch128 訓練集績效:\n",
      "Precision Score: 0.9937180838975518\n",
      "Recall Score: 0.9936833333333334\n",
      "F1 Score 0.9936827871419419\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch128 測試集績效:\n",
      "Precision Score: 0.9820048334894503\n",
      "Recall Score: 0.9819\n",
      "F1 Score 0.9818887619041883\n",
      "Epoch 1/5\n",
      "235/235 - 16s - 68ms/step - acc: 0.9352 - loss: 0.2146\n",
      "Epoch 2/5\n",
      "235/235 - 15s - 62ms/step - acc: 0.9764 - loss: 0.0770\n",
      "Epoch 3/5\n",
      "235/235 - 15s - 62ms/step - acc: 0.9842 - loss: 0.0499\n",
      "Epoch 4/5\n",
      "235/235 - 15s - 62ms/step - acc: 0.9885 - loss: 0.0363\n",
      "Epoch 5/5\n",
      "235/235 - 15s - 62ms/step - acc: 0.9905 - loss: 0.0278\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - acc: 0.9634 - loss: 0.1223\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.984459928459507\n",
      "Recall Score: 0.9840333333333333\n",
      "F1 Score 0.9839666943063472\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9701051055964508\n",
      "Recall Score: 0.9693\n",
      "F1 Score 0.969174509112793\n",
      "Epoch 1/5\n",
      "235/235 - 14s - 59ms/step - acc: 0.9000 - loss: 0.3248\n",
      "Epoch 2/5\n",
      "235/235 - 13s - 54ms/step - acc: 0.9705 - loss: 0.0968\n",
      "Epoch 3/5\n",
      "235/235 - 13s - 54ms/step - acc: 0.9817 - loss: 0.0577\n",
      "Epoch 4/5\n",
      "235/235 - 13s - 54ms/step - acc: 0.9873 - loss: 0.0395\n",
      "Epoch 5/5\n",
      "235/235 - 13s - 55ms/step - acc: 0.9915 - loss: 0.0276\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9766 - loss: 0.0946\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch256 訓練集績效:\n",
      "Precision Score: 0.9914855015510255\n",
      "Recall Score: 0.9911666666666666\n",
      "F1 Score 0.9912067257673942\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch5, batch256 測試集績效:\n",
      "Precision Score: 0.9790437149702433\n",
      "Recall Score: 0.9784\n",
      "F1 Score 0.9784907796249901\n",
      "Epoch 1/10\n",
      "1875/1875 - 73s - 39ms/step - acc: 0.9390 - loss: 0.2018\n",
      "Epoch 2/10\n",
      "1875/1875 - 72s - 38ms/step - acc: 0.9720 - loss: 0.0984\n",
      "Epoch 3/10\n",
      "1875/1875 - 71s - 38ms/step - acc: 0.9782 - loss: 0.0748\n",
      "Epoch 4/10\n",
      "1875/1875 - 67s - 36ms/step - acc: 0.9835 - loss: 0.0573\n",
      "Epoch 5/10\n",
      "1875/1875 - 64s - 34ms/step - acc: 0.9847 - loss: 0.0515\n",
      "Epoch 6/10\n",
      "1875/1875 - 61s - 33ms/step - acc: 0.9885 - loss: 0.0420\n",
      "Epoch 7/10\n",
      "1875/1875 - 61s - 33ms/step - acc: 0.9896 - loss: 0.0375\n",
      "Epoch 8/10\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9906 - loss: 0.0356\n",
      "Epoch 9/10\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9916 - loss: 0.0303\n",
      "Epoch 10/10\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9917 - loss: 0.0313\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - acc: 0.9788 - loss: 0.1245\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.993808871690445\n",
      "Recall Score: 0.9937833333333334\n",
      "F1 Score 0.9937860933395372\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9821563137614127\n",
      "Recall Score: 0.9821\n",
      "F1 Score 0.9821011592257463\n",
      "Epoch 1/10\n",
      "1875/1875 - 58s - 31ms/step - acc: 0.9373 - loss: 0.2171\n",
      "Epoch 2/10\n",
      "1875/1875 - 53s - 28ms/step - acc: 0.9717 - loss: 0.1050\n",
      "Epoch 3/10\n",
      "1875/1875 - 50s - 26ms/step - acc: 0.9800 - loss: 0.0786\n",
      "Epoch 4/10\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9846 - loss: 0.0617\n",
      "Epoch 5/10\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9875 - loss: 0.0531\n",
      "Epoch 6/10\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9902 - loss: 0.0424\n",
      "Epoch 7/10\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9922 - loss: 0.0345\n",
      "Epoch 8/10\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9929 - loss: 0.0321\n",
      "Epoch 9/10\n",
      "1875/1875 - 48s - 26ms/step - acc: 0.9939 - loss: 0.0288\n",
      "Epoch 10/10\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9958 - loss: 0.0191\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - acc: 0.9801 - loss: 0.1957\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch32 訓練集績效:\n",
      "Precision Score: 0.9972234574230043\n",
      "Recall Score: 0.9972166666666666\n",
      "F1 Score 0.9972172425801278\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch32 測試集績效:\n",
      "Precision Score: 0.9846648364021783\n",
      "Recall Score: 0.9846\n",
      "F1 Score 0.9846057691364384\n",
      "Epoch 1/10\n",
      "938/938 - 41s - 44ms/step - acc: 0.9428 - loss: 0.1902\n",
      "Epoch 2/10\n",
      "938/938 - 39s - 42ms/step - acc: 0.9736 - loss: 0.0873\n",
      "Epoch 3/10\n",
      "938/938 - 36s - 39ms/step - acc: 0.9811 - loss: 0.0626\n",
      "Epoch 4/10\n",
      "938/938 - 33s - 35ms/step - acc: 0.9848 - loss: 0.0491\n",
      "Epoch 5/10\n",
      "938/938 - 33s - 36ms/step - acc: 0.9866 - loss: 0.0437\n",
      "Epoch 6/10\n",
      "938/938 - 34s - 36ms/step - acc: 0.9887 - loss: 0.0368\n",
      "Epoch 7/10\n",
      "938/938 - 33s - 35ms/step - acc: 0.9915 - loss: 0.0280\n",
      "Epoch 8/10\n",
      "938/938 - 33s - 35ms/step - acc: 0.9922 - loss: 0.0266\n",
      "Epoch 9/10\n",
      "938/938 - 33s - 35ms/step - acc: 0.9926 - loss: 0.0258\n",
      "Epoch 10/10\n",
      "938/938 - 35s - 37ms/step - acc: 0.9934 - loss: 0.0213\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - acc: 0.9783 - loss: 0.1308\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9951022607578272\n",
      "Recall Score: 0.9950666666666667\n",
      "F1 Score 0.9950619183069742\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.981324221221727\n",
      "Recall Score: 0.9812\n",
      "F1 Score 0.9811879282254852\n",
      "Epoch 1/10\n",
      "938/938 - 34s - 36ms/step - acc: 0.9342 - loss: 0.2182\n",
      "Epoch 2/10\n",
      "938/938 - 32s - 34ms/step - acc: 0.9751 - loss: 0.0866\n",
      "Epoch 3/10\n",
      "938/938 - 32s - 34ms/step - acc: 0.9831 - loss: 0.0593\n",
      "Epoch 4/10\n",
      "938/938 - 27s - 29ms/step - acc: 0.9874 - loss: 0.0442\n",
      "Epoch 5/10\n",
      "938/938 - 27s - 28ms/step - acc: 0.9903 - loss: 0.0333\n",
      "Epoch 6/10\n",
      "938/938 - 27s - 29ms/step - acc: 0.9921 - loss: 0.0269\n",
      "Epoch 7/10\n",
      "938/938 - 27s - 28ms/step - acc: 0.9938 - loss: 0.0216\n",
      "Epoch 8/10\n",
      "938/938 - 27s - 29ms/step - acc: 0.9954 - loss: 0.0164\n",
      "Epoch 9/10\n",
      "938/938 - 27s - 29ms/step - acc: 0.9959 - loss: 0.0154\n",
      "Epoch 10/10\n",
      "938/938 - 27s - 29ms/step - acc: 0.9962 - loss: 0.0124\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9805 - loss: 0.1317\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch64 訓練集績效:\n",
      "Precision Score: 0.9973595816506609\n",
      "Recall Score: 0.99735\n",
      "F1 Score 0.9973513089768599\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch64 測試集績效:\n",
      "Precision Score: 0.9841957914089519\n",
      "Recall Score: 0.9841\n",
      "F1 Score 0.9841166007196309\n",
      "Epoch 1/10\n",
      "469/469 - 26s - 56ms/step - acc: 0.9401 - loss: 0.1954\n",
      "Epoch 2/10\n",
      "469/469 - 21s - 46ms/step - acc: 0.9748 - loss: 0.0826\n",
      "Epoch 3/10\n",
      "469/469 - 22s - 46ms/step - acc: 0.9824 - loss: 0.0563\n",
      "Epoch 4/10\n",
      "469/469 - 21s - 45ms/step - acc: 0.9871 - loss: 0.0409\n",
      "Epoch 5/10\n",
      "469/469 - 18s - 39ms/step - acc: 0.9892 - loss: 0.0345\n",
      "Epoch 6/10\n",
      "469/469 - 20s - 43ms/step - acc: 0.9914 - loss: 0.0287\n",
      "Epoch 7/10\n",
      "469/469 - 19s - 40ms/step - acc: 0.9915 - loss: 0.0260\n",
      "Epoch 8/10\n",
      "469/469 - 19s - 40ms/step - acc: 0.9934 - loss: 0.0211\n",
      "Epoch 9/10\n",
      "469/469 - 19s - 40ms/step - acc: 0.9921 - loss: 0.0258\n",
      "Epoch 10/10\n",
      "469/469 - 19s - 40ms/step - acc: 0.9939 - loss: 0.0189\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - acc: 0.9770 - loss: 0.0968\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9924419677105705\n",
      "Recall Score: 0.9924\n",
      "F1 Score 0.9923962746000514\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9796547513979318\n",
      "Recall Score: 0.9796\n",
      "F1 Score 0.9795771234725827\n",
      "Epoch 1/10\n",
      "469/469 - 20s - 43ms/step - acc: 0.9233 - loss: 0.2442\n",
      "Epoch 2/10\n",
      "469/469 - 19s - 40ms/step - acc: 0.9743 - loss: 0.0858\n",
      "Epoch 3/10\n",
      "469/469 - 19s - 40ms/step - acc: 0.9831 - loss: 0.0547\n",
      "Epoch 4/10\n",
      "469/469 - 19s - 40ms/step - acc: 0.9880 - loss: 0.0389\n",
      "Epoch 5/10\n",
      "469/469 - 19s - 39ms/step - acc: 0.9909 - loss: 0.0291\n",
      "Epoch 6/10\n",
      "469/469 - 15s - 32ms/step - acc: 0.9939 - loss: 0.0203\n",
      "Epoch 7/10\n",
      "469/469 - 15s - 33ms/step - acc: 0.9949 - loss: 0.0164\n",
      "Epoch 8/10\n",
      "469/469 - 15s - 33ms/step - acc: 0.9963 - loss: 0.0120\n",
      "Epoch 9/10\n",
      "469/469 - 15s - 33ms/step - acc: 0.9968 - loss: 0.0101\n",
      "Epoch 10/10\n",
      "469/469 - 15s - 33ms/step - acc: 0.9972 - loss: 0.0097\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9782 - loss: 0.1278\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch128 訓練集績效:\n",
      "Precision Score: 0.9954117246763459\n",
      "Recall Score: 0.9953833333333333\n",
      "F1 Score 0.9953816180613934\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch128 測試集績效:\n",
      "Precision Score: 0.9821115798418628\n",
      "Recall Score: 0.982\n",
      "F1 Score 0.9819773680882947\n",
      "Epoch 1/10\n",
      "235/235 - 16s - 68ms/step - acc: 0.9362 - loss: 0.2118\n",
      "Epoch 2/10\n",
      "235/235 - 15s - 62ms/step - acc: 0.9766 - loss: 0.0766\n",
      "Epoch 3/10\n",
      "235/235 - 14s - 61ms/step - acc: 0.9848 - loss: 0.0489\n",
      "Epoch 4/10\n",
      "235/235 - 14s - 61ms/step - acc: 0.9874 - loss: 0.0385\n",
      "Epoch 5/10\n",
      "235/235 - 14s - 61ms/step - acc: 0.9904 - loss: 0.0289\n",
      "Epoch 6/10\n",
      "235/235 - 15s - 62ms/step - acc: 0.9930 - loss: 0.0209\n",
      "Epoch 7/10\n",
      "235/235 - 12s - 50ms/step - acc: 0.9934 - loss: 0.0205\n",
      "Epoch 8/10\n",
      "235/235 - 11s - 49ms/step - acc: 0.9937 - loss: 0.0194\n",
      "Epoch 9/10\n",
      "235/235 - 12s - 49ms/step - acc: 0.9948 - loss: 0.0171\n",
      "Epoch 10/10\n",
      "235/235 - 12s - 49ms/step - acc: 0.9945 - loss: 0.0164\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9750 - loss: 0.1134\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9946134603790941\n",
      "Recall Score: 0.9945333333333334\n",
      "F1 Score 0.994543492053999\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9797239808611191\n",
      "Recall Score: 0.9795\n",
      "F1 Score 0.979510447375178\n",
      "Epoch 1/10\n",
      "235/235 - 13s - 57ms/step - acc: 0.9039 - loss: 0.3023\n",
      "Epoch 2/10\n",
      "235/235 - 12s - 52ms/step - acc: 0.9715 - loss: 0.0930\n",
      "Epoch 3/10\n",
      "235/235 - 12s - 52ms/step - acc: 0.9828 - loss: 0.0572\n",
      "Epoch 4/10\n",
      "235/235 - 13s - 54ms/step - acc: 0.9882 - loss: 0.0385\n",
      "Epoch 5/10\n",
      "235/235 - 12s - 53ms/step - acc: 0.9908 - loss: 0.0285\n",
      "Epoch 6/10\n",
      "235/235 - 12s - 52ms/step - acc: 0.9930 - loss: 0.0216\n",
      "Epoch 7/10\n",
      "235/235 - 11s - 46ms/step - acc: 0.9948 - loss: 0.0162\n",
      "Epoch 8/10\n",
      "235/235 - 10s - 42ms/step - acc: 0.9959 - loss: 0.0127\n",
      "Epoch 9/10\n",
      "235/235 - 10s - 42ms/step - acc: 0.9972 - loss: 0.0096\n",
      "Epoch 10/10\n",
      "235/235 - 10s - 42ms/step - acc: 0.9975 - loss: 0.0073\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9785 - loss: 0.1018\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch256 訓練集績效:\n",
      "Precision Score: 0.9982841885073377\n",
      "Recall Score: 0.9982833333333333\n",
      "F1 Score 0.9982831947401285\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch10, batch256 測試集績效:\n",
      "Precision Score: 0.9824272552714519\n",
      "Recall Score: 0.9824\n",
      "F1 Score 0.9824020618710989\n",
      "Epoch 1/20\n",
      "1875/1875 - 74s - 39ms/step - acc: 0.9396 - loss: 0.2034\n",
      "Epoch 2/20\n",
      "1875/1875 - 66s - 35ms/step - acc: 0.9707 - loss: 0.1001\n",
      "Epoch 3/20\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9790 - loss: 0.0753\n",
      "Epoch 4/20\n",
      "1875/1875 - 61s - 33ms/step - acc: 0.9836 - loss: 0.0550\n",
      "Epoch 5/20\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9861 - loss: 0.0499\n",
      "Epoch 6/20\n",
      "1875/1875 - 61s - 33ms/step - acc: 0.9883 - loss: 0.0411\n",
      "Epoch 7/20\n",
      "1875/1875 - 62s - 33ms/step - acc: 0.9898 - loss: 0.0344\n",
      "Epoch 8/20\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9903 - loss: 0.0340\n",
      "Epoch 9/20\n",
      "1875/1875 - 61s - 33ms/step - acc: 0.9906 - loss: 0.0337\n",
      "Epoch 10/20\n",
      "1875/1875 - 61s - 33ms/step - acc: 0.9926 - loss: 0.0273\n",
      "Epoch 11/20\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9921 - loss: 0.0303\n",
      "Epoch 12/20\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9940 - loss: 0.0243\n",
      "Epoch 13/20\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9942 - loss: 0.0241\n",
      "Epoch 14/20\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9940 - loss: 0.0258\n",
      "Epoch 15/20\n",
      "1875/1875 - 61s - 32ms/step - acc: 0.9948 - loss: 0.0198\n",
      "Epoch 16/20\n",
      "1875/1875 - 61s - 33ms/step - acc: 0.9947 - loss: 0.0205\n",
      "Epoch 17/20\n",
      "1875/1875 - 63s - 34ms/step - acc: 0.9952 - loss: 0.0236\n",
      "Epoch 18/20\n",
      "1875/1875 - 63s - 34ms/step - acc: 0.9952 - loss: 0.0233\n",
      "Epoch 19/20\n",
      "1875/1875 - 67s - 36ms/step - acc: 0.9965 - loss: 0.0150\n",
      "Epoch 20/20\n",
      "1875/1875 - 61s - 33ms/step - acc: 0.9955 - loss: 0.0194\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9787 - loss: 0.1780\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9981172971890588\n",
      "Recall Score: 0.9981166666666667\n",
      "F1 Score 0.9981166603198034\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9818176802873025\n",
      "Recall Score: 0.9818\n",
      "F1 Score 0.9817901882813626\n",
      "Epoch 1/20\n",
      "1875/1875 - 59s - 31ms/step - acc: 0.9355 - loss: 0.2183\n",
      "Epoch 2/20\n",
      "1875/1875 - 55s - 29ms/step - acc: 0.9726 - loss: 0.1053\n",
      "Epoch 3/20\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9795 - loss: 0.0811\n",
      "Epoch 4/20\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9843 - loss: 0.0664\n",
      "Epoch 5/20\n",
      "1875/1875 - 83s - 44ms/step - acc: 0.9872 - loss: 0.0526\n",
      "Epoch 6/20\n",
      "1875/1875 - 50s - 26ms/step - acc: 0.9896 - loss: 0.0471\n",
      "Epoch 7/20\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9913 - loss: 0.0394\n",
      "Epoch 8/20\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9927 - loss: 0.0345\n",
      "Epoch 9/20\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9933 - loss: 0.0318\n",
      "Epoch 10/20\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9941 - loss: 0.0232\n",
      "Epoch 11/20\n",
      "1875/1875 - 50s - 27ms/step - acc: 0.9959 - loss: 0.0180\n",
      "Epoch 12/20\n",
      "1875/1875 - 48s - 26ms/step - acc: 0.9968 - loss: 0.0148\n",
      "Epoch 13/20\n",
      "1875/1875 - 48s - 26ms/step - acc: 0.9967 - loss: 0.0161\n",
      "Epoch 14/20\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9967 - loss: 0.0148\n",
      "Epoch 15/20\n",
      "1875/1875 - 48s - 26ms/step - acc: 0.9975 - loss: 0.0115\n",
      "Epoch 16/20\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9978 - loss: 0.0110\n",
      "Epoch 17/20\n",
      "1875/1875 - 48s - 26ms/step - acc: 0.9984 - loss: 0.0070\n",
      "Epoch 18/20\n",
      "1875/1875 - 48s - 26ms/step - acc: 0.9985 - loss: 0.0064\n",
      "Epoch 19/20\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9987 - loss: 0.0049\n",
      "Epoch 20/20\n",
      "1875/1875 - 49s - 26ms/step - acc: 0.9993 - loss: 0.0031\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9812 - loss: 0.2440\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch32 訓練集績效:\n",
      "Precision Score: 0.9994668577653965\n",
      "Recall Score: 0.9994666666666666\n",
      "F1 Score 0.9994666322648121\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch32 測試集績效:\n",
      "Precision Score: 0.9843059906968238\n",
      "Recall Score: 0.9843\n",
      "F1 Score 0.9842881989969817\n",
      "Epoch 1/20\n",
      "938/938 - 41s - 44ms/step - acc: 0.9426 - loss: 0.1908\n",
      "Epoch 2/20\n",
      "938/938 - 40s - 42ms/step - acc: 0.9731 - loss: 0.0890\n",
      "Epoch 3/20\n",
      "938/938 - 37s - 40ms/step - acc: 0.9811 - loss: 0.0617\n",
      "Epoch 4/20\n",
      "938/938 - 33s - 35ms/step - acc: 0.9845 - loss: 0.0511\n",
      "Epoch 5/20\n",
      "938/938 - 34s - 36ms/step - acc: 0.9875 - loss: 0.0425\n",
      "Epoch 6/20\n",
      "938/938 - 33s - 36ms/step - acc: 0.9901 - loss: 0.0326\n",
      "Epoch 7/20\n",
      "938/938 - 33s - 35ms/step - acc: 0.9908 - loss: 0.0318\n",
      "Epoch 8/20\n",
      "938/938 - 36s - 38ms/step - acc: 0.9923 - loss: 0.0255\n",
      "Epoch 9/20\n",
      "938/938 - 34s - 36ms/step - acc: 0.9934 - loss: 0.0234\n",
      "Epoch 10/20\n",
      "938/938 - 36s - 38ms/step - acc: 0.9931 - loss: 0.0238\n",
      "Epoch 11/20\n",
      "938/938 - 36s - 39ms/step - acc: 0.9933 - loss: 0.0229\n",
      "Epoch 12/20\n",
      "938/938 - 35s - 38ms/step - acc: 0.9949 - loss: 0.0187\n",
      "Epoch 13/20\n",
      "938/938 - 34s - 36ms/step - acc: 0.9957 - loss: 0.0155\n",
      "Epoch 14/20\n",
      "938/938 - 34s - 36ms/step - acc: 0.9946 - loss: 0.0190\n",
      "Epoch 15/20\n",
      "938/938 - 34s - 36ms/step - acc: 0.9952 - loss: 0.0182\n",
      "Epoch 16/20\n",
      "938/938 - 34s - 36ms/step - acc: 0.9960 - loss: 0.0143\n",
      "Epoch 17/20\n",
      "938/938 - 34s - 36ms/step - acc: 0.9958 - loss: 0.0170\n",
      "Epoch 18/20\n",
      "938/938 - 34s - 36ms/step - acc: 0.9962 - loss: 0.0139\n",
      "Epoch 19/20\n",
      "938/938 - 33s - 36ms/step - acc: 0.9962 - loss: 0.0151\n",
      "Epoch 20/20\n",
      "938/938 - 34s - 36ms/step - acc: 0.9967 - loss: 0.0144\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - acc: 0.9780 - loss: 0.1537\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9979721425816677\n",
      "Recall Score: 0.9979666666666667\n",
      "F1 Score 0.997967002807371\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.9820934213623616\n",
      "Recall Score: 0.982\n",
      "F1 Score 0.9819990898050733\n",
      "Epoch 1/20\n",
      "938/938 - 33s - 36ms/step - acc: 0.9338 - loss: 0.2182\n",
      "Epoch 2/20\n",
      "938/938 - 32s - 34ms/step - acc: 0.9752 - loss: 0.0885\n",
      "Epoch 3/20\n",
      "938/938 - 31s - 33ms/step - acc: 0.9825 - loss: 0.0590\n",
      "Epoch 4/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9870 - loss: 0.0436\n",
      "Epoch 5/20\n",
      "938/938 - 28s - 29ms/step - acc: 0.9900 - loss: 0.0343\n",
      "Epoch 6/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9923 - loss: 0.0266\n",
      "Epoch 7/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9934 - loss: 0.0221\n",
      "Epoch 8/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9954 - loss: 0.0167\n",
      "Epoch 9/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9960 - loss: 0.0145\n",
      "Epoch 10/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9962 - loss: 0.0134\n",
      "Epoch 11/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9972 - loss: 0.0101\n",
      "Epoch 12/20\n",
      "938/938 - 41s - 43ms/step - acc: 0.9975 - loss: 0.0089\n",
      "Epoch 13/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9977 - loss: 0.0081\n",
      "Epoch 14/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9983 - loss: 0.0063\n",
      "Epoch 15/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9983 - loss: 0.0064\n",
      "Epoch 16/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9981 - loss: 0.0072\n",
      "Epoch 17/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9985 - loss: 0.0063\n",
      "Epoch 18/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9990 - loss: 0.0038\n",
      "Epoch 19/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9995 - loss: 0.0019\n",
      "Epoch 20/20\n",
      "938/938 - 27s - 29ms/step - acc: 0.9993 - loss: 0.0029\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - acc: 0.9815 - loss: 0.1909\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch64 訓練集績效:\n",
      "Precision Score: 0.9991508104769296\n",
      "Recall Score: 0.99915\n",
      "F1 Score 0.9991499250521163\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch64 測試集績效:\n",
      "Precision Score: 0.984047801203774\n",
      "Recall Score: 0.984\n",
      "F1 Score 0.983994061654056\n",
      "Epoch 1/20\n",
      "469/469 - 26s - 55ms/step - acc: 0.9399 - loss: 0.1940\n",
      "Epoch 2/20\n",
      "469/469 - 23s - 50ms/step - acc: 0.9745 - loss: 0.0820\n",
      "Epoch 3/20\n",
      "469/469 - 23s - 49ms/step - acc: 0.9828 - loss: 0.0563\n",
      "Epoch 4/20\n",
      "469/469 - 22s - 48ms/step - acc: 0.9872 - loss: 0.0422\n",
      "Epoch 5/20\n",
      "469/469 - 19s - 40ms/step - acc: 0.9901 - loss: 0.0320\n",
      "Epoch 6/20\n",
      "469/469 - 19s - 41ms/step - acc: 0.9898 - loss: 0.0323\n",
      "Epoch 7/20\n",
      "469/469 - 19s - 40ms/step - acc: 0.9916 - loss: 0.0270\n",
      "Epoch 8/20\n",
      "469/469 - 19s - 41ms/step - acc: 0.9926 - loss: 0.0237\n",
      "Epoch 9/20\n",
      "469/469 - 19s - 40ms/step - acc: 0.9946 - loss: 0.0175\n",
      "Epoch 10/20\n",
      "469/469 - 19s - 40ms/step - acc: 0.9942 - loss: 0.0195\n",
      "Epoch 11/20\n",
      "469/469 - 19s - 41ms/step - acc: 0.9953 - loss: 0.0162\n",
      "Epoch 12/20\n",
      "469/469 - 20s - 42ms/step - acc: 0.9962 - loss: 0.0131\n",
      "Epoch 13/20\n",
      "469/469 - 19s - 41ms/step - acc: 0.9947 - loss: 0.0189\n",
      "Epoch 14/20\n",
      "469/469 - 19s - 41ms/step - acc: 0.9964 - loss: 0.0119\n",
      "Epoch 15/20\n",
      "469/469 - 19s - 40ms/step - acc: 0.9959 - loss: 0.0147\n",
      "Epoch 16/20\n",
      "469/469 - 19s - 40ms/step - acc: 0.9969 - loss: 0.0110\n",
      "Epoch 17/20\n",
      "469/469 - 19s - 40ms/step - acc: 0.9970 - loss: 0.0106\n",
      "Epoch 18/20\n",
      "469/469 - 19s - 40ms/step - acc: 0.9967 - loss: 0.0117\n",
      "Epoch 19/20\n",
      "469/469 - 19s - 40ms/step - acc: 0.9967 - loss: 0.0113\n",
      "Epoch 20/20\n",
      "469/469 - 19s - 40ms/step - acc: 0.9963 - loss: 0.0125\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - acc: 0.9779 - loss: 0.1252\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9974623084614895\n",
      "Recall Score: 0.99745\n",
      "F1 Score 0.9974508640001969\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9826243989309511\n",
      "Recall Score: 0.9825\n",
      "F1 Score 0.9825084314656464\n",
      "Epoch 1/20\n",
      "469/469 - 20s - 42ms/step - acc: 0.9219 - loss: 0.2507\n",
      "Epoch 2/20\n",
      "469/469 - 19s - 40ms/step - acc: 0.9745 - loss: 0.0860\n",
      "Epoch 3/20\n",
      "469/469 - 19s - 41ms/step - acc: 0.9831 - loss: 0.0552\n",
      "Epoch 4/20\n",
      "469/469 - 19s - 41ms/step - acc: 0.9875 - loss: 0.0388\n",
      "Epoch 5/20\n",
      "469/469 - 18s - 37ms/step - acc: 0.9908 - loss: 0.0292\n",
      "Epoch 6/20\n",
      "469/469 - 16s - 34ms/step - acc: 0.9937 - loss: 0.0213\n",
      "Epoch 7/20\n",
      "469/469 - 16s - 34ms/step - acc: 0.9949 - loss: 0.0169\n",
      "Epoch 8/20\n",
      "469/469 - 16s - 33ms/step - acc: 0.9956 - loss: 0.0134\n",
      "Epoch 9/20\n",
      "469/469 - 16s - 33ms/step - acc: 0.9962 - loss: 0.0123\n",
      "Epoch 10/20\n",
      "469/469 - 16s - 33ms/step - acc: 0.9971 - loss: 0.0092\n",
      "Epoch 11/20\n",
      "469/469 - 16s - 34ms/step - acc: 0.9978 - loss: 0.0074\n",
      "Epoch 12/20\n",
      "469/469 - 16s - 33ms/step - acc: 0.9980 - loss: 0.0066\n",
      "Epoch 13/20\n",
      "469/469 - 16s - 34ms/step - acc: 0.9981 - loss: 0.0058\n",
      "Epoch 14/20\n",
      "469/469 - 16s - 34ms/step - acc: 0.9988 - loss: 0.0037\n",
      "Epoch 15/20\n",
      "469/469 - 16s - 34ms/step - acc: 0.9991 - loss: 0.0031\n",
      "Epoch 16/20\n",
      "469/469 - 16s - 33ms/step - acc: 0.9987 - loss: 0.0044\n",
      "Epoch 17/20\n",
      "469/469 - 16s - 33ms/step - acc: 0.9985 - loss: 0.0047\n",
      "Epoch 18/20\n",
      "469/469 - 16s - 33ms/step - acc: 0.9991 - loss: 0.0033\n",
      "Epoch 19/20\n",
      "469/469 - 16s - 33ms/step - acc: 0.9993 - loss: 0.0021\n",
      "Epoch 20/20\n",
      "469/469 - 16s - 33ms/step - acc: 0.9991 - loss: 0.0030\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - acc: 0.9819 - loss: 0.1346\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch128 訓練集績效:\n",
      "Precision Score: 0.9998500351838651\n",
      "Recall Score: 0.99985\n",
      "F1 Score 0.9998499997273472\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch128 測試集績效:\n",
      "Precision Score: 0.9850180659429064\n",
      "Recall Score: 0.985\n",
      "F1 Score 0.984997170408668\n",
      "Epoch 1/20\n",
      "235/235 - 16s - 69ms/step - acc: 0.9355 - loss: 0.2114\n",
      "Epoch 2/20\n",
      "235/235 - 20s - 87ms/step - acc: 0.9765 - loss: 0.0769\n",
      "Epoch 3/20\n",
      "235/235 - 15s - 62ms/step - acc: 0.9846 - loss: 0.0475\n",
      "Epoch 4/20\n",
      "235/235 - 14s - 61ms/step - acc: 0.9881 - loss: 0.0371\n",
      "Epoch 5/20\n",
      "235/235 - 15s - 63ms/step - acc: 0.9913 - loss: 0.0257\n",
      "Epoch 6/20\n",
      "235/235 - 13s - 53ms/step - acc: 0.9916 - loss: 0.0254\n",
      "Epoch 7/20\n",
      "235/235 - 12s - 51ms/step - acc: 0.9926 - loss: 0.0223\n",
      "Epoch 8/20\n",
      "235/235 - 12s - 51ms/step - acc: 0.9927 - loss: 0.0209\n",
      "Epoch 9/20\n",
      "235/235 - 12s - 50ms/step - acc: 0.9943 - loss: 0.0171\n",
      "Epoch 10/20\n",
      "235/235 - 12s - 50ms/step - acc: 0.9958 - loss: 0.0126\n",
      "Epoch 11/20\n",
      "235/235 - 12s - 50ms/step - acc: 0.9950 - loss: 0.0159\n",
      "Epoch 12/20\n",
      "235/235 - 12s - 50ms/step - acc: 0.9967 - loss: 0.0102\n",
      "Epoch 13/20\n",
      "235/235 - 12s - 50ms/step - acc: 0.9959 - loss: 0.0133\n",
      "Epoch 14/20\n",
      "235/235 - 12s - 50ms/step - acc: 0.9959 - loss: 0.0145\n",
      "Epoch 15/20\n",
      "235/235 - 12s - 50ms/step - acc: 0.9964 - loss: 0.0122\n",
      "Epoch 16/20\n",
      "235/235 - 12s - 50ms/step - acc: 0.9961 - loss: 0.0124\n",
      "Epoch 17/20\n",
      "235/235 - 12s - 50ms/step - acc: 0.9976 - loss: 0.0080\n",
      "Epoch 18/20\n",
      "235/235 - 12s - 51ms/step - acc: 0.9972 - loss: 0.0096\n",
      "Epoch 19/20\n",
      "235/235 - 12s - 52ms/step - acc: 0.9978 - loss: 0.0071\n",
      "Epoch 20/20\n",
      "235/235 - 12s - 51ms/step - acc: 0.9973 - loss: 0.0090\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - acc: 0.9784 - loss: 0.1137\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9977231754033545\n",
      "Recall Score: 0.9977166666666667\n",
      "F1 Score 0.9977167874374867\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9821275252182784\n",
      "Recall Score: 0.982\n",
      "F1 Score 0.9820112219514121\n",
      "Epoch 1/20\n",
      "235/235 - 14s - 58ms/step - acc: 0.9023 - loss: 0.3049\n",
      "Epoch 2/20\n",
      "235/235 - 13s - 54ms/step - acc: 0.9719 - loss: 0.0902\n",
      "Epoch 3/20\n",
      "235/235 - 12s - 53ms/step - acc: 0.9817 - loss: 0.0565\n",
      "Epoch 4/20\n",
      "235/235 - 12s - 53ms/step - acc: 0.9869 - loss: 0.0403\n",
      "Epoch 5/20\n",
      "235/235 - 12s - 52ms/step - acc: 0.9916 - loss: 0.0270\n",
      "Epoch 6/20\n",
      "235/235 - 12s - 53ms/step - acc: 0.9932 - loss: 0.0210\n",
      "Epoch 7/20\n",
      "235/235 - 12s - 51ms/step - acc: 0.9943 - loss: 0.0177\n",
      "Epoch 8/20\n",
      "235/235 - 10s - 43ms/step - acc: 0.9961 - loss: 0.0124\n",
      "Epoch 9/20\n",
      "235/235 - 10s - 42ms/step - acc: 0.9966 - loss: 0.0096\n",
      "Epoch 10/20\n",
      "235/235 - 10s - 42ms/step - acc: 0.9972 - loss: 0.0086\n",
      "Epoch 11/20\n",
      "235/235 - 10s - 42ms/step - acc: 0.9977 - loss: 0.0073\n",
      "Epoch 12/20\n",
      "235/235 - 10s - 42ms/step - acc: 0.9990 - loss: 0.0027\n",
      "Epoch 13/20\n",
      "235/235 - 10s - 42ms/step - acc: 0.9992 - loss: 0.0029\n",
      "Epoch 14/20\n",
      "235/235 - 10s - 43ms/step - acc: 0.9988 - loss: 0.0037\n",
      "Epoch 15/20\n",
      "235/235 - 10s - 43ms/step - acc: 0.9988 - loss: 0.0037\n",
      "Epoch 16/20\n",
      "235/235 - 10s - 42ms/step - acc: 0.9994 - loss: 0.0018\n",
      "Epoch 17/20\n",
      "235/235 - 10s - 42ms/step - acc: 0.9987 - loss: 0.0035\n",
      "Epoch 18/20\n",
      "235/235 - 10s - 42ms/step - acc: 0.9995 - loss: 0.0017\n",
      "Epoch 19/20\n",
      "235/235 - 10s - 43ms/step - acc: 0.9995 - loss: 0.0021\n",
      "Epoch 20/20\n",
      "235/235 - 10s - 42ms/step - acc: 0.9995 - loss: 0.0014\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - acc: 0.9800 - loss: 0.1174\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch256 訓練集績效:\n",
      "Precision Score: 0.9995334029150944\n",
      "Recall Score: 0.9995333333333334\n",
      "F1 Score 0.9995332332019573\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--------------------------------------------------\n",
      "隱藏層3, 節點1024, epoch20, batch256 測試集績效:\n",
      "Precision Score: 0.9839210969351919\n",
      "Recall Score: 0.9839\n",
      "F1 Score 0.9838955038714006\n",
      "==================================================\n",
      "Completed batch for layer 3\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_size, layer_number, unites_number, activation, optimizer):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(input_size,)))\n",
    "    for i in range(layer_number):\n",
    "        model.add(layers.Dense(unites_number, activation=activation))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "layers_to_test = [1, 2, 3]\n",
    "unites_number = [64, 128, 256, 512, 1024]\n",
    "epochs_to_test = [5, 10, 20]\n",
    "batch_size = [32, 64, 128, 256]\n",
    "\n",
    "all_record = []\n",
    "\n",
    "activation = 'relu'\n",
    "optimizers = ['adam', 'rmsprop']\n",
    "input_size = train_images.shape[1]\n",
    "train_truth_label = argmax(train_labels, axis=1)\n",
    "test_truth_label = argmax(test_labels, axis=1)\n",
    "\n",
    "\n",
    "for lay in layers_to_test:\n",
    "    for unit in unites_number:\n",
    "        for epo in epochs_to_test:\n",
    "            for bat in batch_size:\n",
    "                for optimizer in optimizers:\n",
    "                    test_model = create_model(input_size, lay, unit, activation, optimizer)\n",
    "                    history = test_model.fit(train_images, train_labels, epochs=epo, batch_size=bat, verbose=2)\n",
    "                    test_loss, test_acc = test_model.evaluate(test_images, test_labels)\n",
    "                    \n",
    "                    train_prediction = test_model.predict(train_images)\n",
    "                    train_prediction_label = argmax(train_prediction, axis=1)\n",
    "            \n",
    "                    train_precision = precision_score(train_truth_label, train_prediction_label, average=\"weighted\", zero_division=1)\n",
    "                    train_recall = recall_score(train_truth_label, train_prediction_label, average=\"weighted\", zero_division=1)\n",
    "                    train_f1 = f1_score(train_truth_label, train_prediction_label, average=\"weighted\", zero_division=1)\n",
    "                    \n",
    "                    print(\"-\"*50)\n",
    "                    print(f\"隱藏層{lay}, 節點{unit}, epoch{epo}, batch{bat} 訓練集績效:\")\n",
    "                    print(\"Precision Score:\", train_precision)\n",
    "                    print(\"Recall Score:\", train_recall)\n",
    "                    print(\"F1 Score\", train_f1)\n",
    "    \n",
    "                    test_prediction = test_model.predict(test_images)\n",
    "                    test_prediction_label = argmax(test_prediction, axis=1)\n",
    "                    \n",
    "                    test_precision = precision_score(test_truth_label, test_prediction_label, average=\"weighted\", zero_division=1)\n",
    "                    test_recall = recall_score(test_truth_label, test_prediction_label, average=\"weighted\", zero_division=1)\n",
    "                    test_f1 = f1_score(test_truth_label, test_prediction_label, average=\"weighted\", zero_division=1)\n",
    "                    \n",
    "                    print(\"-\"*50)\n",
    "                    print(f\"隱藏層{lay}, 節點{unit}, epoch{epo}, batch{bat} 測試集績效:\")\n",
    "                    print(\"Precision Score:\", test_precision)\n",
    "                    print(\"Recall Score:\", test_recall)\n",
    "                    print(\"F1 Score\", test_f1)\n",
    "    \n",
    "                    current_result = {\"layer\": lay,\n",
    "                                      \"units\": unit,\n",
    "                                      \"epochs\": epo,\n",
    "                                      \"batch_size\": bat,\n",
    "                                      \"activation\": activation,\n",
    "                                      \"optimizer\": optimizer,\n",
    "                                      \"train_precision\": train_precision,\n",
    "                                      \"train_recall\": train_recall,\n",
    "                                      \"train_f1\": train_f1,\n",
    "                                      \"test_precision\": test_precision,\n",
    "                                      \"test_recall\": test_recall,\n",
    "                                      \"test_f1\": test_f1\n",
    "                                     }\n",
    "                    \n",
    "                    all_record.append(current_result)\n",
    "                    K.clear_session()\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Completed batch for layer {lay}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d04ed7e-f43c-432d-ae8f-96384bc75959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'layer': 1, 'units': 64, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9819863182944305, 'train_recall': 0.9818166666666667, 'train_f1': 0.9817856884469943, 'test_precision': 0.9694186180968071, 'test_recall': 0.9691, 'test_f1': 0.969041739798803}, {'layer': 1, 'units': 64, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9814885369920594, 'train_recall': 0.9814166666666667, 'train_f1': 0.9814127958239065, 'test_precision': 0.9738613850870086, 'test_recall': 0.9738, 'test_f1': 0.973787409548254}, {'layer': 1, 'units': 64, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9802833546988728, 'train_recall': 0.9802333333333333, 'train_f1': 0.9802164271362876, 'test_precision': 0.9715969965996556, 'test_recall': 0.9715, 'test_f1': 0.9714681808175385}, {'layer': 1, 'units': 64, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9780668278736931, 'train_recall': 0.9779, 'train_f1': 0.9778964395068115, 'test_precision': 0.968966825948843, 'test_recall': 0.9688, 'test_f1': 0.9688115436709736}, {'layer': 1, 'units': 64, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9750746991147236, 'train_recall': 0.97505, 'train_f1': 0.975022791210627, 'test_precision': 0.966796883497835, 'test_recall': 0.9668, 'test_f1': 0.9667682276160479}, {'layer': 1, 'units': 64, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9719392987590523, 'train_recall': 0.9718166666666667, 'train_f1': 0.9718139109385737, 'test_precision': 0.9648303598395424, 'test_recall': 0.9647, 'test_f1': 0.9646661405713641}, {'layer': 1, 'units': 64, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9666273390161563, 'train_recall': 0.9665666666666667, 'train_f1': 0.9665535958248251, 'test_precision': 0.9605512229883035, 'test_recall': 0.9604, 'test_f1': 0.9603640648209543}, {'layer': 1, 'units': 64, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9656598679863312, 'train_recall': 0.9656166666666667, 'train_f1': 0.9655824858255136, 'test_precision': 0.9593938724424341, 'test_recall': 0.9593, 'test_f1': 0.9592360401258747}, {'layer': 1, 'units': 64, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9937498656176073, 'train_recall': 0.9937333333333334, 'train_f1': 0.9937328578868719, 'test_precision': 0.9750714120833347, 'test_recall': 0.975, 'test_f1': 0.9749895332522194}, {'layer': 1, 'units': 64, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9888609154459298, 'train_recall': 0.9888333333333333, 'train_f1': 0.9888326656122609, 'test_precision': 0.9737851365170043, 'test_recall': 0.9737, 'test_f1': 0.973697076697502}, {'layer': 1, 'units': 64, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9902371779533207, 'train_recall': 0.9901833333333333, 'train_f1': 0.9901747974108638, 'test_precision': 0.9732761522790475, 'test_recall': 0.9732, 'test_f1': 0.9731914607055558}, {'layer': 1, 'units': 64, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9884026140992835, 'train_recall': 0.9883833333333333, 'train_f1': 0.9883779083788606, 'test_precision': 0.9746314502033061, 'test_recall': 0.9746, 'test_f1': 0.9745901838983285}, {'layer': 1, 'units': 64, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9873972607287889, 'train_recall': 0.9873666666666666, 'train_f1': 0.9873732948458752, 'test_precision': 0.9765518195857097, 'test_recall': 0.9765, 'test_f1': 0.9765023088550634}, {'layer': 1, 'units': 64, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.985977923872626, 'train_recall': 0.9859166666666667, 'train_f1': 0.9859115142738238, 'test_precision': 0.9745531853799281, 'test_recall': 0.9743, 'test_f1': 0.9742976571991985}, {'layer': 1, 'units': 64, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9806762485195101, 'train_recall': 0.9806, 'train_f1': 0.9806055814068718, 'test_precision': 0.9683350077369345, 'test_recall': 0.9683, 'test_f1': 0.9683027189361297}, {'layer': 1, 'units': 64, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9806965308548371, 'train_recall': 0.9806, 'train_f1': 0.9806056184356188, 'test_precision': 0.9717957045346078, 'test_recall': 0.9717, 'test_f1': 0.971697194793637}, {'layer': 1, 'units': 64, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9980411470502399, 'train_recall': 0.9980333333333333, 'train_f1': 0.998033514092578, 'test_precision': 0.9742053890937171, 'test_recall': 0.9741, 'test_f1': 0.9740989776087757}, {'layer': 1, 'units': 64, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9941140310914329, 'train_recall': 0.9940666666666667, 'train_f1': 0.9940715766751812, 'test_precision': 0.9720571955269648, 'test_recall': 0.9719, 'test_f1': 0.9719039034945168}, {'layer': 1, 'units': 64, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9979054676399388, 'train_recall': 0.9979, 'train_f1': 0.9978997562699087, 'test_precision': 0.9754414050708939, 'test_recall': 0.9753, 'test_f1': 0.9753194740362225}, {'layer': 1, 'units': 64, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9948396732370164, 'train_recall': 0.9948, 'train_f1': 0.9948054251008268, 'test_precision': 0.9734263517519296, 'test_recall': 0.9733, 'test_f1': 0.9733028225338106}, {'layer': 1, 'units': 64, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.996405309019754, 'train_recall': 0.9964, 'train_f1': 0.9963999811297155, 'test_precision': 0.9739309984214451, 'test_recall': 0.9739, 'test_f1': 0.9739000178663387}, {'layer': 1, 'units': 64, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.994561039023261, 'train_recall': 0.9945333333333334, 'train_f1': 0.9945339949264681, 'test_precision': 0.977316095102528, 'test_recall': 0.9772, 'test_f1': 0.9771992522982721}, {'layer': 1, 'units': 64, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9922401633357885, 'train_recall': 0.9922333333333333, 'train_f1': 0.9922340547384605, 'test_precision': 0.9752223303863861, 'test_recall': 0.9752, 'test_f1': 0.9751990154345737}, {'layer': 1, 'units': 64, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9921133013250399, 'train_recall': 0.9921, 'train_f1': 0.9920946461981353, 'test_precision': 0.9745772704728696, 'test_recall': 0.9745, 'test_f1': 0.9744676282257582}, {'layer': 1, 'units': 128, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.99037771702798, 'train_recall': 0.9903333333333333, 'train_f1': 0.990332955459535, 'test_precision': 0.97657921614097, 'test_recall': 0.9765, 'test_f1': 0.9765024783396122}, {'layer': 1, 'units': 128, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9873893475742203, 'train_recall': 0.9873166666666666, 'train_f1': 0.9873145476899138, 'test_precision': 0.9742980005336928, 'test_recall': 0.9741, 'test_f1': 0.9741013514009285}, {'layer': 1, 'units': 128, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9894603863531681, 'train_recall': 0.9894333333333334, 'train_f1': 0.9894315383917724, 'test_precision': 0.9772211288928196, 'test_recall': 0.9771, 'test_f1': 0.9770873554507882}, {'layer': 1, 'units': 128, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9859764700306021, 'train_recall': 0.9859, 'train_f1': 0.9858929481760845, 'test_precision': 0.9749384888453057, 'test_recall': 0.9747, 'test_f1': 0.974679498275498}, {'layer': 1, 'units': 128, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9841108087580853, 'train_recall': 0.9840833333333333, 'train_f1': 0.9840795890409563, 'test_precision': 0.9743037764326993, 'test_recall': 0.9743, 'test_f1': 0.9742879021063549}, {'layer': 1, 'units': 128, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.982172027307669, 'train_recall': 0.98205, 'train_f1': 0.982056397357344, 'test_precision': 0.9735212294139941, 'test_recall': 0.9734, 'test_f1': 0.9733825305621818}, {'layer': 1, 'units': 128, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9756672025562905, 'train_recall': 0.9755666666666667, 'train_f1': 0.9755298729810589, 'test_precision': 0.968972267979087, 'test_recall': 0.9688, 'test_f1': 0.9687481013147566}, {'layer': 1, 'units': 128, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9769163275137633, 'train_recall': 0.9768166666666667, 'train_f1': 0.9767999877600778, 'test_precision': 0.9691652948843795, 'test_recall': 0.9691, 'test_f1': 0.9690789569565572}, {'layer': 1, 'units': 128, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.996569065809689, 'train_recall': 0.99655, 'train_f1': 0.9965504609539013, 'test_precision': 0.9790428866588758, 'test_recall': 0.9789, 'test_f1': 0.9789030128221148}, {'layer': 1, 'units': 128, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9944467939398556, 'train_recall': 0.9944333333333333, 'train_f1': 0.99443307488693, 'test_precision': 0.9769565882192802, 'test_recall': 0.9769, 'test_f1': 0.9768847398348275}, {'layer': 1, 'units': 128, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9937291996162788, 'train_recall': 0.9936166666666667, 'train_f1': 0.9936314292307685, 'test_precision': 0.9762294486251277, 'test_recall': 0.9759, 'test_f1': 0.9759392918067382}, {'layer': 1, 'units': 128, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9933441795578416, 'train_recall': 0.9933, 'train_f1': 0.9932944975385576, 'test_precision': 0.9758102198837956, 'test_recall': 0.9757, 'test_f1': 0.9756785017966242}, {'layer': 1, 'units': 128, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9949226602824921, 'train_recall': 0.9949166666666667, 'train_f1': 0.9949163774129253, 'test_precision': 0.9789279923413143, 'test_recall': 0.9789, 'test_f1': 0.9788956410027487}, {'layer': 1, 'units': 128, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9932367865124158, 'train_recall': 0.9932166666666666, 'train_f1': 0.9932196246745542, 'test_precision': 0.9786682574599137, 'test_recall': 0.9786, 'test_f1': 0.9786005223989589}, {'layer': 1, 'units': 128, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9906324145803933, 'train_recall': 0.9906166666666667, 'train_f1': 0.9906174963604955, 'test_precision': 0.9768502684383132, 'test_recall': 0.9768, 'test_f1': 0.9767901455011364}, {'layer': 1, 'units': 128, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9891915235668595, 'train_recall': 0.98915, 'train_f1': 0.9891493848324894, 'test_precision': 0.9757684560446054, 'test_recall': 0.9756, 'test_f1': 0.9755882618231386}, {'layer': 1, 'units': 128, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9986851782643114, 'train_recall': 0.9986833333333334, 'train_f1': 0.9986827761616391, 'test_precision': 0.9785740619046106, 'test_recall': 0.9785, 'test_f1': 0.9784997316261188}, {'layer': 1, 'units': 128, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.999034125804266, 'train_recall': 0.9990333333333333, 'train_f1': 0.9990333496033855, 'test_precision': 0.9789044052460808, 'test_recall': 0.9788, 'test_f1': 0.9788051958238789}, {'layer': 1, 'units': 128, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9989511808111673, 'train_recall': 0.99895, 'train_f1': 0.998950119617806, 'test_precision': 0.9776812841895682, 'test_recall': 0.9776, 'test_f1': 0.9775993616970158}, {'layer': 1, 'units': 128, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9995667950174904, 'train_recall': 0.9995666666666667, 'train_f1': 0.9995666669077832, 'test_precision': 0.9795454256388466, 'test_recall': 0.9795, 'test_f1': 0.9795011987870914}, {'layer': 1, 'units': 128, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.998968047144063, 'train_recall': 0.9989666666666667, 'train_f1': 0.9989661685972162, 'test_precision': 0.9795680568736862, 'test_recall': 0.9795, 'test_f1': 0.9794909781821521}, {'layer': 1, 'units': 128, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.99893403209195, 'train_recall': 0.9989333333333333, 'train_f1': 0.9989334220045348, 'test_precision': 0.9799524642796048, 'test_recall': 0.9799, 'test_f1': 0.979904405570073}, {'layer': 1, 'units': 128, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9977427513555003, 'train_recall': 0.9977333333333334, 'train_f1': 0.9977332935740002, 'test_precision': 0.9774020122502115, 'test_recall': 0.9773, 'test_f1': 0.9772947099323208}, {'layer': 1, 'units': 128, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9985345004322992, 'train_recall': 0.9985333333333334, 'train_f1': 0.9985333511049017, 'test_precision': 0.9788273621703852, 'test_recall': 0.9788, 'test_f1': 0.978788786583452}, {'layer': 1, 'units': 256, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9910631631977213, 'train_recall': 0.9909, 'train_f1': 0.9909171262897329, 'test_precision': 0.9758005546508842, 'test_recall': 0.9753, 'test_f1': 0.9753423384521964}, {'layer': 1, 'units': 256, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9915813684291518, 'train_recall': 0.99155, 'train_f1': 0.99154911451424, 'test_precision': 0.9775437626409504, 'test_recall': 0.9774, 'test_f1': 0.9773925468792611}, {'layer': 1, 'units': 256, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9921901413911057, 'train_recall': 0.99215, 'train_f1': 0.9921536115438901, 'test_precision': 0.9786194942867272, 'test_recall': 0.9785, 'test_f1': 0.9785082000858367}, {'layer': 1, 'units': 256, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9910359714579552, 'train_recall': 0.9909833333333333, 'train_f1': 0.9909843528473048, 'test_precision': 0.9806186676739157, 'test_recall': 0.9806, 'test_f1': 0.9805886837575155}, {'layer': 1, 'units': 256, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9886936243030289, 'train_recall': 0.9886166666666667, 'train_f1': 0.9886105714163754, 'test_precision': 0.9771821434160485, 'test_recall': 0.9771, 'test_f1': 0.9770772433935266}, {'layer': 1, 'units': 256, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9903731367153014, 'train_recall': 0.9903666666666666, 'train_f1': 0.9903620837510554, 'test_precision': 0.9784378227739368, 'test_recall': 0.9784, 'test_f1': 0.9783865456203154}, {'layer': 1, 'units': 256, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9847284527083197, 'train_recall': 0.9847, 'train_f1': 0.9846932300330427, 'test_precision': 0.9748456029166961, 'test_recall': 0.9748, 'test_f1': 0.9747795474634088}, {'layer': 1, 'units': 256, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9820512653321866, 'train_recall': 0.9818833333333333, 'train_f1': 0.9818728299410656, 'test_precision': 0.9746898870837208, 'test_recall': 0.9745, 'test_f1': 0.9744738710088555}, {'layer': 1, 'units': 256, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9967741666046608, 'train_recall': 0.9967666666666667, 'train_f1': 0.996766401488962, 'test_precision': 0.9789925901591098, 'test_recall': 0.9789, 'test_f1': 0.978908203910565}, {'layer': 1, 'units': 256, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9965261580549818, 'train_recall': 0.9965166666666667, 'train_f1': 0.9965159174294415, 'test_precision': 0.979282993099858, 'test_recall': 0.9792, 'test_f1': 0.9792025464451566}, {'layer': 1, 'units': 256, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9973279783618413, 'train_recall': 0.9973166666666666, 'train_f1': 0.9973165887057907, 'test_precision': 0.9795067738799613, 'test_recall': 0.9794, 'test_f1': 0.979402116518271}, {'layer': 1, 'units': 256, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.996072769489215, 'train_recall': 0.9960166666666667, 'train_f1': 0.9960131064405497, 'test_precision': 0.9781155175526275, 'test_recall': 0.9779, 'test_f1': 0.9779026228397782}, {'layer': 1, 'units': 256, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9977735555241647, 'train_recall': 0.9977666666666667, 'train_f1': 0.9977671529533612, 'test_precision': 0.9818359342282278, 'test_recall': 0.9818, 'test_f1': 0.9818047019765368}, {'layer': 1, 'units': 256, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.996943018306726, 'train_recall': 0.9969333333333333, 'train_f1': 0.9969324321438465, 'test_precision': 0.9821249458327975, 'test_recall': 0.9821, 'test_f1': 0.9820854662861193}, {'layer': 1, 'units': 256, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9948650246799293, 'train_recall': 0.9948333333333333, 'train_f1': 0.9948371621401009, 'test_precision': 0.9779458975894693, 'test_recall': 0.9778, 'test_f1': 0.9778050321677362}, {'layer': 1, 'units': 256, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9942529486502829, 'train_recall': 0.9942333333333333, 'train_f1': 0.9942365017980924, 'test_precision': 0.9794840078967498, 'test_recall': 0.9794, 'test_f1': 0.9794089623644808}, {'layer': 1, 'units': 256, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9982225693599278, 'train_recall': 0.9982166666666666, 'train_f1': 0.9982163088553493, 'test_precision': 0.9801616766197161, 'test_recall': 0.9801, 'test_f1': 0.9800977052581759}, {'layer': 1, 'units': 256, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9999500003755968, 'train_recall': 0.99995, 'train_f1': 0.9999499988578814, 'test_precision': 0.981970847768589, 'test_recall': 0.9819, 'test_f1': 0.9819068083118102}, {'layer': 1, 'units': 256, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9994505380908775, 'train_recall': 0.99945, 'train_f1': 0.9994499765928216, 'test_precision': 0.9816303681522126, 'test_recall': 0.9816, 'test_f1': 0.9815983349789514}, {'layer': 1, 'units': 256, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9999833358050324, 'train_recall': 0.9999833333333333, 'train_f1': 0.9999833332861803, 'test_precision': 0.9815278813845462, 'test_recall': 0.9815, 'test_f1': 0.9814940165862784}, {'layer': 1, 'units': 256, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9999500087377167, 'train_recall': 0.99995, 'train_f1': 0.9999500000912148, 'test_precision': 0.9816979035818855, 'test_recall': 0.9817, 'test_f1': 0.9816971024822222}, {'layer': 1, 'units': 256, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9999500054609779, 'train_recall': 0.99995, 'train_f1': 0.999949998892924, 'test_precision': 0.9820325078465055, 'test_recall': 0.982, 'test_f1': 0.9819988594991781}, {'layer': 1, 'units': 256, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9999166901414634, 'train_recall': 0.9999166666666667, 'train_f1': 0.9999166674838158, 'test_precision': 0.982324435426674, 'test_recall': 0.9823, 'test_f1': 0.9823022952757947}, {'layer': 1, 'units': 256, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9987942453774673, 'train_recall': 0.9987833333333334, 'train_f1': 0.9987854432657566, 'test_precision': 0.9791638313981748, 'test_recall': 0.979, 'test_f1': 0.9790147419164846}, {'layer': 1, 'units': 512, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.993523087780216, 'train_recall': 0.9934333333333333, 'train_f1': 0.9934431229051409, 'test_precision': 0.9794538366710007, 'test_recall': 0.9792, 'test_f1': 0.9792393843702237}, {'layer': 1, 'units': 512, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9930481538060576, 'train_recall': 0.993, 'train_f1': 0.9930049124636318, 'test_precision': 0.9793901004237028, 'test_recall': 0.9792, 'test_f1': 0.9792136826164808}, {'layer': 1, 'units': 512, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.995232457196684, 'train_recall': 0.9952166666666666, 'train_f1': 0.9952148369309577, 'test_precision': 0.981063028596848, 'test_recall': 0.981, 'test_f1': 0.9809980409938344}, {'layer': 1, 'units': 512, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9942072494984081, 'train_recall': 0.9941833333333333, 'train_f1': 0.9941869814017772, 'test_precision': 0.9815569702060256, 'test_recall': 0.9815, 'test_f1': 0.9815084439666736}, {'layer': 1, 'units': 512, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9939758175790686, 'train_recall': 0.9939666666666667, 'train_f1': 0.9939667045961287, 'test_precision': 0.9802645644276892, 'test_recall': 0.9802, 'test_f1': 0.9802034731215196}, {'layer': 1, 'units': 512, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9939980943820742, 'train_recall': 0.9939833333333333, 'train_f1': 0.9939844127458709, 'test_precision': 0.9814347301871172, 'test_recall': 0.9814, 'test_f1': 0.9814030757632342}, {'layer': 1, 'units': 512, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9893791605615246, 'train_recall': 0.9893333333333333, 'train_f1': 0.9893322517280666, 'test_precision': 0.9774503201975809, 'test_recall': 0.9774, 'test_f1': 0.9773979465448424}, {'layer': 1, 'units': 512, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9876679834651427, 'train_recall': 0.9875833333333334, 'train_f1': 0.9875787640722647, 'test_precision': 0.9772213115748959, 'test_recall': 0.9771, 'test_f1': 0.9770902402794136}, {'layer': 1, 'units': 512, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9967229447819316, 'train_recall': 0.9967166666666667, 'train_f1': 0.9967166963150176, 'test_precision': 0.9793522535046232, 'test_recall': 0.9793, 'test_f1': 0.9793024457153225}, {'layer': 1, 'units': 512, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9980375971044662, 'train_recall': 0.9980333333333333, 'train_f1': 0.9980317641985043, 'test_precision': 0.9819654503436647, 'test_recall': 0.9819, 'test_f1': 0.9818819864097728}, {'layer': 1, 'units': 512, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9973830777949936, 'train_recall': 0.9973666666666666, 'train_f1': 0.9973675828742716, 'test_precision': 0.9805274114620953, 'test_recall': 0.9803, 'test_f1': 0.9803191649000961}, {'layer': 1, 'units': 512, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9987525233360306, 'train_recall': 0.99875, 'train_f1': 0.9987497381309678, 'test_precision': 0.9825563606096573, 'test_recall': 0.9825, 'test_f1': 0.9824955489780212}, {'layer': 1, 'units': 512, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9983856787097048, 'train_recall': 0.9983833333333333, 'train_f1': 0.998383291318085, 'test_precision': 0.9815378798298702, 'test_recall': 0.9815, 'test_f1': 0.9814928420671711}, {'layer': 1, 'units': 512, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9987360672783331, 'train_recall': 0.9987333333333334, 'train_f1': 0.9987335371286085, 'test_precision': 0.9813890220361827, 'test_recall': 0.9813, 'test_f1': 0.9813102238054578}, {'layer': 1, 'units': 512, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.998436624615943, 'train_recall': 0.9984333333333333, 'train_f1': 0.9984336194204644, 'test_precision': 0.9812663178159915, 'test_recall': 0.9812, 'test_f1': 0.9812086973696578}, {'layer': 1, 'units': 512, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9978393472091491, 'train_recall': 0.9978333333333333, 'train_f1': 0.9978341140940791, 'test_precision': 0.9804589467105188, 'test_recall': 0.9804, 'test_f1': 0.980399795736554}, {'layer': 1, 'units': 512, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.998551467946111, 'train_recall': 0.99855, 'train_f1': 0.9985497497041635, 'test_precision': 0.9824859598385131, 'test_recall': 0.9824, 'test_f1': 0.9824118960011722}, {'layer': 1, 'units': 512, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9843242766070096, 'test_recall': 0.9843, 'test_f1': 0.9842999609364781}, {'layer': 1, 'units': 512, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9969492603982473, 'train_recall': 0.9969333333333333, 'train_f1': 0.9969336733964689, 'test_precision': 0.9797455293859666, 'test_recall': 0.9796, 'test_f1': 0.979599853691383}, {'layer': 1, 'units': 512, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.983106841847332, 'test_recall': 0.9831, 'test_f1': 0.983098281407508}, {'layer': 1, 'units': 512, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9998833806003737, 'train_recall': 0.9998833333333333, 'train_f1': 0.999883333058716, 'test_precision': 0.9833718975959062, 'test_recall': 0.9833, 'test_f1': 0.983310641062306}, {'layer': 1, 'units': 512, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9828063912550475, 'test_recall': 0.9828, 'test_f1': 0.9827987652392319}, {'layer': 1, 'units': 512, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9819188866944687, 'test_recall': 0.9819, 'test_f1': 0.9818962770879307}, {'layer': 1, 'units': 512, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.983424284490296, 'test_recall': 0.9834, 'test_f1': 0.9834018052542147}, {'layer': 1, 'units': 1024, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9928275450623895, 'train_recall': 0.9927333333333334, 'train_f1': 0.9927387732754475, 'test_precision': 0.9761595674742027, 'test_recall': 0.9757, 'test_f1': 0.9757181625249058}, {'layer': 1, 'units': 1024, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.992728136610935, 'train_recall': 0.9926833333333334, 'train_f1': 0.9926758929105297, 'test_precision': 0.9793543065176531, 'test_recall': 0.9792, 'test_f1': 0.9791798077802457}, {'layer': 1, 'units': 1024, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9943132414532097, 'train_recall': 0.9942666666666666, 'train_f1': 0.9942719390743451, 'test_precision': 0.9808563705174776, 'test_recall': 0.9807, 'test_f1': 0.9807163720835861}, {'layer': 1, 'units': 1024, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9954006103179643, 'train_recall': 0.9953833333333333, 'train_f1': 0.9953847586404436, 'test_precision': 0.9817581151343183, 'test_recall': 0.9817, 'test_f1': 0.9817039749338241}, {'layer': 1, 'units': 1024, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9957849525010971, 'train_recall': 0.9957666666666667, 'train_f1': 0.9957696382740575, 'test_precision': 0.9823058521038227, 'test_recall': 0.9822, 'test_f1': 0.982211585497043}, {'layer': 1, 'units': 1024, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.993580983343827, 'train_recall': 0.99355, 'train_f1': 0.993545590543315, 'test_precision': 0.9801009967939297, 'test_recall': 0.9799, 'test_f1': 0.9799025147820816}, {'layer': 1, 'units': 1024, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9933935424002135, 'train_recall': 0.9933833333333333, 'train_f1': 0.9933753596819896, 'test_precision': 0.9791426278828113, 'test_recall': 0.9791, 'test_f1': 0.9790750833558124}, {'layer': 1, 'units': 1024, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9865276427260339, 'train_recall': 0.9858666666666667, 'train_f1': 0.9859504050352996, 'test_precision': 0.9747449749484056, 'test_recall': 0.9739, 'test_f1': 0.9739663802445111}, {'layer': 1, 'units': 1024, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9966809256370497, 'train_recall': 0.9966666666666667, 'train_f1': 0.9966644792999203, 'test_precision': 0.9815997016560152, 'test_recall': 0.9815, 'test_f1': 0.9814936078812172}, {'layer': 1, 'units': 1024, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9984045949466798, 'train_recall': 0.9984, 'train_f1': 0.9983990662088451, 'test_precision': 0.9817600193164799, 'test_recall': 0.9817, 'test_f1': 0.9816975676002992}, {'layer': 1, 'units': 1024, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9977048147687352, 'train_recall': 0.9977, 'train_f1': 0.9976993303382972, 'test_precision': 0.9801128248615034, 'test_recall': 0.98, 'test_f1': 0.9800073791729031}, {'layer': 1, 'units': 1024, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9994339660375348, 'train_recall': 0.9994333333333333, 'train_f1': 0.9994334000612722, 'test_precision': 0.9830233881625502, 'test_recall': 0.983, 'test_f1': 0.9830025773379343}, {'layer': 1, 'units': 1024, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9989177147860585, 'train_recall': 0.9989166666666667, 'train_f1': 0.9989167987243238, 'test_precision': 0.9815514450153543, 'test_recall': 0.9815, 'test_f1': 0.9814988709811554}, {'layer': 1, 'units': 1024, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.999550562354621, 'train_recall': 0.99955, 'train_f1': 0.9995500440548024, 'test_precision': 0.9834387470506445, 'test_recall': 0.9834, 'test_f1': 0.9833873700494052}, {'layer': 1, 'units': 1024, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.99925051940824, 'train_recall': 0.99925, 'train_f1': 0.9992500405543547, 'test_precision': 0.9837406157194679, 'test_recall': 0.9837, 'test_f1': 0.9837016500673451}, {'layer': 1, 'units': 1024, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9991839124247908, 'train_recall': 0.9991833333333333, 'train_f1': 0.9991833699797031, 'test_precision': 0.9825123893423302, 'test_recall': 0.9825, 'test_f1': 0.9824976249884112}, {'layer': 1, 'units': 1024, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9978212333146476, 'train_recall': 0.9978166666666667, 'train_f1': 0.9978162735535113, 'test_precision': 0.9800597165551662, 'test_recall': 0.98, 'test_f1': 0.9799947828216405}, {'layer': 1, 'units': 1024, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9846098283490803, 'test_recall': 0.9846, 'test_f1': 0.9846008465990969}, {'layer': 1, 'units': 1024, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9974386405800958, 'train_recall': 0.9974333333333333, 'train_f1': 0.9974331300144815, 'test_precision': 0.9807374848788047, 'test_recall': 0.9807, 'test_f1': 0.9806872050152297}, {'layer': 1, 'units': 1024, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9850149846470151, 'test_recall': 0.985, 'test_f1': 0.9850027420047905}, {'layer': 1, 'units': 1024, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9863111393395935, 'test_recall': 0.9863, 'test_f1': 0.9863018174929994}, {'layer': 1, 'units': 1024, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9852132047723157, 'test_recall': 0.9852, 'test_f1': 0.9852003786533089}, {'layer': 1, 'units': 1024, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9841046933521995, 'test_recall': 0.9841, 'test_f1': 0.9841000213540528}, {'layer': 1, 'units': 1024, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9835207876107155, 'test_recall': 0.9835, 'test_f1': 0.9835034110547218}, {'layer': 2, 'units': 64, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.985351284375898, 'train_recall': 0.9852833333333333, 'train_f1': 0.9852910561840799, 'test_precision': 0.9752595027825975, 'test_recall': 0.9752, 'test_f1': 0.975196234006008}, {'layer': 2, 'units': 64, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9842801927014743, 'train_recall': 0.9841166666666666, 'train_f1': 0.9841390944725529, 'test_precision': 0.972820816684749, 'test_recall': 0.9726, 'test_f1': 0.9726446436499595}, {'layer': 2, 'units': 64, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9835902294894856, 'train_recall': 0.9834333333333334, 'train_f1': 0.9834391305742592, 'test_precision': 0.9725850555938148, 'test_recall': 0.9724, 'test_f1': 0.9724307800124327}, {'layer': 2, 'units': 64, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.98558898321895, 'train_recall': 0.98555, 'train_f1': 0.9855521328639979, 'test_precision': 0.9740823725619379, 'test_recall': 0.974, 'test_f1': 0.9740034803528704}, {'layer': 2, 'units': 64, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9800052229740223, 'train_recall': 0.9799, 'train_f1': 0.979899969421461, 'test_precision': 0.9700864246535595, 'test_recall': 0.9697, 'test_f1': 0.9697061061606553}, {'layer': 2, 'units': 64, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9805956109010134, 'train_recall': 0.9805166666666667, 'train_f1': 0.9805042180788602, 'test_precision': 0.9723185501122215, 'test_recall': 0.9722, 'test_f1': 0.9721806968680785}, {'layer': 2, 'units': 64, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9767071235017143, 'train_recall': 0.97665, 'train_f1': 0.9766565876043023, 'test_precision': 0.966189356157122, 'test_recall': 0.9661, 'test_f1': 0.9660891751991696}, {'layer': 2, 'units': 64, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9752430604445353, 'train_recall': 0.9751833333333333, 'train_f1': 0.9751371606843336, 'test_precision': 0.9670886736430002, 'test_recall': 0.9669, 'test_f1': 0.9668311264049556}, {'layer': 2, 'units': 64, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9923525824009133, 'train_recall': 0.9923, 'train_f1': 0.9922974436042411, 'test_precision': 0.9739415623242892, 'test_recall': 0.9738, 'test_f1': 0.9738056551774367}, {'layer': 2, 'units': 64, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9869748330324504, 'train_recall': 0.9868333333333333, 'train_f1': 0.9868250511927094, 'test_precision': 0.9737070274670127, 'test_recall': 0.9734, 'test_f1': 0.9733972209834157}, {'layer': 2, 'units': 64, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9904014725587469, 'train_recall': 0.9903333333333333, 'train_f1': 0.9903270844767008, 'test_precision': 0.9739208346618315, 'test_recall': 0.9738, 'test_f1': 0.9738051971577238}, {'layer': 2, 'units': 64, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9915446141493057, 'train_recall': 0.9915166666666667, 'train_f1': 0.9915134214709056, 'test_precision': 0.9763817323314412, 'test_recall': 0.9763, 'test_f1': 0.9762742887895711}, {'layer': 2, 'units': 64, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9906777030384183, 'train_recall': 0.9906333333333334, 'train_f1': 0.9906375137286918, 'test_precision': 0.9738086354783975, 'test_recall': 0.9737, 'test_f1': 0.973692817036768}, {'layer': 2, 'units': 64, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9918911435276843, 'train_recall': 0.9918666666666667, 'train_f1': 0.991871356650434, 'test_precision': 0.9784669409079546, 'test_recall': 0.9784, 'test_f1': 0.9784062699629021}, {'layer': 2, 'units': 64, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9847251825814161, 'train_recall': 0.9845833333333334, 'train_f1': 0.9845755288698083, 'test_precision': 0.9693566122619617, 'test_recall': 0.969, 'test_f1': 0.9689843185718179}, {'layer': 2, 'units': 64, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9790897216408996, 'train_recall': 0.9784333333333334, 'train_f1': 0.9783792932592144, 'test_precision': 0.9658953680268944, 'test_recall': 0.9646, 'test_f1': 0.9646192134561543}, {'layer': 2, 'units': 64, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9981542052497664, 'train_recall': 0.99815, 'train_f1': 0.9981504539078294, 'test_precision': 0.9740360699099502, 'test_recall': 0.974, 'test_f1': 0.9739972118472795}, {'layer': 2, 'units': 64, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9972407730101432, 'train_recall': 0.9972333333333333, 'train_f1': 0.9972340568406748, 'test_precision': 0.9775476152584839, 'test_recall': 0.9775, 'test_f1': 0.9775027867742472}, {'layer': 2, 'units': 64, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9947049149421144, 'train_recall': 0.99465, 'train_f1': 0.9946491258185343, 'test_precision': 0.9733032621584553, 'test_recall': 0.973, 'test_f1': 0.9729912087953034}, {'layer': 2, 'units': 64, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9963583520105026, 'train_recall': 0.9963166666666666, 'train_f1': 0.9963240640565574, 'test_precision': 0.971901680213027, 'test_recall': 0.9716, 'test_f1': 0.9716419296626305}, {'layer': 2, 'units': 64, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9971379743710952, 'train_recall': 0.9971166666666667, 'train_f1': 0.9971190926602322, 'test_precision': 0.9753015169533424, 'test_recall': 0.9752, 'test_f1': 0.9751991389543977}, {'layer': 2, 'units': 64, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9971230191322586, 'train_recall': 0.9971166666666667, 'train_f1': 0.9971152820084703, 'test_precision': 0.9784862419948198, 'test_recall': 0.9784, 'test_f1': 0.9783903156986493}, {'layer': 2, 'units': 64, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.996368992220894, 'train_recall': 0.99635, 'train_f1': 0.9963471778586164, 'test_precision': 0.9762007428342744, 'test_recall': 0.9761, 'test_f1': 0.9761086151591897}, {'layer': 2, 'units': 64, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9962783580775748, 'train_recall': 0.9962666666666666, 'train_f1': 0.996267733682232, 'test_precision': 0.9763558333363486, 'test_recall': 0.9763, 'test_f1': 0.9762943501815827}, {'layer': 2, 'units': 128, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9908413569455836, 'train_recall': 0.9908, 'train_f1': 0.9908035659281649, 'test_precision': 0.9759561300887161, 'test_recall': 0.9758, 'test_f1': 0.9757937073524227}, {'layer': 2, 'units': 128, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9912963549858596, 'train_recall': 0.9912833333333333, 'train_f1': 0.9912827249212669, 'test_precision': 0.9765339931383494, 'test_recall': 0.9764, 'test_f1': 0.976412535987805}, {'layer': 2, 'units': 128, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9897951905325877, 'train_recall': 0.9897166666666667, 'train_f1': 0.989730053531046, 'test_precision': 0.9763807867938569, 'test_recall': 0.9762, 'test_f1': 0.976225941860153}, {'layer': 2, 'units': 128, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9904013588786006, 'train_recall': 0.9903666666666666, 'train_f1': 0.9903689164926098, 'test_precision': 0.9772772734120261, 'test_recall': 0.9772, 'test_f1': 0.9771935335593307}, {'layer': 2, 'units': 128, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9888852707164406, 'train_recall': 0.9888333333333333, 'train_f1': 0.9888298832731142, 'test_precision': 0.9730422410288262, 'test_recall': 0.9728, 'test_f1': 0.9727754395414089}, {'layer': 2, 'units': 128, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9909492251262498, 'train_recall': 0.9909333333333333, 'train_f1': 0.9909350963029475, 'test_precision': 0.9798518214624601, 'test_recall': 0.9798, 'test_f1': 0.9797965766087853}, {'layer': 2, 'units': 128, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9863688118111436, 'train_recall': 0.9863333333333333, 'train_f1': 0.9863332381482794, 'test_precision': 0.9729813798554585, 'test_recall': 0.9729, 'test_f1': 0.9729061479559877}, {'layer': 2, 'units': 128, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9864087792113243, 'train_recall': 0.9864, 'train_f1': 0.9863972820465458, 'test_precision': 0.9748574824452233, 'test_recall': 0.9748, 'test_f1': 0.9747962194456607}, {'layer': 2, 'units': 128, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9954137406748111, 'train_recall': 0.9954, 'train_f1': 0.9953993064759202, 'test_precision': 0.9770252691372505, 'test_recall': 0.9769, 'test_f1': 0.9768963210472869}, {'layer': 2, 'units': 128, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9959126311734895, 'train_recall': 0.9959, 'train_f1': 0.9958993314887391, 'test_precision': 0.9805543780143428, 'test_recall': 0.9805, 'test_f1': 0.9804959004161301}, {'layer': 2, 'units': 128, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9937426478728001, 'train_recall': 0.9936666666666667, 'train_f1': 0.9936763502196517, 'test_precision': 0.9726173153441183, 'test_recall': 0.9722, 'test_f1': 0.9722438923442407}, {'layer': 2, 'units': 128, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9971531058536377, 'train_recall': 0.99715, 'train_f1': 0.9971500938888402, 'test_precision': 0.9809591520905684, 'test_recall': 0.9809, 'test_f1': 0.9808978732186883}, {'layer': 2, 'units': 128, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9961322742301251, 'train_recall': 0.9961166666666667, 'train_f1': 0.9961164937093879, 'test_precision': 0.9787317221242123, 'test_recall': 0.9786, 'test_f1': 0.9785970508645668}, {'layer': 2, 'units': 128, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9938643331852909, 'train_recall': 0.9937, 'train_f1': 0.993723884046867, 'test_precision': 0.9758655544334808, 'test_recall': 0.9752, 'test_f1': 0.975270856545977}, {'layer': 2, 'units': 128, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.995115056444157, 'train_recall': 0.9951, 'train_f1': 0.9951010281322586, 'test_precision': 0.9777973691658685, 'test_recall': 0.9777, 'test_f1': 0.9777017488415272}, {'layer': 2, 'units': 128, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9960900659861, 'train_recall': 0.9960833333333333, 'train_f1': 0.9960838122818215, 'test_precision': 0.9805450940722943, 'test_recall': 0.9805, 'test_f1': 0.9804966688916684}, {'layer': 2, 'units': 128, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9963484817370523, 'train_recall': 0.9963333333333333, 'train_f1': 0.9963334839638083, 'test_precision': 0.9805417362953536, 'test_recall': 0.9804, 'test_f1': 0.980401448484472}, {'layer': 2, 'units': 128, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9988185309508619, 'train_recall': 0.9988166666666667, 'train_f1': 0.9988165943632138, 'test_precision': 0.9804715426505787, 'test_recall': 0.9804, 'test_f1': 0.9804050953715412}, {'layer': 2, 'units': 128, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9948614617575819, 'train_recall': 0.9948, 'train_f1': 0.994800333718404, 'test_precision': 0.977861243073892, 'test_recall': 0.9777, 'test_f1': 0.9777073010994842}, {'layer': 2, 'units': 128, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9988856800650436, 'train_recall': 0.9988833333333333, 'train_f1': 0.9988835256501098, 'test_precision': 0.9798039605170363, 'test_recall': 0.9797, 'test_f1': 0.9797032183100604}, {'layer': 2, 'units': 128, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9983525932391714, 'train_recall': 0.99835, 'train_f1': 0.9983498340023161, 'test_precision': 0.9785494926797293, 'test_recall': 0.9785, 'test_f1': 0.9784993923704255}, {'layer': 2, 'units': 128, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9993175907975231, 'train_recall': 0.9993166666666666, 'train_f1': 0.9993165185061693, 'test_precision': 0.9797852897956006, 'test_recall': 0.9797, 'test_f1': 0.9796995824833999}, {'layer': 2, 'units': 128, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9984042649384295, 'train_recall': 0.9984, 'train_f1': 0.9983998773903922, 'test_precision': 0.9784202447417992, 'test_recall': 0.9783, 'test_f1': 0.9782959344923756}, {'layer': 2, 'units': 128, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9981451613375972, 'train_recall': 0.9981333333333333, 'train_f1': 0.998134146144987, 'test_precision': 0.9789154443970373, 'test_recall': 0.9788, 'test_f1': 0.978799624600487}, {'layer': 2, 'units': 256, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9926547899365021, 'train_recall': 0.9926, 'train_f1': 0.9926040610372053, 'test_precision': 0.978162143467105, 'test_recall': 0.9779, 'test_f1': 0.977922670306783}, {'layer': 2, 'units': 256, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9919039899374997, 'train_recall': 0.99185, 'train_f1': 0.99185118332401, 'test_precision': 0.9789200888822661, 'test_recall': 0.9788, 'test_f1': 0.9787829652681957}, {'layer': 2, 'units': 256, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9919483047908233, 'train_recall': 0.9918666666666667, 'train_f1': 0.9918670703091872, 'test_precision': 0.9785445928474693, 'test_recall': 0.9784, 'test_f1': 0.9784026900896744}, {'layer': 2, 'units': 256, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9930365521474613, 'train_recall': 0.993, 'train_f1': 0.9929959794428271, 'test_precision': 0.9798229636973141, 'test_recall': 0.9796, 'test_f1': 0.979591793552813}, {'layer': 2, 'units': 256, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9939635868141049, 'train_recall': 0.9939333333333333, 'train_f1': 0.993937220632725, 'test_precision': 0.977912573443532, 'test_recall': 0.9778, 'test_f1': 0.9778071695061109}, {'layer': 2, 'units': 256, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.990827650570299, 'train_recall': 0.9907166666666667, 'train_f1': 0.9907179184865508, 'test_precision': 0.9787087987683836, 'test_recall': 0.9785, 'test_f1': 0.978523200072978}, {'layer': 2, 'units': 256, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9914946994689021, 'train_recall': 0.9914833333333334, 'train_f1': 0.991482141320219, 'test_precision': 0.9790231005340997, 'test_recall': 0.979, 'test_f1': 0.9789912693994285}, {'layer': 2, 'units': 256, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9924416335508965, 'train_recall': 0.9924166666666666, 'train_f1': 0.9924165899766817, 'test_precision': 0.9800061826285781, 'test_recall': 0.9799, 'test_f1': 0.979904248280138}, {'layer': 2, 'units': 256, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9953400976037949, 'train_recall': 0.9953333333333333, 'train_f1': 0.9953315740375622, 'test_precision': 0.9797136503352676, 'test_recall': 0.9797, 'test_f1': 0.9796946162480729}, {'layer': 2, 'units': 256, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9955576094635337, 'train_recall': 0.9955166666666667, 'train_f1': 0.9955205980555637, 'test_precision': 0.9786602776098143, 'test_recall': 0.9785, 'test_f1': 0.9785036714926011}, {'layer': 2, 'units': 256, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9961852146540989, 'train_recall': 0.9961666666666666, 'train_f1': 0.9961659757095679, 'test_precision': 0.9795465171319191, 'test_recall': 0.9794, 'test_f1': 0.9793990943718667}, {'layer': 2, 'units': 256, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9980023733426856, 'train_recall': 0.998, 'train_f1': 0.9979995025549444, 'test_precision': 0.9816661073115002, 'test_recall': 0.9816, 'test_f1': 0.9815903695097798}, {'layer': 2, 'units': 256, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9939529879287312, 'train_recall': 0.9938833333333333, 'train_f1': 0.9938880538517206, 'test_precision': 0.9755992011872553, 'test_recall': 0.9753, 'test_f1': 0.9753085970652273}, {'layer': 2, 'units': 256, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9975265743107685, 'train_recall': 0.9975166666666667, 'train_f1': 0.9975159808585174, 'test_precision': 0.9812076268138202, 'test_recall': 0.9811, 'test_f1': 0.981091087951243}, {'layer': 2, 'units': 256, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9990504179542328, 'train_recall': 0.99905, 'train_f1': 0.9990499500135076, 'test_precision': 0.9817981577525828, 'test_recall': 0.9818, 'test_f1': 0.98179287912071}, {'layer': 2, 'units': 256, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9982520775918425, 'train_recall': 0.99825, 'train_f1': 0.9982497243841487, 'test_precision': 0.9822185899676333, 'test_recall': 0.9822, 'test_f1': 0.9821995693596316}, {'layer': 2, 'units': 256, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9974430553453125, 'train_recall': 0.9974333333333333, 'train_f1': 0.9974338193991888, 'test_precision': 0.9807381233076521, 'test_recall': 0.9806, 'test_f1': 0.9806169872268612}, {'layer': 2, 'units': 256, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9993671814321037, 'train_recall': 0.9993666666666666, 'train_f1': 0.9993665724666987, 'test_precision': 0.9814687230786039, 'test_recall': 0.9814, 'test_f1': 0.981397346798378}, {'layer': 2, 'units': 256, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9975055552673363, 'train_recall': 0.9975, 'train_f1': 0.9975002830095122, 'test_precision': 0.980152777285692, 'test_recall': 0.9801, 'test_f1': 0.9801007605089596}, {'layer': 2, 'units': 256, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9996338775964162, 'train_recall': 0.9996333333333334, 'train_f1': 0.9996332502003415, 'test_precision': 0.9842426085666444, 'test_recall': 0.9842, 'test_f1': 0.9842024327206309}, {'layer': 2, 'units': 256, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9987183189031273, 'train_recall': 0.9987166666666667, 'train_f1': 0.9987167428741588, 'test_precision': 0.9816594501754571, 'test_recall': 0.9816, 'test_f1': 0.98160245008979}, {'layer': 2, 'units': 256, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9999833361491243, 'train_recall': 0.9999833333333333, 'train_f1': 0.9999833334192783, 'test_precision': 0.983432213731576, 'test_recall': 0.9834, 'test_f1': 0.9834027563469337}, {'layer': 2, 'units': 256, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9978701134120801, 'train_recall': 0.9978666666666667, 'train_f1': 0.9978669569604036, 'test_precision': 0.9790295380620774, 'test_recall': 0.979, 'test_f1': 0.9789948764306222}, {'layer': 2, 'units': 256, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9837127243982746, 'test_recall': 0.9837, 'test_f1': 0.9837022751040387}, {'layer': 2, 'units': 512, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9909704139920938, 'train_recall': 0.9909166666666667, 'train_f1': 0.9909258797001748, 'test_precision': 0.9774770543538918, 'test_recall': 0.9772, 'test_f1': 0.977255052648568}, {'layer': 2, 'units': 512, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9941683178902946, 'train_recall': 0.99415, 'train_f1': 0.9941496852638101, 'test_precision': 0.9833000208800642, 'test_recall': 0.9832, 'test_f1': 0.9832043517343518}, {'layer': 2, 'units': 512, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9950713961110792, 'train_recall': 0.9950666666666667, 'train_f1': 0.995064653743663, 'test_precision': 0.9812767695496606, 'test_recall': 0.9812, 'test_f1': 0.9811958140382371}, {'layer': 2, 'units': 512, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9902548920139218, 'train_recall': 0.9901, 'train_f1': 0.9900993923460196, 'test_precision': 0.9771626916408426, 'test_recall': 0.9766, 'test_f1': 0.9766135915175819}, {'layer': 2, 'units': 512, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9937208124021704, 'train_recall': 0.9937, 'train_f1': 0.9936963842083294, 'test_precision': 0.9803629845852915, 'test_recall': 0.9803, 'test_f1': 0.98027912142726}, {'layer': 2, 'units': 512, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9941487377616128, 'train_recall': 0.9941166666666666, 'train_f1': 0.9941172239826642, 'test_precision': 0.9799540483330899, 'test_recall': 0.9798, 'test_f1': 0.9798104518745012}, {'layer': 2, 'units': 512, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9920651615802206, 'train_recall': 0.992, 'train_f1': 0.991993113240206, 'test_precision': 0.9786641201579425, 'test_recall': 0.9785, 'test_f1': 0.978492027121763}, {'layer': 2, 'units': 512, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9907913838004486, 'train_recall': 0.99055, 'train_f1': 0.9905843758416226, 'test_precision': 0.9783144124664165, 'test_recall': 0.9777, 'test_f1': 0.9778041345757472}, {'layer': 2, 'units': 512, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9913534940323443, 'train_recall': 0.99115, 'train_f1': 0.9911618040966993, 'test_precision': 0.9740641041954635, 'test_recall': 0.9737, 'test_f1': 0.973691835207439}, {'layer': 2, 'units': 512, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9968052966721425, 'train_recall': 0.9968, 'train_f1': 0.9968003681252365, 'test_precision': 0.9818409546566065, 'test_recall': 0.9818, 'test_f1': 0.9818059632818548}, {'layer': 2, 'units': 512, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9960446103035576, 'train_recall': 0.9960333333333333, 'train_f1': 0.996034064030169, 'test_precision': 0.9786915752084385, 'test_recall': 0.9786, 'test_f1': 0.9786019475076945}, {'layer': 2, 'units': 512, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9982040391085341, 'train_recall': 0.9982, 'train_f1': 0.9982002054306897, 'test_precision': 0.9852231728124574, 'test_recall': 0.9852, 'test_f1': 0.9852007147758053}, {'layer': 2, 'units': 512, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9979689982048424, 'train_recall': 0.9979666666666667, 'train_f1': 0.9979669171049297, 'test_precision': 0.9831392590807781, 'test_recall': 0.9831, 'test_f1': 0.9831057788536965}, {'layer': 2, 'units': 512, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9982869409971377, 'train_recall': 0.9982833333333333, 'train_f1': 0.9982832600407908, 'test_precision': 0.9823573825542284, 'test_recall': 0.9823, 'test_f1': 0.9822905216824729}, {'layer': 2, 'units': 512, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.997230447777931, 'train_recall': 0.9972166666666666, 'train_f1': 0.9972168015426072, 'test_precision': 0.9785743752624818, 'test_recall': 0.9784, 'test_f1': 0.9784011201488299}, {'layer': 2, 'units': 512, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9983566046916613, 'train_recall': 0.99835, 'train_f1': 0.9983501400076584, 'test_precision': 0.9825941527043497, 'test_recall': 0.9825, 'test_f1': 0.9825023319871034}, {'layer': 2, 'units': 512, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9977196761370051, 'train_recall': 0.9977166666666667, 'train_f1': 0.997716767169853, 'test_precision': 0.9810835087224854, 'test_recall': 0.981, 'test_f1': 0.9810064639098993}, {'layer': 2, 'units': 512, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9997335302690555, 'train_recall': 0.9997333333333334, 'train_f1': 0.9997333287211976, 'test_precision': 0.9859057354041056, 'test_recall': 0.9859, 'test_f1': 0.9858952997825092}, {'layer': 2, 'units': 512, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9975737756232032, 'train_recall': 0.9975666666666667, 'train_f1': 0.9975671028718839, 'test_precision': 0.9813264973548894, 'test_recall': 0.9812, 'test_f1': 0.9812216061607122}, {'layer': 2, 'units': 512, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9864052351225712, 'test_recall': 0.9864, 'test_f1': 0.9863993839006051}, {'layer': 2, 'units': 512, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9986559445691361, 'train_recall': 0.99865, 'train_f1': 0.9986509106986572, 'test_precision': 0.9818725320455923, 'test_recall': 0.9817, 'test_f1': 0.9817176969676177}, {'layer': 2, 'units': 512, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9864004923447078, 'test_recall': 0.9864, 'test_f1': 0.9863985911563463}, {'layer': 2, 'units': 512, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9985693184346943, 'train_recall': 0.9985666666666667, 'train_f1': 0.9985660966252159, 'test_precision': 0.9807783719177432, 'test_recall': 0.9807, 'test_f1': 0.9807003275455938}, {'layer': 2, 'units': 512, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9853079262876843, 'test_recall': 0.9853, 'test_f1': 0.9852973864293668}, {'layer': 2, 'units': 1024, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9877620929356461, 'train_recall': 0.9875333333333334, 'train_f1': 0.987531454056248, 'test_precision': 0.9751003351228197, 'test_recall': 0.9746, 'test_f1': 0.9746165544561248}, {'layer': 2, 'units': 1024, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9934013312694238, 'train_recall': 0.9933333333333333, 'train_f1': 0.9933425912551604, 'test_precision': 0.9793511507587411, 'test_recall': 0.9791, 'test_f1': 0.9791234825592932}, {'layer': 2, 'units': 1024, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9923851252314625, 'train_recall': 0.9923333333333333, 'train_f1': 0.9923334836120353, 'test_precision': 0.9780013348552548, 'test_recall': 0.9778, 'test_f1': 0.9777899248898099}, {'layer': 2, 'units': 1024, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9927685117881839, 'train_recall': 0.9927333333333334, 'train_f1': 0.9927287194391167, 'test_precision': 0.9784880466756154, 'test_recall': 0.9784, 'test_f1': 0.9783815378588517}, {'layer': 2, 'units': 1024, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9943615749377899, 'train_recall': 0.99435, 'train_f1': 0.9943495561028981, 'test_precision': 0.9813478925742023, 'test_recall': 0.9813, 'test_f1': 0.9813037603913739}, {'layer': 2, 'units': 1024, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9919677839579996, 'train_recall': 0.9918333333333333, 'train_f1': 0.9918470947921146, 'test_precision': 0.9778158080132636, 'test_recall': 0.9775, 'test_f1': 0.9775399468605661}, {'layer': 2, 'units': 1024, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9951556805603768, 'train_recall': 0.9951333333333333, 'train_f1': 0.9951354017746437, 'test_precision': 0.9811418010614571, 'test_recall': 0.981, 'test_f1': 0.981012069542681}, {'layer': 2, 'units': 1024, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.983406195916407, 'train_recall': 0.9817166666666667, 'train_f1': 0.9819420665354168, 'test_precision': 0.9682776268878405, 'test_recall': 0.9654, 'test_f1': 0.9657762720633436}, {'layer': 2, 'units': 1024, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9956362785719967, 'train_recall': 0.9956166666666667, 'train_f1': 0.9956135953084434, 'test_precision': 0.9792235435504877, 'test_recall': 0.9791, 'test_f1': 0.9790765220964754}, {'layer': 2, 'units': 1024, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.998485147366653, 'train_recall': 0.9984833333333333, 'train_f1': 0.9984832146562024, 'test_precision': 0.9815189847428303, 'test_recall': 0.9815, 'test_f1': 0.9814964801993381}, {'layer': 2, 'units': 1024, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9921013098844335, 'train_recall': 0.99185, 'train_f1': 0.9918352812855921, 'test_precision': 0.9769974888919897, 'test_recall': 0.9763, 'test_f1': 0.9762851553835122}, {'layer': 2, 'units': 1024, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9988038451330445, 'train_recall': 0.9988, 'train_f1': 0.9988004062546648, 'test_precision': 0.9828429471586276, 'test_recall': 0.9827, 'test_f1': 0.9827112539947066}, {'layer': 2, 'units': 1024, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9945179586940771, 'train_recall': 0.9944666666666667, 'train_f1': 0.9944578260104199, 'test_precision': 0.9809958552654146, 'test_recall': 0.9809, 'test_f1': 0.9808587076854789}, {'layer': 2, 'units': 1024, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.998505777369287, 'train_recall': 0.9985, 'train_f1': 0.998500818197995, 'test_precision': 0.9830118184571256, 'test_recall': 0.9829, 'test_f1': 0.9829163948055537}, {'layer': 2, 'units': 1024, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9982515377186076, 'train_recall': 0.99825, 'train_f1': 0.9982502152504672, 'test_precision': 0.9842341250316715, 'test_recall': 0.9842, 'test_f1': 0.9842012138086073}, {'layer': 2, 'units': 1024, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.998654572551217, 'train_recall': 0.99865, 'train_f1': 0.9986502393969766, 'test_precision': 0.983953165655569, 'test_recall': 0.9839, 'test_f1': 0.9839032145634881}, {'layer': 2, 'units': 1024, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9983847595941063, 'train_recall': 0.9983833333333333, 'train_f1': 0.9983833072626787, 'test_precision': 0.9834363984412731, 'test_recall': 0.9834, 'test_f1': 0.98339855268655}, {'layer': 2, 'units': 1024, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9861019445649656, 'test_recall': 0.9861, 'test_f1': 0.9860944082624646}, {'layer': 2, 'units': 1024, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9971565283494397, 'train_recall': 0.99715, 'train_f1': 0.9971505602915081, 'test_precision': 0.9806935090174619, 'test_recall': 0.9806, 'test_f1': 0.9806077513591246}, {'layer': 2, 'units': 1024, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9871092653360467, 'test_recall': 0.9871, 'test_f1': 0.98709887882925}, {'layer': 2, 'units': 1024, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9978564289648839, 'train_recall': 0.99785, 'train_f1': 0.9978509421960287, 'test_precision': 0.98176299941136, 'test_recall': 0.9817, 'test_f1': 0.9817139216213995}, {'layer': 2, 'units': 1024, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9860001524189133, 'test_recall': 0.986, 'test_f1': 0.985997909561115}, {'layer': 2, 'units': 1024, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9984533286741312, 'train_recall': 0.99845, 'train_f1': 0.9984501319211901, 'test_precision': 0.9828810423225092, 'test_recall': 0.9828, 'test_f1': 0.9828057520434484}, {'layer': 2, 'units': 1024, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9870082449261932, 'test_recall': 0.987, 'test_f1': 0.9870004904608126}, {'layer': 3, 'units': 64, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9846123587596161, 'train_recall': 0.9845333333333334, 'train_f1': 0.9845313245158348, 'test_precision': 0.9701734222710764, 'test_recall': 0.97, 'test_f1': 0.9699950709396605}, {'layer': 3, 'units': 64, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9796215307389874, 'train_recall': 0.9789833333333333, 'train_f1': 0.9790967878810233, 'test_precision': 0.9681070458217428, 'test_recall': 0.9674, 'test_f1': 0.9675012669343281}, {'layer': 3, 'units': 64, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9861052716888283, 'train_recall': 0.9860666666666666, 'train_f1': 0.9860560189949917, 'test_precision': 0.9727003212145068, 'test_recall': 0.9725, 'test_f1': 0.9724789103601763}, {'layer': 3, 'units': 64, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9851863987746575, 'train_recall': 0.98505, 'train_f1': 0.9850573325930561, 'test_precision': 0.9724396746736022, 'test_recall': 0.9723, 'test_f1': 0.9722823323351251}, {'layer': 3, 'units': 64, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9821968328489593, 'train_recall': 0.9820833333333333, 'train_f1': 0.9820621821023697, 'test_precision': 0.9696490305651443, 'test_recall': 0.9696, 'test_f1': 0.9695594892444689}, {'layer': 3, 'units': 64, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.983724830805587, 'train_recall': 0.98365, 'train_f1': 0.9836501461345639, 'test_precision': 0.9748351957515828, 'test_recall': 0.9748, 'test_f1': 0.974790062915218}, {'layer': 3, 'units': 64, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9758449549896315, 'train_recall': 0.9757333333333333, 'train_f1': 0.9757355903239471, 'test_precision': 0.968857230497941, 'test_recall': 0.9688, 'test_f1': 0.968794742443524}, {'layer': 3, 'units': 64, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9725933436362494, 'train_recall': 0.9719666666666666, 'train_f1': 0.9719791874899785, 'test_precision': 0.9655653503786478, 'test_recall': 0.9647, 'test_f1': 0.9647350235563387}, {'layer': 3, 'units': 64, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9896337300528214, 'train_recall': 0.9895166666666667, 'train_f1': 0.9895129442579814, 'test_precision': 0.9720167105276103, 'test_recall': 0.9718, 'test_f1': 0.971754584035386}, {'layer': 3, 'units': 64, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9916404179374683, 'train_recall': 0.9915833333333334, 'train_f1': 0.9915910666636117, 'test_precision': 0.9767793303526674, 'test_recall': 0.9766, 'test_f1': 0.976615418112073}, {'layer': 3, 'units': 64, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9924708143269215, 'train_recall': 0.9924166666666666, 'train_f1': 0.9924117017012885, 'test_precision': 0.9746138142083223, 'test_recall': 0.9745, 'test_f1': 0.9744715681450634}, {'layer': 3, 'units': 64, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9920900396515482, 'train_recall': 0.9920333333333333, 'train_f1': 0.9920438050844369, 'test_precision': 0.9753328008844697, 'test_recall': 0.9752, 'test_f1': 0.9752211512568679}, {'layer': 3, 'units': 64, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9920820260109171, 'train_recall': 0.9920666666666667, 'train_f1': 0.9920647964248914, 'test_precision': 0.9737652344009472, 'test_recall': 0.9737, 'test_f1': 0.973686686835666}, {'layer': 3, 'units': 64, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9915216294820612, 'train_recall': 0.9914833333333334, 'train_f1': 0.9914852992781895, 'test_precision': 0.9756409674640462, 'test_recall': 0.9755, 'test_f1': 0.9754993495087528}, {'layer': 3, 'units': 64, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9869315628030373, 'train_recall': 0.9868666666666667, 'train_f1': 0.9868541304860157, 'test_precision': 0.9728070352673538, 'test_recall': 0.9727, 'test_f1': 0.972689691489526}, {'layer': 3, 'units': 64, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.986166327179171, 'train_recall': 0.9860666666666666, 'train_f1': 0.986037319648329, 'test_precision': 0.9738376667065293, 'test_recall': 0.9737, 'test_f1': 0.9736769711672423}, {'layer': 3, 'units': 64, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9967763270802311, 'train_recall': 0.9967666666666667, 'train_f1': 0.9967674620866296, 'test_precision': 0.9797780129104962, 'test_recall': 0.9797, 'test_f1': 0.9797168732727021}, {'layer': 3, 'units': 64, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.993824469608022, 'train_recall': 0.9937666666666667, 'train_f1': 0.9937701529842796, 'test_precision': 0.9767841828323395, 'test_recall': 0.9766, 'test_f1': 0.9766123710511265}, {'layer': 3, 'units': 64, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9949063877101935, 'train_recall': 0.9948833333333333, 'train_f1': 0.9948807107072917, 'test_precision': 0.9739373960023924, 'test_recall': 0.9738, 'test_f1': 0.9737996456405692}, {'layer': 3, 'units': 64, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9965296848550586, 'train_recall': 0.9965166666666667, 'train_f1': 0.9965184605367134, 'test_precision': 0.9747648753805952, 'test_recall': 0.9746, 'test_f1': 0.9746141544354597}, {'layer': 3, 'units': 64, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9918743452164916, 'train_recall': 0.9918, 'train_f1': 0.9917856047515297, 'test_precision': 0.9711510975413761, 'test_recall': 0.9708, 'test_f1': 0.9707780841064813}, {'layer': 3, 'units': 64, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9963294560869592, 'train_recall': 0.9962833333333333, 'train_f1': 0.996289821126011, 'test_precision': 0.9747252476484022, 'test_recall': 0.9744, 'test_f1': 0.9744294127190407}, {'layer': 3, 'units': 64, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9968051120378735, 'train_recall': 0.9968, 'train_f1': 0.9967986364472031, 'test_precision': 0.9738720602453809, 'test_recall': 0.9738, 'test_f1': 0.9737974140566091}, {'layer': 3, 'units': 64, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9924241296938098, 'train_recall': 0.99225, 'train_f1': 0.9922599810994166, 'test_precision': 0.9734779794140739, 'test_recall': 0.9729, 'test_f1': 0.9729828923389566}, {'layer': 3, 'units': 128, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9901413949634154, 'train_recall': 0.9901, 'train_f1': 0.9900935235803717, 'test_precision': 0.9754732689716357, 'test_recall': 0.9754, 'test_f1': 0.9753898956922827}, {'layer': 3, 'units': 128, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.984617713241524, 'train_recall': 0.9843833333333334, 'train_f1': 0.9843130348850632, 'test_precision': 0.9728923832977429, 'test_recall': 0.9725, 'test_f1': 0.9724121766778518}, {'layer': 3, 'units': 128, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9896275103340029, 'train_recall': 0.98955, 'train_f1': 0.9895478752513002, 'test_precision': 0.9768152613787315, 'test_recall': 0.9766, 'test_f1': 0.9766021026833686}, {'layer': 3, 'units': 128, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9850378294317836, 'train_recall': 0.9846666666666667, 'train_f1': 0.9847224866255138, 'test_precision': 0.9720512186455893, 'test_recall': 0.9712, 'test_f1': 0.9713344354987549}, {'layer': 3, 'units': 128, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9903044506601176, 'train_recall': 0.9902666666666666, 'train_f1': 0.9902656033929207, 'test_precision': 0.9776690165686519, 'test_recall': 0.9776, 'test_f1': 0.9775971291621485}, {'layer': 3, 'units': 128, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9893878872309458, 'train_recall': 0.98935, 'train_f1': 0.9893390844617435, 'test_precision': 0.9765655967306786, 'test_recall': 0.9764, 'test_f1': 0.976377282226321}, {'layer': 3, 'units': 128, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9833834712031266, 'train_recall': 0.9830166666666666, 'train_f1': 0.983032880220837, 'test_precision': 0.9707562975017965, 'test_recall': 0.9701, 'test_f1': 0.9701326353780408}, {'layer': 3, 'units': 128, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9842637542142465, 'train_recall': 0.9841166666666666, 'train_f1': 0.9840847372566246, 'test_precision': 0.9724414159946536, 'test_recall': 0.972, 'test_f1': 0.9719616275345161}, {'layer': 3, 'units': 128, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9938063377085967, 'train_recall': 0.9937666666666667, 'train_f1': 0.9937639182762446, 'test_precision': 0.98095681725592, 'test_recall': 0.9809, 'test_f1': 0.9809003789121805}, {'layer': 3, 'units': 128, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9938640502689959, 'train_recall': 0.9938333333333333, 'train_f1': 0.9938352823064985, 'test_precision': 0.9774782061499278, 'test_recall': 0.9774, 'test_f1': 0.9773993288651832}, {'layer': 3, 'units': 128, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.99388117017439, 'train_recall': 0.9938666666666667, 'train_f1': 0.9938631468231127, 'test_precision': 0.9768719692237408, 'test_recall': 0.9768, 'test_f1': 0.9767859850485429}, {'layer': 3, 'units': 128, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9928582921608298, 'train_recall': 0.9927333333333334, 'train_f1': 0.9927289120205995, 'test_precision': 0.976656282150752, 'test_recall': 0.9764, 'test_f1': 0.9763788122836546}, {'layer': 3, 'units': 128, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.994154596136558, 'train_recall': 0.9941166666666666, 'train_f1': 0.9941145103297027, 'test_precision': 0.9787313372851119, 'test_recall': 0.9786, 'test_f1': 0.9786075500379615}, {'layer': 3, 'units': 128, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9931739685957469, 'train_recall': 0.9931166666666666, 'train_f1': 0.9931017369381687, 'test_precision': 0.9774275534738559, 'test_recall': 0.9773, 'test_f1': 0.9772640230237466}, {'layer': 3, 'units': 128, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9946388284476431, 'train_recall': 0.9946166666666667, 'train_f1': 0.9946159540579561, 'test_precision': 0.979127341972101, 'test_recall': 0.979, 'test_f1': 0.9789955329766673}, {'layer': 3, 'units': 128, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9938039931144715, 'train_recall': 0.99375, 'train_f1': 0.9937439916929182, 'test_precision': 0.976070020783395, 'test_recall': 0.9758, 'test_f1': 0.9757842699094353}, {'layer': 3, 'units': 128, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9966484125247009, 'train_recall': 0.9966166666666667, 'train_f1': 0.9966187163924459, 'test_precision': 0.9784987362193742, 'test_recall': 0.9783, 'test_f1': 0.9783017950118101}, {'layer': 3, 'units': 128, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9972211863934363, 'train_recall': 0.9972166666666666, 'train_f1': 0.9972165912649507, 'test_precision': 0.9793561743498703, 'test_recall': 0.9793, 'test_f1': 0.9792923145436544}, {'layer': 3, 'units': 128, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9972069598744058, 'train_recall': 0.9972, 'train_f1': 0.9972003390548001, 'test_precision': 0.9792763079852531, 'test_recall': 0.9792, 'test_f1': 0.9792020423790156}, {'layer': 3, 'units': 128, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9966457354546207, 'train_recall': 0.9966166666666667, 'train_f1': 0.9966123165055447, 'test_precision': 0.9766896281701567, 'test_recall': 0.9765, 'test_f1': 0.9765105965657451}, {'layer': 3, 'units': 128, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9985026492076899, 'train_recall': 0.9985, 'train_f1': 0.9985001424231845, 'test_precision': 0.9813634649416841, 'test_recall': 0.9813, 'test_f1': 0.9813094932199224}, {'layer': 3, 'units': 128, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9986511188781151, 'train_recall': 0.99865, 'train_f1': 0.9986501378212522, 'test_precision': 0.9787739074423891, 'test_recall': 0.9787, 'test_f1': 0.9787021999573342}, {'layer': 3, 'units': 128, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9963134906960022, 'train_recall': 0.9963, 'train_f1': 0.9962991300362546, 'test_precision': 0.9762509824433279, 'test_recall': 0.976, 'test_f1': 0.9759986135917151}, {'layer': 3, 'units': 128, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9989344031722814, 'train_recall': 0.9989333333333333, 'train_f1': 0.9989334864507361, 'test_precision': 0.9814327776524037, 'test_recall': 0.9814, 'test_f1': 0.9813973475826264}, {'layer': 3, 'units': 256, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.989656928791026, 'train_recall': 0.9896333333333334, 'train_f1': 0.9896273037715553, 'test_precision': 0.9775488995960623, 'test_recall': 0.9774, 'test_f1': 0.9774000440033965}, {'layer': 3, 'units': 256, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9848015533685838, 'train_recall': 0.9842666666666666, 'train_f1': 0.9843399924741425, 'test_precision': 0.9714153224230527, 'test_recall': 0.9704, 'test_f1': 0.9705484784747672}, {'layer': 3, 'units': 256, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9900047874067763, 'train_recall': 0.9897666666666667, 'train_f1': 0.9897822948942802, 'test_precision': 0.9774523435420902, 'test_recall': 0.9771, 'test_f1': 0.9771182738150886}, {'layer': 3, 'units': 256, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9872018741639729, 'train_recall': 0.9869666666666667, 'train_f1': 0.9869653328433755, 'test_precision': 0.9745452201948845, 'test_recall': 0.9741, 'test_f1': 0.974106359900817}, {'layer': 3, 'units': 256, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.991867675307225, 'train_recall': 0.9918, 'train_f1': 0.9918054244749163, 'test_precision': 0.9769867375408441, 'test_recall': 0.9766, 'test_f1': 0.9766441284126146}, {'layer': 3, 'units': 256, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.992505878817484, 'train_recall': 0.99245, 'train_f1': 0.9924554279101343, 'test_precision': 0.9792795240645014, 'test_recall': 0.9791, 'test_f1': 0.9791136383926973}, {'layer': 3, 'units': 256, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9920883841331453, 'train_recall': 0.99205, 'train_f1': 0.9920525758416876, 'test_precision': 0.980013820219748, 'test_recall': 0.9799, 'test_f1': 0.9799155580335515}, {'layer': 3, 'units': 256, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9920626071200553, 'train_recall': 0.9920333333333333, 'train_f1': 0.9920342992133429, 'test_precision': 0.9794077487123449, 'test_recall': 0.9793, 'test_f1': 0.9793018318343282}, {'layer': 3, 'units': 256, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9935819960819465, 'train_recall': 0.99355, 'train_f1': 0.9935397332307366, 'test_precision': 0.9757578788376439, 'test_recall': 0.9756, 'test_f1': 0.9755992028462495}, {'layer': 3, 'units': 256, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9949703831540428, 'train_recall': 0.99495, 'train_f1': 0.9949509467112246, 'test_precision': 0.9777081957541893, 'test_recall': 0.9776, 'test_f1': 0.9775978526227739}, {'layer': 3, 'units': 256, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9927752690948003, 'train_recall': 0.9927, 'train_f1': 0.9926991279207151, 'test_precision': 0.979689104718744, 'test_recall': 0.9796, 'test_f1': 0.9795839784429067}, {'layer': 3, 'units': 256, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9971695396944843, 'train_recall': 0.9971666666666666, 'train_f1': 0.9971665173676905, 'test_precision': 0.9827269547610121, 'test_recall': 0.9827, 'test_f1': 0.9827023575099701}, {'layer': 3, 'units': 256, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9956831200836098, 'train_recall': 0.9956666666666667, 'train_f1': 0.9956675504430782, 'test_precision': 0.9789877816917153, 'test_recall': 0.9789, 'test_f1': 0.9788942775720205}, {'layer': 3, 'units': 256, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9970286469575543, 'train_recall': 0.9970166666666667, 'train_f1': 0.9970161691588906, 'test_precision': 0.9823932260317697, 'test_recall': 0.9823, 'test_f1': 0.9823055181130772}, {'layer': 3, 'units': 256, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9939401722095791, 'train_recall': 0.9938166666666667, 'train_f1': 0.9938291100918303, 'test_precision': 0.9785654681799093, 'test_recall': 0.9782, 'test_f1': 0.978248349146434}, {'layer': 3, 'units': 256, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9967869520453303, 'train_recall': 0.99675, 'train_f1': 0.9967558298423376, 'test_precision': 0.9807542070147445, 'test_recall': 0.9806, 'test_f1': 0.9806037244683532}, {'layer': 3, 'units': 256, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9936339871423899, 'train_recall': 0.9935833333333334, 'train_f1': 0.993578182209077, 'test_precision': 0.9802053377264046, 'test_recall': 0.98, 'test_f1': 0.9800040514070842}, {'layer': 3, 'units': 256, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9954914179725731, 'train_recall': 0.9954333333333333, 'train_f1': 0.9954365804861602, 'test_precision': 0.9797210782133128, 'test_recall': 0.9795, 'test_f1': 0.9795167234261487}, {'layer': 3, 'units': 256, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.997107821315253, 'train_recall': 0.9970833333333333, 'train_f1': 0.9970850498766213, 'test_precision': 0.9810440928680233, 'test_recall': 0.9809, 'test_f1': 0.9809146078806403}, {'layer': 3, 'units': 256, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9991181877267801, 'train_recall': 0.9991166666666667, 'train_f1': 0.9991167436206118, 'test_precision': 0.981220512954906, 'test_recall': 0.9811, 'test_f1': 0.9811046568905284}, {'layer': 3, 'units': 256, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.998983562301712, 'train_recall': 0.9989833333333333, 'train_f1': 0.9989832754476137, 'test_precision': 0.9827025226681033, 'test_recall': 0.9827, 'test_f1': 0.9826939837248143}, {'layer': 3, 'units': 256, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9988381753913107, 'train_recall': 0.9988333333333334, 'train_f1': 0.9988330056588147, 'test_precision': 0.983087233168787, 'test_recall': 0.983, 'test_f1': 0.9830013269451623}, {'layer': 3, 'units': 256, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9970122175722542, 'train_recall': 0.997, 'train_f1': 0.9970005740750645, 'test_precision': 0.9785584520945314, 'test_recall': 0.9784, 'test_f1': 0.9784063834567641}, {'layer': 3, 'units': 256, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9993013337314057, 'train_recall': 0.9993, 'train_f1': 0.9993001475792828, 'test_precision': 0.9830191405676841, 'test_recall': 0.983, 'test_f1': 0.9829927844182476}, {'layer': 3, 'units': 512, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9914545701976843, 'train_recall': 0.9914166666666666, 'train_f1': 0.9914086761732779, 'test_precision': 0.9794753465601455, 'test_recall': 0.9793, 'test_f1': 0.9792857459047017}, {'layer': 3, 'units': 512, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9898290889217014, 'train_recall': 0.9897333333333334, 'train_f1': 0.9897412208571535, 'test_precision': 0.9777023058678118, 'test_recall': 0.9772, 'test_f1': 0.977251375577823}, {'layer': 3, 'units': 512, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9936483129220639, 'train_recall': 0.9936166666666667, 'train_f1': 0.9936193813127294, 'test_precision': 0.9787793753467966, 'test_recall': 0.9786, 'test_f1': 0.9786100560022194}, {'layer': 3, 'units': 512, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9956384548554643, 'train_recall': 0.9956333333333334, 'train_f1': 0.9956333865547785, 'test_precision': 0.9835467252438642, 'test_recall': 0.9835, 'test_f1': 0.983502557690528}, {'layer': 3, 'units': 512, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9901857214286185, 'train_recall': 0.99005, 'train_f1': 0.9900674577805053, 'test_precision': 0.9741879621940756, 'test_recall': 0.9738, 'test_f1': 0.9738499650851711}, {'layer': 3, 'units': 512, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9937968026973533, 'train_recall': 0.9937666666666667, 'train_f1': 0.9937657088471689, 'test_precision': 0.9811381346819992, 'test_recall': 0.981, 'test_f1': 0.9809989860222138}, {'layer': 3, 'units': 512, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9915271051552595, 'train_recall': 0.9914833333333334, 'train_f1': 0.9914754019811867, 'test_precision': 0.9758376088041605, 'test_recall': 0.9757, 'test_f1': 0.975688083795519}, {'layer': 3, 'units': 512, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9954536991610485, 'train_recall': 0.99545, 'train_f1': 0.9954499879741869, 'test_precision': 0.9824490719859769, 'test_recall': 0.9824, 'test_f1': 0.9823978515714087}, {'layer': 3, 'units': 512, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9933841961849507, 'train_recall': 0.99335, 'train_f1': 0.9933411557473999, 'test_precision': 0.9788380923367229, 'test_recall': 0.9787, 'test_f1': 0.9786783170100626}, {'layer': 3, 'units': 512, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9953052990754789, 'train_recall': 0.9952666666666666, 'train_f1': 0.9952665511367403, 'test_precision': 0.9812160327067623, 'test_recall': 0.9811, 'test_f1': 0.9810966030934954}, {'layer': 3, 'units': 512, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9951331435990856, 'train_recall': 0.9951166666666666, 'train_f1': 0.9951169770486514, 'test_precision': 0.9798108517761129, 'test_recall': 0.9797, 'test_f1': 0.9796940847757336}, {'layer': 3, 'units': 512, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9976893960009802, 'train_recall': 0.9976833333333334, 'train_f1': 0.9976832917849334, 'test_precision': 0.9824783022461974, 'test_recall': 0.9824, 'test_f1': 0.982396646051242}, {'layer': 3, 'units': 512, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9954038564987835, 'train_recall': 0.9953833333333333, 'train_f1': 0.9953826613163305, 'test_precision': 0.9788611132101219, 'test_recall': 0.9787, 'test_f1': 0.9787114967010994}, {'layer': 3, 'units': 512, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9989347646483484, 'train_recall': 0.9989333333333333, 'train_f1': 0.9989334404506233, 'test_precision': 0.9849600900195166, 'test_recall': 0.9849, 'test_f1': 0.984906942486785}, {'layer': 3, 'units': 512, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9980693899544021, 'train_recall': 0.9980666666666667, 'train_f1': 0.9980665261362475, 'test_precision': 0.981823598513251, 'test_recall': 0.9818, 'test_f1': 0.9817979054187393}, {'layer': 3, 'units': 512, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9984870902299554, 'train_recall': 0.9984833333333333, 'train_f1': 0.9984836375602358, 'test_precision': 0.9806579766989701, 'test_recall': 0.9805, 'test_f1': 0.9805185416919047}, {'layer': 3, 'units': 512, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9970021059284034, 'train_recall': 0.997, 'train_f1': 0.9969992988084293, 'test_precision': 0.9812149881695502, 'test_recall': 0.9812, 'test_f1': 0.9811990438364563}, {'layer': 3, 'units': 512, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9983734386655783, 'train_recall': 0.9983666666666666, 'train_f1': 0.9983662623372116, 'test_precision': 0.9809384875987566, 'test_recall': 0.9808, 'test_f1': 0.9807979937930712}, {'layer': 3, 'units': 512, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9972949951990238, 'train_recall': 0.9972833333333333, 'train_f1': 0.9972835359129898, 'test_precision': 0.9817673033451271, 'test_recall': 0.9817, 'test_f1': 0.9817081405596028}, {'layer': 3, 'units': 512, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.999251839404567, 'train_recall': 0.99925, 'train_f1': 0.9992500060687891, 'test_precision': 0.9817948132352214, 'test_recall': 0.9817, 'test_f1': 0.9817082919306779}, {'layer': 3, 'units': 512, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9969455398204131, 'train_recall': 0.9969333333333333, 'train_f1': 0.9969343078582612, 'test_precision': 0.9819665026178929, 'test_recall': 0.9818, 'test_f1': 0.9818042021407669}, {'layer': 3, 'units': 512, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9998667506976595, 'train_recall': 0.9998666666666667, 'train_f1': 0.9998666662775028, 'test_precision': 0.9862125624492961, 'test_recall': 0.9862, 'test_f1': 0.9861995662986194}, {'layer': 3, 'units': 512, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9983690652100287, 'train_recall': 0.9983666666666666, 'train_f1': 0.9983664172640064, 'test_precision': 0.9820691819135371, 'test_recall': 0.982, 'test_f1': 0.9820022462410596}, {'layer': 3, 'units': 512, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.99621841271372, 'train_recall': 0.9961666666666666, 'train_f1': 0.9961567302134449, 'test_precision': 0.9800722545458348, 'test_recall': 0.9798, 'test_f1': 0.9798012144419401}, {'layer': 3, 'units': 1024, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9901373260761146, 'train_recall': 0.9900833333333333, 'train_f1': 0.9900905090700632, 'test_precision': 0.9753510058270578, 'test_recall': 0.9752, 'test_f1': 0.9752043569088419}, {'layer': 3, 'units': 1024, 'epochs': 5, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9888373619861549, 'train_recall': 0.9886833333333334, 'train_f1': 0.9886767201805315, 'test_precision': 0.9770870100247053, 'test_recall': 0.9768, 'test_f1': 0.976773915752169}, {'layer': 3, 'units': 1024, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.990638397322713, 'train_recall': 0.9904666666666667, 'train_f1': 0.9904883484813856, 'test_precision': 0.9766063716339857, 'test_recall': 0.9762, 'test_f1': 0.9762294031375969}, {'layer': 3, 'units': 1024, 'epochs': 5, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.991407499799136, 'train_recall': 0.9913166666666666, 'train_f1': 0.9913208367131884, 'test_precision': 0.9806536482206543, 'test_recall': 0.9805, 'test_f1': 0.9805036992875635}, {'layer': 3, 'units': 1024, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9901481295808876, 'train_recall': 0.9899833333333333, 'train_f1': 0.9900015625389574, 'test_precision': 0.9769775355071295, 'test_recall': 0.9767, 'test_f1': 0.9767175700374723}, {'layer': 3, 'units': 1024, 'epochs': 5, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9937180838975518, 'train_recall': 0.9936833333333334, 'train_f1': 0.9936827871419419, 'test_precision': 0.9820048334894503, 'test_recall': 0.9819, 'test_f1': 0.9818887619041883}, {'layer': 3, 'units': 1024, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.984459928459507, 'train_recall': 0.9840333333333333, 'train_f1': 0.9839666943063472, 'test_precision': 0.9701051055964508, 'test_recall': 0.9693, 'test_f1': 0.969174509112793}, {'layer': 3, 'units': 1024, 'epochs': 5, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9914855015510255, 'train_recall': 0.9911666666666666, 'train_f1': 0.9912067257673942, 'test_precision': 0.9790437149702433, 'test_recall': 0.9784, 'test_f1': 0.9784907796249901}, {'layer': 3, 'units': 1024, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.993808871690445, 'train_recall': 0.9937833333333334, 'train_f1': 0.9937860933395372, 'test_precision': 0.9821563137614127, 'test_recall': 0.9821, 'test_f1': 0.9821011592257463}, {'layer': 3, 'units': 1024, 'epochs': 10, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9972234574230043, 'train_recall': 0.9972166666666666, 'train_f1': 0.9972172425801278, 'test_precision': 0.9846648364021783, 'test_recall': 0.9846, 'test_f1': 0.9846057691364384}, {'layer': 3, 'units': 1024, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9951022607578272, 'train_recall': 0.9950666666666667, 'train_f1': 0.9950619183069742, 'test_precision': 0.981324221221727, 'test_recall': 0.9812, 'test_f1': 0.9811879282254852}, {'layer': 3, 'units': 1024, 'epochs': 10, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9973595816506609, 'train_recall': 0.99735, 'train_f1': 0.9973513089768599, 'test_precision': 0.9841957914089519, 'test_recall': 0.9841, 'test_f1': 0.9841166007196309}, {'layer': 3, 'units': 1024, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9924419677105705, 'train_recall': 0.9924, 'train_f1': 0.9923962746000514, 'test_precision': 0.9796547513979318, 'test_recall': 0.9796, 'test_f1': 0.9795771234725827}, {'layer': 3, 'units': 1024, 'epochs': 10, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9954117246763459, 'train_recall': 0.9953833333333333, 'train_f1': 0.9953816180613934, 'test_precision': 0.9821115798418628, 'test_recall': 0.982, 'test_f1': 0.9819773680882947}, {'layer': 3, 'units': 1024, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9946134603790941, 'train_recall': 0.9945333333333334, 'train_f1': 0.994543492053999, 'test_precision': 0.9797239808611191, 'test_recall': 0.9795, 'test_f1': 0.979510447375178}, {'layer': 3, 'units': 1024, 'epochs': 10, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9982841885073377, 'train_recall': 0.9982833333333333, 'train_f1': 0.9982831947401285, 'test_precision': 0.9824272552714519, 'test_recall': 0.9824, 'test_f1': 0.9824020618710989}, {'layer': 3, 'units': 1024, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9981172971890588, 'train_recall': 0.9981166666666667, 'train_f1': 0.9981166603198034, 'test_precision': 0.9818176802873025, 'test_recall': 0.9818, 'test_f1': 0.9817901882813626}, {'layer': 3, 'units': 1024, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9994668577653965, 'train_recall': 0.9994666666666666, 'train_f1': 0.9994666322648121, 'test_precision': 0.9843059906968238, 'test_recall': 0.9843, 'test_f1': 0.9842881989969817}, {'layer': 3, 'units': 1024, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9979721425816677, 'train_recall': 0.9979666666666667, 'train_f1': 0.997967002807371, 'test_precision': 0.9820934213623616, 'test_recall': 0.982, 'test_f1': 0.9819990898050733}, {'layer': 3, 'units': 1024, 'epochs': 20, 'batch_size': 64, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9991508104769296, 'train_recall': 0.99915, 'train_f1': 0.9991499250521163, 'test_precision': 0.984047801203774, 'test_recall': 0.984, 'test_f1': 0.983994061654056}, {'layer': 3, 'units': 1024, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9974623084614895, 'train_recall': 0.99745, 'train_f1': 0.9974508640001969, 'test_precision': 0.9826243989309511, 'test_recall': 0.9825, 'test_f1': 0.9825084314656464}, {'layer': 3, 'units': 1024, 'epochs': 20, 'batch_size': 128, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9998500351838651, 'train_recall': 0.99985, 'train_f1': 0.9998499997273472, 'test_precision': 0.9850180659429064, 'test_recall': 0.985, 'test_f1': 0.984997170408668}, {'layer': 3, 'units': 1024, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9977231754033545, 'train_recall': 0.9977166666666667, 'train_f1': 0.9977167874374867, 'test_precision': 0.9821275252182784, 'test_recall': 0.982, 'test_f1': 0.9820112219514121}, {'layer': 3, 'units': 1024, 'epochs': 20, 'batch_size': 256, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9995334029150944, 'train_recall': 0.9995333333333334, 'train_f1': 0.9995332332019573, 'test_precision': 0.9839210969351919, 'test_recall': 0.9839, 'test_f1': 0.9838955038714006}]\n"
     ]
    }
   ],
   "source": [
    "print(all_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "732cf8c1-6b36-4810-b4bd-1f785b5b1880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.981986</td>\n",
       "      <td>0.981817</td>\n",
       "      <td>0.981786</td>\n",
       "      <td>0.969419</td>\n",
       "      <td>0.9691</td>\n",
       "      <td>0.969042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.981489</td>\n",
       "      <td>0.981417</td>\n",
       "      <td>0.981413</td>\n",
       "      <td>0.973861</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.973787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.980283</td>\n",
       "      <td>0.980233</td>\n",
       "      <td>0.980216</td>\n",
       "      <td>0.971597</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.971468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.978067</td>\n",
       "      <td>0.977900</td>\n",
       "      <td>0.977896</td>\n",
       "      <td>0.968967</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.968812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.975075</td>\n",
       "      <td>0.975050</td>\n",
       "      <td>0.975023</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.966768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.999150</td>\n",
       "      <td>0.999150</td>\n",
       "      <td>0.984048</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.983994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.997462</td>\n",
       "      <td>0.997450</td>\n",
       "      <td>0.997451</td>\n",
       "      <td>0.982624</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.982508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.985018</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.984997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.997723</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.982128</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.982011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>relu</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.999533</td>\n",
       "      <td>0.999533</td>\n",
       "      <td>0.999533</td>\n",
       "      <td>0.983921</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.983896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer  units  epochs  batch_size activation optimizer  train_precision   \n",
       "0        1     64       5          32       relu      adam         0.981986  \\\n",
       "1        1     64       5          32       relu   rmsprop         0.981489   \n",
       "2        1     64       5          64       relu      adam         0.980283   \n",
       "3        1     64       5          64       relu   rmsprop         0.978067   \n",
       "4        1     64       5         128       relu      adam         0.975075   \n",
       "..     ...    ...     ...         ...        ...       ...              ...   \n",
       "355      3   1024      20          64       relu   rmsprop         0.999151   \n",
       "356      3   1024      20         128       relu      adam         0.997462   \n",
       "357      3   1024      20         128       relu   rmsprop         0.999850   \n",
       "358      3   1024      20         256       relu      adam         0.997723   \n",
       "359      3   1024      20         256       relu   rmsprop         0.999533   \n",
       "\n",
       "     train_recall  train_f1  test_precision  test_recall   test_f1  \n",
       "0        0.981817  0.981786        0.969419       0.9691  0.969042  \n",
       "1        0.981417  0.981413        0.973861       0.9738  0.973787  \n",
       "2        0.980233  0.980216        0.971597       0.9715  0.971468  \n",
       "3        0.977900  0.977896        0.968967       0.9688  0.968812  \n",
       "4        0.975050  0.975023        0.966797       0.9668  0.966768  \n",
       "..            ...       ...             ...          ...       ...  \n",
       "355      0.999150  0.999150        0.984048       0.9840  0.983994  \n",
       "356      0.997450  0.997451        0.982624       0.9825  0.982508  \n",
       "357      0.999850  0.999850        0.985018       0.9850  0.984997  \n",
       "358      0.997717  0.997717        0.982128       0.9820  0.982011  \n",
       "359      0.999533  0.999533        0.983921       0.9839  0.983896  \n",
       "\n",
       "[360 rows x 12 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "record_data_forConvert = pd.DataFrame(all_record)\n",
    "record_df = pd.DataFrame.from_records(record_data_forConvert)\n",
    "record_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97a87f15-967f-4a6f-9874-351f29e8ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_df.to_csv('./lab1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8db70595-1e22-42f7-bb0f-ef750af00cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.981986</td>\n",
       "      <td>0.981817</td>\n",
       "      <td>0.981786</td>\n",
       "      <td>0.969419</td>\n",
       "      <td>0.9691</td>\n",
       "      <td>0.969042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.980283</td>\n",
       "      <td>0.980233</td>\n",
       "      <td>0.980216</td>\n",
       "      <td>0.971597</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.971468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.975075</td>\n",
       "      <td>0.975050</td>\n",
       "      <td>0.975023</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.966768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>0.966567</td>\n",
       "      <td>0.966554</td>\n",
       "      <td>0.960551</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.960364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.975071</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>0.974990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.994613</td>\n",
       "      <td>0.994533</td>\n",
       "      <td>0.994543</td>\n",
       "      <td>0.979724</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>0.979510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.998117</td>\n",
       "      <td>0.998117</td>\n",
       "      <td>0.998117</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.981790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.997972</td>\n",
       "      <td>0.997967</td>\n",
       "      <td>0.997967</td>\n",
       "      <td>0.982093</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.981999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.997462</td>\n",
       "      <td>0.997450</td>\n",
       "      <td>0.997451</td>\n",
       "      <td>0.982624</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.982508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.997723</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.982128</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.982011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer  units  epochs  batch_size activation optimizer  train_precision   \n",
       "0        1     64       5          32       relu      adam         0.981986  \\\n",
       "2        1     64       5          64       relu      adam         0.980283   \n",
       "4        1     64       5         128       relu      adam         0.975075   \n",
       "6        1     64       5         256       relu      adam         0.966627   \n",
       "8        1     64      10          32       relu      adam         0.993750   \n",
       "..     ...    ...     ...         ...        ...       ...              ...   \n",
       "350      3   1024      10         256       relu      adam         0.994613   \n",
       "352      3   1024      20          32       relu      adam         0.998117   \n",
       "354      3   1024      20          64       relu      adam         0.997972   \n",
       "356      3   1024      20         128       relu      adam         0.997462   \n",
       "358      3   1024      20         256       relu      adam         0.997723   \n",
       "\n",
       "     train_recall  train_f1  test_precision  test_recall   test_f1  \n",
       "0        0.981817  0.981786        0.969419       0.9691  0.969042  \n",
       "2        0.980233  0.980216        0.971597       0.9715  0.971468  \n",
       "4        0.975050  0.975023        0.966797       0.9668  0.966768  \n",
       "6        0.966567  0.966554        0.960551       0.9604  0.960364  \n",
       "8        0.993733  0.993733        0.975071       0.9750  0.974990  \n",
       "..            ...       ...             ...          ...       ...  \n",
       "350      0.994533  0.994543        0.979724       0.9795  0.979510  \n",
       "352      0.998117  0.998117        0.981818       0.9818  0.981790  \n",
       "354      0.997967  0.997967        0.982093       0.9820  0.981999  \n",
       "356      0.997450  0.997451        0.982624       0.9825  0.982508  \n",
       "358      0.997717  0.997717        0.982128       0.9820  0.982011  \n",
       "\n",
       "[180 rows x 12 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = record_df['optimizer'] == 'rmsprop'\n",
    "\n",
    "new_record_df = record_df[~mask]\n",
    "\n",
    "new_record_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "512204e6-0781-43a1-83f6-8ef5a5598fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('group_layer_1',\n",
       "       layer  units  epochs  batch_size activation optimizer  train_precision   \n",
       "  0        1     64       5          32       relu      adam         0.981986  \\\n",
       "  2        1     64       5          64       relu      adam         0.980283   \n",
       "  4        1     64       5         128       relu      adam         0.975075   \n",
       "  6        1     64       5         256       relu      adam         0.966627   \n",
       "  8        1     64      10          32       relu      adam         0.993750   \n",
       "  10       1     64      10          64       relu      adam         0.990237   \n",
       "  12       1     64      10         128       relu      adam         0.987397   \n",
       "  14       1     64      10         256       relu      adam         0.980676   \n",
       "  16       1     64      20          32       relu      adam         0.998041   \n",
       "  18       1     64      20          64       relu      adam         0.997905   \n",
       "  20       1     64      20         128       relu      adam         0.996405   \n",
       "  22       1     64      20         256       relu      adam         0.992240   \n",
       "  24       1    128       5          32       relu      adam         0.990378   \n",
       "  26       1    128       5          64       relu      adam         0.989460   \n",
       "  28       1    128       5         128       relu      adam         0.984111   \n",
       "  30       1    128       5         256       relu      adam         0.975667   \n",
       "  32       1    128      10          32       relu      adam         0.996569   \n",
       "  34       1    128      10          64       relu      adam         0.993729   \n",
       "  36       1    128      10         128       relu      adam         0.994923   \n",
       "  38       1    128      10         256       relu      adam         0.990632   \n",
       "  40       1    128      20          32       relu      adam         0.998685   \n",
       "  42       1    128      20          64       relu      adam         0.998951   \n",
       "  44       1    128      20         128       relu      adam         0.998968   \n",
       "  46       1    128      20         256       relu      adam         0.997743   \n",
       "  48       1    256       5          32       relu      adam         0.991063   \n",
       "  50       1    256       5          64       relu      adam         0.992190   \n",
       "  52       1    256       5         128       relu      adam         0.988694   \n",
       "  54       1    256       5         256       relu      adam         0.984728   \n",
       "  56       1    256      10          32       relu      adam         0.996774   \n",
       "  58       1    256      10          64       relu      adam         0.997328   \n",
       "  60       1    256      10         128       relu      adam         0.997774   \n",
       "  62       1    256      10         256       relu      adam         0.994865   \n",
       "  64       1    256      20          32       relu      adam         0.998223   \n",
       "  66       1    256      20          64       relu      adam         0.999451   \n",
       "  68       1    256      20         128       relu      adam         0.999950   \n",
       "  70       1    256      20         256       relu      adam         0.999917   \n",
       "  72       1    512       5          32       relu      adam         0.993523   \n",
       "  74       1    512       5          64       relu      adam         0.995232   \n",
       "  76       1    512       5         128       relu      adam         0.993976   \n",
       "  78       1    512       5         256       relu      adam         0.989379   \n",
       "  80       1    512      10          32       relu      adam         0.996723   \n",
       "  82       1    512      10          64       relu      adam         0.997383   \n",
       "  84       1    512      10         128       relu      adam         0.998386   \n",
       "  86       1    512      10         256       relu      adam         0.998437   \n",
       "  88       1    512      20          32       relu      adam         0.998551   \n",
       "  90       1    512      20          64       relu      adam         0.996949   \n",
       "  92       1    512      20         128       relu      adam         0.999883   \n",
       "  94       1    512      20         256       relu      adam         1.000000   \n",
       "  96       1   1024       5          32       relu      adam         0.992828   \n",
       "  98       1   1024       5          64       relu      adam         0.994313   \n",
       "  100      1   1024       5         128       relu      adam         0.995785   \n",
       "  102      1   1024       5         256       relu      adam         0.993394   \n",
       "  104      1   1024      10          32       relu      adam         0.996681   \n",
       "  106      1   1024      10          64       relu      adam         0.997705   \n",
       "  108      1   1024      10         128       relu      adam         0.998918   \n",
       "  110      1   1024      10         256       relu      adam         0.999251   \n",
       "  112      1   1024      20          32       relu      adam         0.997821   \n",
       "  114      1   1024      20          64       relu      adam         0.997439   \n",
       "  116      1   1024      20         128       relu      adam         1.000000   \n",
       "  118      1   1024      20         256       relu      adam         1.000000   \n",
       "  \n",
       "       train_recall  train_f1  test_precision  test_recall   test_f1  \n",
       "  0        0.981817  0.981786        0.969419       0.9691  0.969042  \n",
       "  2        0.980233  0.980216        0.971597       0.9715  0.971468  \n",
       "  4        0.975050  0.975023        0.966797       0.9668  0.966768  \n",
       "  6        0.966567  0.966554        0.960551       0.9604  0.960364  \n",
       "  8        0.993733  0.993733        0.975071       0.9750  0.974990  \n",
       "  10       0.990183  0.990175        0.973276       0.9732  0.973191  \n",
       "  12       0.987367  0.987373        0.976552       0.9765  0.976502  \n",
       "  14       0.980600  0.980606        0.968335       0.9683  0.968303  \n",
       "  16       0.998033  0.998034        0.974205       0.9741  0.974099  \n",
       "  18       0.997900  0.997900        0.975441       0.9753  0.975319  \n",
       "  20       0.996400  0.996400        0.973931       0.9739  0.973900  \n",
       "  22       0.992233  0.992234        0.975222       0.9752  0.975199  \n",
       "  24       0.990333  0.990333        0.976579       0.9765  0.976502  \n",
       "  26       0.989433  0.989432        0.977221       0.9771  0.977087  \n",
       "  28       0.984083  0.984080        0.974304       0.9743  0.974288  \n",
       "  30       0.975567  0.975530        0.968972       0.9688  0.968748  \n",
       "  32       0.996550  0.996550        0.979043       0.9789  0.978903  \n",
       "  34       0.993617  0.993631        0.976229       0.9759  0.975939  \n",
       "  36       0.994917  0.994916        0.978928       0.9789  0.978896  \n",
       "  38       0.990617  0.990617        0.976850       0.9768  0.976790  \n",
       "  40       0.998683  0.998683        0.978574       0.9785  0.978500  \n",
       "  42       0.998950  0.998950        0.977681       0.9776  0.977599  \n",
       "  44       0.998967  0.998966        0.979568       0.9795  0.979491  \n",
       "  46       0.997733  0.997733        0.977402       0.9773  0.977295  \n",
       "  48       0.990900  0.990917        0.975801       0.9753  0.975342  \n",
       "  50       0.992150  0.992154        0.978619       0.9785  0.978508  \n",
       "  52       0.988617  0.988611        0.977182       0.9771  0.977077  \n",
       "  54       0.984700  0.984693        0.974846       0.9748  0.974780  \n",
       "  56       0.996767  0.996766        0.978993       0.9789  0.978908  \n",
       "  58       0.997317  0.997317        0.979507       0.9794  0.979402  \n",
       "  60       0.997767  0.997767        0.981836       0.9818  0.981805  \n",
       "  62       0.994833  0.994837        0.977946       0.9778  0.977805  \n",
       "  64       0.998217  0.998216        0.980162       0.9801  0.980098  \n",
       "  66       0.999450  0.999450        0.981630       0.9816  0.981598  \n",
       "  68       0.999950  0.999950        0.981698       0.9817  0.981697  \n",
       "  70       0.999917  0.999917        0.982324       0.9823  0.982302  \n",
       "  72       0.993433  0.993443        0.979454       0.9792  0.979239  \n",
       "  74       0.995217  0.995215        0.981063       0.9810  0.980998  \n",
       "  76       0.993967  0.993967        0.980265       0.9802  0.980203  \n",
       "  78       0.989333  0.989332        0.977450       0.9774  0.977398  \n",
       "  80       0.996717  0.996717        0.979352       0.9793  0.979302  \n",
       "  82       0.997367  0.997368        0.980527       0.9803  0.980319  \n",
       "  84       0.998383  0.998383        0.981538       0.9815  0.981493  \n",
       "  86       0.998433  0.998434        0.981266       0.9812  0.981209  \n",
       "  88       0.998550  0.998550        0.982486       0.9824  0.982412  \n",
       "  90       0.996933  0.996934        0.979746       0.9796  0.979600  \n",
       "  92       0.999883  0.999883        0.983372       0.9833  0.983311  \n",
       "  94       1.000000  1.000000        0.981919       0.9819  0.981896  \n",
       "  96       0.992733  0.992739        0.976160       0.9757  0.975718  \n",
       "  98       0.994267  0.994272        0.980856       0.9807  0.980716  \n",
       "  100      0.995767  0.995770        0.982306       0.9822  0.982212  \n",
       "  102      0.993383  0.993375        0.979143       0.9791  0.979075  \n",
       "  104      0.996667  0.996664        0.981600       0.9815  0.981494  \n",
       "  106      0.997700  0.997699        0.980113       0.9800  0.980007  \n",
       "  108      0.998917  0.998917        0.981551       0.9815  0.981499  \n",
       "  110      0.999250  0.999250        0.983741       0.9837  0.983702  \n",
       "  112      0.997817  0.997816        0.980060       0.9800  0.979995  \n",
       "  114      0.997433  0.997433        0.980737       0.9807  0.980687  \n",
       "  116      1.000000  1.000000        0.986311       0.9863  0.986302  \n",
       "  118      1.000000  1.000000        0.984105       0.9841  0.984100  ),\n",
       " ('group_layer_2',\n",
       "       layer  units  epochs  batch_size activation optimizer  train_precision   \n",
       "  120      2     64       5          32       relu      adam         0.985351  \\\n",
       "  122      2     64       5          64       relu      adam         0.983590   \n",
       "  124      2     64       5         128       relu      adam         0.980005   \n",
       "  126      2     64       5         256       relu      adam         0.976707   \n",
       "  128      2     64      10          32       relu      adam         0.992353   \n",
       "  130      2     64      10          64       relu      adam         0.990401   \n",
       "  132      2     64      10         128       relu      adam         0.990678   \n",
       "  134      2     64      10         256       relu      adam         0.984725   \n",
       "  136      2     64      20          32       relu      adam         0.998154   \n",
       "  138      2     64      20          64       relu      adam         0.994705   \n",
       "  140      2     64      20         128       relu      adam         0.997138   \n",
       "  142      2     64      20         256       relu      adam         0.996369   \n",
       "  144      2    128       5          32       relu      adam         0.990841   \n",
       "  146      2    128       5          64       relu      adam         0.989795   \n",
       "  148      2    128       5         128       relu      adam         0.988885   \n",
       "  150      2    128       5         256       relu      adam         0.986369   \n",
       "  152      2    128      10          32       relu      adam         0.995414   \n",
       "  154      2    128      10          64       relu      adam         0.993743   \n",
       "  156      2    128      10         128       relu      adam         0.996132   \n",
       "  158      2    128      10         256       relu      adam         0.995115   \n",
       "  160      2    128      20          32       relu      adam         0.996348   \n",
       "  162      2    128      20          64       relu      adam         0.994861   \n",
       "  164      2    128      20         128       relu      adam         0.998353   \n",
       "  166      2    128      20         256       relu      adam         0.998404   \n",
       "  168      2    256       5          32       relu      adam         0.992655   \n",
       "  170      2    256       5          64       relu      adam         0.991948   \n",
       "  172      2    256       5         128       relu      adam         0.993964   \n",
       "  174      2    256       5         256       relu      adam         0.991495   \n",
       "  176      2    256      10          32       relu      adam         0.995340   \n",
       "  178      2    256      10          64       relu      adam         0.996185   \n",
       "  180      2    256      10         128       relu      adam         0.993953   \n",
       "  182      2    256      10         256       relu      adam         0.999050   \n",
       "  184      2    256      20          32       relu      adam         0.997443   \n",
       "  186      2    256      20          64       relu      adam         0.997506   \n",
       "  188      2    256      20         128       relu      adam         0.998718   \n",
       "  190      2    256      20         256       relu      adam         0.997870   \n",
       "  192      2    512       5          32       relu      adam         0.990970   \n",
       "  194      2    512       5          64       relu      adam         0.995071   \n",
       "  196      2    512       5         128       relu      adam         0.993721   \n",
       "  198      2    512       5         256       relu      adam         0.992065   \n",
       "  200      2    512      10          32       relu      adam         0.991353   \n",
       "  202      2    512      10          64       relu      adam         0.996045   \n",
       "  204      2    512      10         128       relu      adam         0.997969   \n",
       "  206      2    512      10         256       relu      adam         0.997230   \n",
       "  208      2    512      20          32       relu      adam         0.997720   \n",
       "  210      2    512      20          64       relu      adam         0.997574   \n",
       "  212      2    512      20         128       relu      adam         0.998656   \n",
       "  214      2    512      20         256       relu      adam         0.998569   \n",
       "  216      2   1024       5          32       relu      adam         0.987762   \n",
       "  218      2   1024       5          64       relu      adam         0.992385   \n",
       "  220      2   1024       5         128       relu      adam         0.994362   \n",
       "  222      2   1024       5         256       relu      adam         0.995156   \n",
       "  224      2   1024      10          32       relu      adam         0.995636   \n",
       "  226      2   1024      10          64       relu      adam         0.992101   \n",
       "  228      2   1024      10         128       relu      adam         0.994518   \n",
       "  230      2   1024      10         256       relu      adam         0.998252   \n",
       "  232      2   1024      20          32       relu      adam         0.998385   \n",
       "  234      2   1024      20          64       relu      adam         0.997157   \n",
       "  236      2   1024      20         128       relu      adam         0.997856   \n",
       "  238      2   1024      20         256       relu      adam         0.998453   \n",
       "  \n",
       "       train_recall  train_f1  test_precision  test_recall   test_f1  \n",
       "  120      0.985283  0.985291        0.975260       0.9752  0.975196  \n",
       "  122      0.983433  0.983439        0.972585       0.9724  0.972431  \n",
       "  124      0.979900  0.979900        0.970086       0.9697  0.969706  \n",
       "  126      0.976650  0.976657        0.966189       0.9661  0.966089  \n",
       "  128      0.992300  0.992297        0.973942       0.9738  0.973806  \n",
       "  130      0.990333  0.990327        0.973921       0.9738  0.973805  \n",
       "  132      0.990633  0.990638        0.973809       0.9737  0.973693  \n",
       "  134      0.984583  0.984576        0.969357       0.9690  0.968984  \n",
       "  136      0.998150  0.998150        0.974036       0.9740  0.973997  \n",
       "  138      0.994650  0.994649        0.973303       0.9730  0.972991  \n",
       "  140      0.997117  0.997119        0.975302       0.9752  0.975199  \n",
       "  142      0.996350  0.996347        0.976201       0.9761  0.976109  \n",
       "  144      0.990800  0.990804        0.975956       0.9758  0.975794  \n",
       "  146      0.989717  0.989730        0.976381       0.9762  0.976226  \n",
       "  148      0.988833  0.988830        0.973042       0.9728  0.972775  \n",
       "  150      0.986333  0.986333        0.972981       0.9729  0.972906  \n",
       "  152      0.995400  0.995399        0.977025       0.9769  0.976896  \n",
       "  154      0.993667  0.993676        0.972617       0.9722  0.972244  \n",
       "  156      0.996117  0.996116        0.978732       0.9786  0.978597  \n",
       "  158      0.995100  0.995101        0.977797       0.9777  0.977702  \n",
       "  160      0.996333  0.996333        0.980542       0.9804  0.980401  \n",
       "  162      0.994800  0.994800        0.977861       0.9777  0.977707  \n",
       "  164      0.998350  0.998350        0.978549       0.9785  0.978499  \n",
       "  166      0.998400  0.998400        0.978420       0.9783  0.978296  \n",
       "  168      0.992600  0.992604        0.978162       0.9779  0.977923  \n",
       "  170      0.991867  0.991867        0.978545       0.9784  0.978403  \n",
       "  172      0.993933  0.993937        0.977913       0.9778  0.977807  \n",
       "  174      0.991483  0.991482        0.979023       0.9790  0.978991  \n",
       "  176      0.995333  0.995332        0.979714       0.9797  0.979695  \n",
       "  178      0.996167  0.996166        0.979547       0.9794  0.979399  \n",
       "  180      0.993883  0.993888        0.975599       0.9753  0.975309  \n",
       "  182      0.999050  0.999050        0.981798       0.9818  0.981793  \n",
       "  184      0.997433  0.997434        0.980738       0.9806  0.980617  \n",
       "  186      0.997500  0.997500        0.980153       0.9801  0.980101  \n",
       "  188      0.998717  0.998717        0.981659       0.9816  0.981602  \n",
       "  190      0.997867  0.997867        0.979030       0.9790  0.978995  \n",
       "  192      0.990917  0.990926        0.977477       0.9772  0.977255  \n",
       "  194      0.995067  0.995065        0.981277       0.9812  0.981196  \n",
       "  196      0.993700  0.993696        0.980363       0.9803  0.980279  \n",
       "  198      0.992000  0.991993        0.978664       0.9785  0.978492  \n",
       "  200      0.991150  0.991162        0.974064       0.9737  0.973692  \n",
       "  202      0.996033  0.996034        0.978692       0.9786  0.978602  \n",
       "  204      0.997967  0.997967        0.983139       0.9831  0.983106  \n",
       "  206      0.997217  0.997217        0.978574       0.9784  0.978401  \n",
       "  208      0.997717  0.997717        0.981084       0.9810  0.981006  \n",
       "  210      0.997567  0.997567        0.981326       0.9812  0.981222  \n",
       "  212      0.998650  0.998651        0.981873       0.9817  0.981718  \n",
       "  214      0.998567  0.998566        0.980778       0.9807  0.980700  \n",
       "  216      0.987533  0.987531        0.975100       0.9746  0.974617  \n",
       "  218      0.992333  0.992333        0.978001       0.9778  0.977790  \n",
       "  220      0.994350  0.994350        0.981348       0.9813  0.981304  \n",
       "  222      0.995133  0.995135        0.981142       0.9810  0.981012  \n",
       "  224      0.995617  0.995614        0.979224       0.9791  0.979077  \n",
       "  226      0.991850  0.991835        0.976997       0.9763  0.976285  \n",
       "  228      0.994467  0.994458        0.980996       0.9809  0.980859  \n",
       "  230      0.998250  0.998250        0.984234       0.9842  0.984201  \n",
       "  232      0.998383  0.998383        0.983436       0.9834  0.983399  \n",
       "  234      0.997150  0.997151        0.980694       0.9806  0.980608  \n",
       "  236      0.997850  0.997851        0.981763       0.9817  0.981714  \n",
       "  238      0.998450  0.998450        0.982881       0.9828  0.982806  ),\n",
       " ('group_layer_3',\n",
       "       layer  units  epochs  batch_size activation optimizer  train_precision   \n",
       "  240      3     64       5          32       relu      adam         0.984612  \\\n",
       "  242      3     64       5          64       relu      adam         0.986105   \n",
       "  244      3     64       5         128       relu      adam         0.982197   \n",
       "  246      3     64       5         256       relu      adam         0.975845   \n",
       "  248      3     64      10          32       relu      adam         0.989634   \n",
       "  250      3     64      10          64       relu      adam         0.992471   \n",
       "  252      3     64      10         128       relu      adam         0.992082   \n",
       "  254      3     64      10         256       relu      adam         0.986932   \n",
       "  256      3     64      20          32       relu      adam         0.996776   \n",
       "  258      3     64      20          64       relu      adam         0.994906   \n",
       "  260      3     64      20         128       relu      adam         0.991874   \n",
       "  262      3     64      20         256       relu      adam         0.996805   \n",
       "  264      3    128       5          32       relu      adam         0.990141   \n",
       "  266      3    128       5          64       relu      adam         0.989628   \n",
       "  268      3    128       5         128       relu      adam         0.990304   \n",
       "  270      3    128       5         256       relu      adam         0.983383   \n",
       "  272      3    128      10          32       relu      adam         0.993806   \n",
       "  274      3    128      10          64       relu      adam         0.993881   \n",
       "  276      3    128      10         128       relu      adam         0.994155   \n",
       "  278      3    128      10         256       relu      adam         0.994639   \n",
       "  280      3    128      20          32       relu      adam         0.996648   \n",
       "  282      3    128      20          64       relu      adam         0.997207   \n",
       "  284      3    128      20         128       relu      adam         0.998503   \n",
       "  286      3    128      20         256       relu      adam         0.996313   \n",
       "  288      3    256       5          32       relu      adam         0.989657   \n",
       "  290      3    256       5          64       relu      adam         0.990005   \n",
       "  292      3    256       5         128       relu      adam         0.991868   \n",
       "  294      3    256       5         256       relu      adam         0.992088   \n",
       "  296      3    256      10          32       relu      adam         0.993582   \n",
       "  298      3    256      10          64       relu      adam         0.992775   \n",
       "  300      3    256      10         128       relu      adam         0.995683   \n",
       "  302      3    256      10         256       relu      adam         0.993940   \n",
       "  304      3    256      20          32       relu      adam         0.993634   \n",
       "  306      3    256      20          64       relu      adam         0.997108   \n",
       "  308      3    256      20         128       relu      adam         0.998984   \n",
       "  310      3    256      20         256       relu      adam         0.997012   \n",
       "  312      3    512       5          32       relu      adam         0.991455   \n",
       "  314      3    512       5          64       relu      adam         0.993648   \n",
       "  316      3    512       5         128       relu      adam         0.990186   \n",
       "  318      3    512       5         256       relu      adam         0.991527   \n",
       "  320      3    512      10          32       relu      adam         0.993384   \n",
       "  322      3    512      10          64       relu      adam         0.995133   \n",
       "  324      3    512      10         128       relu      adam         0.995404   \n",
       "  326      3    512      10         256       relu      adam         0.998069   \n",
       "  328      3    512      20          32       relu      adam         0.997002   \n",
       "  330      3    512      20          64       relu      adam         0.997295   \n",
       "  332      3    512      20         128       relu      adam         0.996946   \n",
       "  334      3    512      20         256       relu      adam         0.998369   \n",
       "  336      3   1024       5          32       relu      adam         0.990137   \n",
       "  338      3   1024       5          64       relu      adam         0.990638   \n",
       "  340      3   1024       5         128       relu      adam         0.990148   \n",
       "  342      3   1024       5         256       relu      adam         0.984460   \n",
       "  344      3   1024      10          32       relu      adam         0.993809   \n",
       "  346      3   1024      10          64       relu      adam         0.995102   \n",
       "  348      3   1024      10         128       relu      adam         0.992442   \n",
       "  350      3   1024      10         256       relu      adam         0.994613   \n",
       "  352      3   1024      20          32       relu      adam         0.998117   \n",
       "  354      3   1024      20          64       relu      adam         0.997972   \n",
       "  356      3   1024      20         128       relu      adam         0.997462   \n",
       "  358      3   1024      20         256       relu      adam         0.997723   \n",
       "  \n",
       "       train_recall  train_f1  test_precision  test_recall   test_f1  \n",
       "  240      0.984533  0.984531        0.970173       0.9700  0.969995  \n",
       "  242      0.986067  0.986056        0.972700       0.9725  0.972479  \n",
       "  244      0.982083  0.982062        0.969649       0.9696  0.969559  \n",
       "  246      0.975733  0.975736        0.968857       0.9688  0.968795  \n",
       "  248      0.989517  0.989513        0.972017       0.9718  0.971755  \n",
       "  250      0.992417  0.992412        0.974614       0.9745  0.974472  \n",
       "  252      0.992067  0.992065        0.973765       0.9737  0.973687  \n",
       "  254      0.986867  0.986854        0.972807       0.9727  0.972690  \n",
       "  256      0.996767  0.996767        0.979778       0.9797  0.979717  \n",
       "  258      0.994883  0.994881        0.973937       0.9738  0.973800  \n",
       "  260      0.991800  0.991786        0.971151       0.9708  0.970778  \n",
       "  262      0.996800  0.996799        0.973872       0.9738  0.973797  \n",
       "  264      0.990100  0.990094        0.975473       0.9754  0.975390  \n",
       "  266      0.989550  0.989548        0.976815       0.9766  0.976602  \n",
       "  268      0.990267  0.990266        0.977669       0.9776  0.977597  \n",
       "  270      0.983017  0.983033        0.970756       0.9701  0.970133  \n",
       "  272      0.993767  0.993764        0.980957       0.9809  0.980900  \n",
       "  274      0.993867  0.993863        0.976872       0.9768  0.976786  \n",
       "  276      0.994117  0.994115        0.978731       0.9786  0.978608  \n",
       "  278      0.994617  0.994616        0.979127       0.9790  0.978996  \n",
       "  280      0.996617  0.996619        0.978499       0.9783  0.978302  \n",
       "  282      0.997200  0.997200        0.979276       0.9792  0.979202  \n",
       "  284      0.998500  0.998500        0.981363       0.9813  0.981309  \n",
       "  286      0.996300  0.996299        0.976251       0.9760  0.975999  \n",
       "  288      0.989633  0.989627        0.977549       0.9774  0.977400  \n",
       "  290      0.989767  0.989782        0.977452       0.9771  0.977118  \n",
       "  292      0.991800  0.991805        0.976987       0.9766  0.976644  \n",
       "  294      0.992050  0.992053        0.980014       0.9799  0.979916  \n",
       "  296      0.993550  0.993540        0.975758       0.9756  0.975599  \n",
       "  298      0.992700  0.992699        0.979689       0.9796  0.979584  \n",
       "  300      0.995667  0.995668        0.978988       0.9789  0.978894  \n",
       "  302      0.993817  0.993829        0.978565       0.9782  0.978248  \n",
       "  304      0.993583  0.993578        0.980205       0.9800  0.980004  \n",
       "  306      0.997083  0.997085        0.981044       0.9809  0.980915  \n",
       "  308      0.998983  0.998983        0.982703       0.9827  0.982694  \n",
       "  310      0.997000  0.997001        0.978558       0.9784  0.978406  \n",
       "  312      0.991417  0.991409        0.979475       0.9793  0.979286  \n",
       "  314      0.993617  0.993619        0.978779       0.9786  0.978610  \n",
       "  316      0.990050  0.990067        0.974188       0.9738  0.973850  \n",
       "  318      0.991483  0.991475        0.975838       0.9757  0.975688  \n",
       "  320      0.993350  0.993341        0.978838       0.9787  0.978678  \n",
       "  322      0.995117  0.995117        0.979811       0.9797  0.979694  \n",
       "  324      0.995383  0.995383        0.978861       0.9787  0.978711  \n",
       "  326      0.998067  0.998067        0.981824       0.9818  0.981798  \n",
       "  328      0.997000  0.996999        0.981215       0.9812  0.981199  \n",
       "  330      0.997283  0.997284        0.981767       0.9817  0.981708  \n",
       "  332      0.996933  0.996934        0.981967       0.9818  0.981804  \n",
       "  334      0.998367  0.998366        0.982069       0.9820  0.982002  \n",
       "  336      0.990083  0.990091        0.975351       0.9752  0.975204  \n",
       "  338      0.990467  0.990488        0.976606       0.9762  0.976229  \n",
       "  340      0.989983  0.990002        0.976978       0.9767  0.976718  \n",
       "  342      0.984033  0.983967        0.970105       0.9693  0.969175  \n",
       "  344      0.993783  0.993786        0.982156       0.9821  0.982101  \n",
       "  346      0.995067  0.995062        0.981324       0.9812  0.981188  \n",
       "  348      0.992400  0.992396        0.979655       0.9796  0.979577  \n",
       "  350      0.994533  0.994543        0.979724       0.9795  0.979510  \n",
       "  352      0.998117  0.998117        0.981818       0.9818  0.981790  \n",
       "  354      0.997967  0.997967        0.982093       0.9820  0.981999  \n",
       "  356      0.997450  0.997451        0.982624       0.9825  0.982508  \n",
       "  358      0.997717  0.997717        0.982128       0.9820  0.982011  )]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = new_record_df.groupby('layer')\n",
    "\n",
    "grouped_dfs = []\n",
    "\n",
    "for layer, group in grouped:\n",
    "    group_name = f\"group_layer_{layer}\"\n",
    "    group_df = pd.DataFrame(group)\n",
    "    grouped_dfs.append((group_name, group_df))\n",
    "\n",
    "grouped_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35c191de-03c8-48a4-a470-54b2f6ebb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = new_record_df.groupby(['layer', 'units', 'epochs', 'batch_size'])\n",
    "\n",
    "# 對每個分組計算所需的統計信息\n",
    "grouped_stats = grouped.agg({\n",
    "    'train_precision': ['mean', 'std', 'max', 'min'],\n",
    "    'train_recall': ['mean', 'std', 'max', 'min'],\n",
    "    'train_f1': ['mean', 'std', 'max', 'min'],\n",
    "    'test_precision': ['mean', 'std', 'max', 'min'],\n",
    "    'test_recall': ['mean', 'std', 'max', 'min'],\n",
    "    'test_f1': ['mean', 'std', 'max', 'min']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "44fa65d7-51d6-4848-af60-df2265ec5ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986311</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.986302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984105</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.983702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.983372</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.983311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.982486</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>0.982412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.982324</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.982302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.995785</td>\n",
       "      <td>0.995767</td>\n",
       "      <td>0.995770</td>\n",
       "      <td>0.982306</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>0.982212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981919</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>0.981896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.997774</td>\n",
       "      <td>0.997767</td>\n",
       "      <td>0.997767</td>\n",
       "      <td>0.981836</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.981805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.981698</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.981697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.981630</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.981598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996681</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.996664</td>\n",
       "      <td>0.981600</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998918</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.981551</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998386</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.981538</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.998437</td>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.998434</td>\n",
       "      <td>0.981266</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.981209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.995232</td>\n",
       "      <td>0.995217</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.981063</td>\n",
       "      <td>0.9810</td>\n",
       "      <td>0.980998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.994313</td>\n",
       "      <td>0.994267</td>\n",
       "      <td>0.994272</td>\n",
       "      <td>0.980856</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.980716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997439</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.980737</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.980687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997383</td>\n",
       "      <td>0.997367</td>\n",
       "      <td>0.997368</td>\n",
       "      <td>0.980527</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.980319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.980265</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.980203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998223</td>\n",
       "      <td>0.998217</td>\n",
       "      <td>0.998216</td>\n",
       "      <td>0.980162</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.980098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997705</td>\n",
       "      <td>0.997700</td>\n",
       "      <td>0.997699</td>\n",
       "      <td>0.980113</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.980007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.997821</td>\n",
       "      <td>0.997817</td>\n",
       "      <td>0.997816</td>\n",
       "      <td>0.980060</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.979995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.996949</td>\n",
       "      <td>0.996933</td>\n",
       "      <td>0.996934</td>\n",
       "      <td>0.979746</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998968</td>\n",
       "      <td>0.998967</td>\n",
       "      <td>0.998966</td>\n",
       "      <td>0.979568</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>0.979491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997328</td>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.979507</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.979402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993523</td>\n",
       "      <td>0.993433</td>\n",
       "      <td>0.993443</td>\n",
       "      <td>0.979454</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.979239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996723</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.9793</td>\n",
       "      <td>0.979302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.993394</td>\n",
       "      <td>0.993383</td>\n",
       "      <td>0.993375</td>\n",
       "      <td>0.979143</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.979075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996569</td>\n",
       "      <td>0.996550</td>\n",
       "      <td>0.996550</td>\n",
       "      <td>0.979043</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.996767</td>\n",
       "      <td>0.996766</td>\n",
       "      <td>0.978993</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.994923</td>\n",
       "      <td>0.994917</td>\n",
       "      <td>0.994916</td>\n",
       "      <td>0.978928</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.992190</td>\n",
       "      <td>0.992150</td>\n",
       "      <td>0.992154</td>\n",
       "      <td>0.978619</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998685</td>\n",
       "      <td>0.998683</td>\n",
       "      <td>0.998683</td>\n",
       "      <td>0.978574</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.994865</td>\n",
       "      <td>0.994833</td>\n",
       "      <td>0.994837</td>\n",
       "      <td>0.977946</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.977805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.998951</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.977681</td>\n",
       "      <td>0.9776</td>\n",
       "      <td>0.977599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.989379</td>\n",
       "      <td>0.989333</td>\n",
       "      <td>0.989332</td>\n",
       "      <td>0.977450</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>0.977398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.997743</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>0.977402</td>\n",
       "      <td>0.9773</td>\n",
       "      <td>0.977295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.989460</td>\n",
       "      <td>0.989433</td>\n",
       "      <td>0.989432</td>\n",
       "      <td>0.977221</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.977087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.988694</td>\n",
       "      <td>0.988617</td>\n",
       "      <td>0.988611</td>\n",
       "      <td>0.977182</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.977077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.990632</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.976850</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>0.976790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.990378</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.976579</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.976502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.987397</td>\n",
       "      <td>0.987367</td>\n",
       "      <td>0.987373</td>\n",
       "      <td>0.976552</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.976502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.993729</td>\n",
       "      <td>0.993617</td>\n",
       "      <td>0.993631</td>\n",
       "      <td>0.976229</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.975939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.992828</td>\n",
       "      <td>0.992733</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.976160</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.975718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.991063</td>\n",
       "      <td>0.990900</td>\n",
       "      <td>0.990917</td>\n",
       "      <td>0.975801</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.975342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997905</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.975441</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.975319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.992240</td>\n",
       "      <td>0.992233</td>\n",
       "      <td>0.992234</td>\n",
       "      <td>0.975222</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.975199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.975071</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>0.974990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.984728</td>\n",
       "      <td>0.984700</td>\n",
       "      <td>0.984693</td>\n",
       "      <td>0.974846</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>0.974780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.984111</td>\n",
       "      <td>0.984083</td>\n",
       "      <td>0.984080</td>\n",
       "      <td>0.974304</td>\n",
       "      <td>0.9743</td>\n",
       "      <td>0.974288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998041</td>\n",
       "      <td>0.998033</td>\n",
       "      <td>0.998034</td>\n",
       "      <td>0.974205</td>\n",
       "      <td>0.9741</td>\n",
       "      <td>0.974099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.996405</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.973931</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.990237</td>\n",
       "      <td>0.990183</td>\n",
       "      <td>0.990175</td>\n",
       "      <td>0.973276</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.973191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.980283</td>\n",
       "      <td>0.980233</td>\n",
       "      <td>0.980216</td>\n",
       "      <td>0.971597</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.971468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.981986</td>\n",
       "      <td>0.981817</td>\n",
       "      <td>0.981786</td>\n",
       "      <td>0.969419</td>\n",
       "      <td>0.9691</td>\n",
       "      <td>0.969042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.975667</td>\n",
       "      <td>0.975567</td>\n",
       "      <td>0.975530</td>\n",
       "      <td>0.968972</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.968748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.980676</td>\n",
       "      <td>0.980600</td>\n",
       "      <td>0.980606</td>\n",
       "      <td>0.968335</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>0.968303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.975075</td>\n",
       "      <td>0.975050</td>\n",
       "      <td>0.975023</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.966768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>0.966567</td>\n",
       "      <td>0.966554</td>\n",
       "      <td>0.960551</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.960364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer  units  epochs  batch_size  train_precision  train_recall   \n",
       "116      1   1024      20         128         1.000000      1.000000  \\\n",
       "118      1   1024      20         256         1.000000      1.000000   \n",
       "110      1   1024      10         256         0.999251      0.999250   \n",
       "92       1    512      20         128         0.999883      0.999883   \n",
       "88       1    512      20          32         0.998551      0.998550   \n",
       "70       1    256      20         256         0.999917      0.999917   \n",
       "100      1   1024       5         128         0.995785      0.995767   \n",
       "94       1    512      20         256         1.000000      1.000000   \n",
       "60       1    256      10         128         0.997774      0.997767   \n",
       "68       1    256      20         128         0.999950      0.999950   \n",
       "66       1    256      20          64         0.999451      0.999450   \n",
       "104      1   1024      10          32         0.996681      0.996667   \n",
       "108      1   1024      10         128         0.998918      0.998917   \n",
       "84       1    512      10         128         0.998386      0.998383   \n",
       "86       1    512      10         256         0.998437      0.998433   \n",
       "74       1    512       5          64         0.995232      0.995217   \n",
       "98       1   1024       5          64         0.994313      0.994267   \n",
       "114      1   1024      20          64         0.997439      0.997433   \n",
       "82       1    512      10          64         0.997383      0.997367   \n",
       "76       1    512       5         128         0.993976      0.993967   \n",
       "64       1    256      20          32         0.998223      0.998217   \n",
       "106      1   1024      10          64         0.997705      0.997700   \n",
       "112      1   1024      20          32         0.997821      0.997817   \n",
       "90       1    512      20          64         0.996949      0.996933   \n",
       "44       1    128      20         128         0.998968      0.998967   \n",
       "58       1    256      10          64         0.997328      0.997317   \n",
       "72       1    512       5          32         0.993523      0.993433   \n",
       "80       1    512      10          32         0.996723      0.996717   \n",
       "102      1   1024       5         256         0.993394      0.993383   \n",
       "32       1    128      10          32         0.996569      0.996550   \n",
       "56       1    256      10          32         0.996774      0.996767   \n",
       "36       1    128      10         128         0.994923      0.994917   \n",
       "50       1    256       5          64         0.992190      0.992150   \n",
       "40       1    128      20          32         0.998685      0.998683   \n",
       "62       1    256      10         256         0.994865      0.994833   \n",
       "42       1    128      20          64         0.998951      0.998950   \n",
       "78       1    512       5         256         0.989379      0.989333   \n",
       "46       1    128      20         256         0.997743      0.997733   \n",
       "26       1    128       5          64         0.989460      0.989433   \n",
       "52       1    256       5         128         0.988694      0.988617   \n",
       "38       1    128      10         256         0.990632      0.990617   \n",
       "24       1    128       5          32         0.990378      0.990333   \n",
       "12       1     64      10         128         0.987397      0.987367   \n",
       "34       1    128      10          64         0.993729      0.993617   \n",
       "96       1   1024       5          32         0.992828      0.992733   \n",
       "48       1    256       5          32         0.991063      0.990900   \n",
       "18       1     64      20          64         0.997905      0.997900   \n",
       "22       1     64      20         256         0.992240      0.992233   \n",
       "8        1     64      10          32         0.993750      0.993733   \n",
       "54       1    256       5         256         0.984728      0.984700   \n",
       "28       1    128       5         128         0.984111      0.984083   \n",
       "16       1     64      20          32         0.998041      0.998033   \n",
       "20       1     64      20         128         0.996405      0.996400   \n",
       "10       1     64      10          64         0.990237      0.990183   \n",
       "2        1     64       5          64         0.980283      0.980233   \n",
       "0        1     64       5          32         0.981986      0.981817   \n",
       "30       1    128       5         256         0.975667      0.975567   \n",
       "14       1     64      10         256         0.980676      0.980600   \n",
       "4        1     64       5         128         0.975075      0.975050   \n",
       "6        1     64       5         256         0.966627      0.966567   \n",
       "\n",
       "     train_f1  test_precision  test_recall   test_f1  \n",
       "116  1.000000        0.986311       0.9863  0.986302  \n",
       "118  1.000000        0.984105       0.9841  0.984100  \n",
       "110  0.999250        0.983741       0.9837  0.983702  \n",
       "92   0.999883        0.983372       0.9833  0.983311  \n",
       "88   0.998550        0.982486       0.9824  0.982412  \n",
       "70   0.999917        0.982324       0.9823  0.982302  \n",
       "100  0.995770        0.982306       0.9822  0.982212  \n",
       "94   1.000000        0.981919       0.9819  0.981896  \n",
       "60   0.997767        0.981836       0.9818  0.981805  \n",
       "68   0.999950        0.981698       0.9817  0.981697  \n",
       "66   0.999450        0.981630       0.9816  0.981598  \n",
       "104  0.996664        0.981600       0.9815  0.981494  \n",
       "108  0.998917        0.981551       0.9815  0.981499  \n",
       "84   0.998383        0.981538       0.9815  0.981493  \n",
       "86   0.998434        0.981266       0.9812  0.981209  \n",
       "74   0.995215        0.981063       0.9810  0.980998  \n",
       "98   0.994272        0.980856       0.9807  0.980716  \n",
       "114  0.997433        0.980737       0.9807  0.980687  \n",
       "82   0.997368        0.980527       0.9803  0.980319  \n",
       "76   0.993967        0.980265       0.9802  0.980203  \n",
       "64   0.998216        0.980162       0.9801  0.980098  \n",
       "106  0.997699        0.980113       0.9800  0.980007  \n",
       "112  0.997816        0.980060       0.9800  0.979995  \n",
       "90   0.996934        0.979746       0.9796  0.979600  \n",
       "44   0.998966        0.979568       0.9795  0.979491  \n",
       "58   0.997317        0.979507       0.9794  0.979402  \n",
       "72   0.993443        0.979454       0.9792  0.979239  \n",
       "80   0.996717        0.979352       0.9793  0.979302  \n",
       "102  0.993375        0.979143       0.9791  0.979075  \n",
       "32   0.996550        0.979043       0.9789  0.978903  \n",
       "56   0.996766        0.978993       0.9789  0.978908  \n",
       "36   0.994916        0.978928       0.9789  0.978896  \n",
       "50   0.992154        0.978619       0.9785  0.978508  \n",
       "40   0.998683        0.978574       0.9785  0.978500  \n",
       "62   0.994837        0.977946       0.9778  0.977805  \n",
       "42   0.998950        0.977681       0.9776  0.977599  \n",
       "78   0.989332        0.977450       0.9774  0.977398  \n",
       "46   0.997733        0.977402       0.9773  0.977295  \n",
       "26   0.989432        0.977221       0.9771  0.977087  \n",
       "52   0.988611        0.977182       0.9771  0.977077  \n",
       "38   0.990617        0.976850       0.9768  0.976790  \n",
       "24   0.990333        0.976579       0.9765  0.976502  \n",
       "12   0.987373        0.976552       0.9765  0.976502  \n",
       "34   0.993631        0.976229       0.9759  0.975939  \n",
       "96   0.992739        0.976160       0.9757  0.975718  \n",
       "48   0.990917        0.975801       0.9753  0.975342  \n",
       "18   0.997900        0.975441       0.9753  0.975319  \n",
       "22   0.992234        0.975222       0.9752  0.975199  \n",
       "8    0.993733        0.975071       0.9750  0.974990  \n",
       "54   0.984693        0.974846       0.9748  0.974780  \n",
       "28   0.984080        0.974304       0.9743  0.974288  \n",
       "16   0.998034        0.974205       0.9741  0.974099  \n",
       "20   0.996400        0.973931       0.9739  0.973900  \n",
       "10   0.990175        0.973276       0.9732  0.973191  \n",
       "2    0.980216        0.971597       0.9715  0.971468  \n",
       "0    0.981786        0.969419       0.9691  0.969042  \n",
       "30   0.975530        0.968972       0.9688  0.968748  \n",
       "14   0.980606        0.968335       0.9683  0.968303  \n",
       "4    0.975023        0.966797       0.9668  0.966768  \n",
       "6    0.966554        0.960551       0.9604  0.960364  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_name = 'group_layer_1'\n",
    "group_df = next(df for name, df in grouped_dfs if name == group_name)\n",
    "# print(group_df)\n",
    "\n",
    "sorted_group_df = group_df[['layer', 'units', 'epochs', 'batch_size', 'train_precision', 'train_recall', 'train_f1', 'test_precision', 'test_recall', 'test_f1']]\n",
    "sorted_group_df = sorted_group_df.sort_values(by='test_precision', ascending=False)\n",
    "sorted_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a40fa35-ace2-4c8d-be6e-c16a5681518a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986311</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.986302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984105</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.983702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.983372</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.983311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.982486</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>0.982412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.982324</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.982302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.995785</td>\n",
       "      <td>0.995767</td>\n",
       "      <td>0.995770</td>\n",
       "      <td>0.982306</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>0.982212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981919</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>0.981896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.997774</td>\n",
       "      <td>0.997767</td>\n",
       "      <td>0.997767</td>\n",
       "      <td>0.981836</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.981805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.981698</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.981697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.981630</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.981598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996681</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.996664</td>\n",
       "      <td>0.981600</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998918</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.981551</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998386</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.981538</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.998437</td>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.998434</td>\n",
       "      <td>0.981266</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.981209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.995232</td>\n",
       "      <td>0.995217</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.981063</td>\n",
       "      <td>0.9810</td>\n",
       "      <td>0.980998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.994313</td>\n",
       "      <td>0.994267</td>\n",
       "      <td>0.994272</td>\n",
       "      <td>0.980856</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.980716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997439</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.980737</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.980687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997383</td>\n",
       "      <td>0.997367</td>\n",
       "      <td>0.997368</td>\n",
       "      <td>0.980527</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.980319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.980265</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.980203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998223</td>\n",
       "      <td>0.998217</td>\n",
       "      <td>0.998216</td>\n",
       "      <td>0.980162</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.980098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997705</td>\n",
       "      <td>0.997700</td>\n",
       "      <td>0.997699</td>\n",
       "      <td>0.980113</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.980007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.997821</td>\n",
       "      <td>0.997817</td>\n",
       "      <td>0.997816</td>\n",
       "      <td>0.980060</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.979995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.996949</td>\n",
       "      <td>0.996933</td>\n",
       "      <td>0.996934</td>\n",
       "      <td>0.979746</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998968</td>\n",
       "      <td>0.998967</td>\n",
       "      <td>0.998966</td>\n",
       "      <td>0.979568</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>0.979491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997328</td>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.979507</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.979402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996723</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.9793</td>\n",
       "      <td>0.979302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993523</td>\n",
       "      <td>0.993433</td>\n",
       "      <td>0.993443</td>\n",
       "      <td>0.979454</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.979239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.993394</td>\n",
       "      <td>0.993383</td>\n",
       "      <td>0.993375</td>\n",
       "      <td>0.979143</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.979075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.994923</td>\n",
       "      <td>0.994917</td>\n",
       "      <td>0.994916</td>\n",
       "      <td>0.978928</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.996767</td>\n",
       "      <td>0.996766</td>\n",
       "      <td>0.978993</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996569</td>\n",
       "      <td>0.996550</td>\n",
       "      <td>0.996550</td>\n",
       "      <td>0.979043</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998685</td>\n",
       "      <td>0.998683</td>\n",
       "      <td>0.998683</td>\n",
       "      <td>0.978574</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.992190</td>\n",
       "      <td>0.992150</td>\n",
       "      <td>0.992154</td>\n",
       "      <td>0.978619</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.994865</td>\n",
       "      <td>0.994833</td>\n",
       "      <td>0.994837</td>\n",
       "      <td>0.977946</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.977805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.998951</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.977681</td>\n",
       "      <td>0.9776</td>\n",
       "      <td>0.977599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.989379</td>\n",
       "      <td>0.989333</td>\n",
       "      <td>0.989332</td>\n",
       "      <td>0.977450</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>0.977398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.997743</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>0.977402</td>\n",
       "      <td>0.9773</td>\n",
       "      <td>0.977295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.989460</td>\n",
       "      <td>0.989433</td>\n",
       "      <td>0.989432</td>\n",
       "      <td>0.977221</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.977087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.988694</td>\n",
       "      <td>0.988617</td>\n",
       "      <td>0.988611</td>\n",
       "      <td>0.977182</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.977077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.990632</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.976850</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>0.976790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.990378</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.976579</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.976502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.987397</td>\n",
       "      <td>0.987367</td>\n",
       "      <td>0.987373</td>\n",
       "      <td>0.976552</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.976502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.993729</td>\n",
       "      <td>0.993617</td>\n",
       "      <td>0.993631</td>\n",
       "      <td>0.976229</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.975939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.992828</td>\n",
       "      <td>0.992733</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.976160</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.975718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997905</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.975441</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.975319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.991063</td>\n",
       "      <td>0.990900</td>\n",
       "      <td>0.990917</td>\n",
       "      <td>0.975801</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.975342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.992240</td>\n",
       "      <td>0.992233</td>\n",
       "      <td>0.992234</td>\n",
       "      <td>0.975222</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.975199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.975071</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>0.974990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.984728</td>\n",
       "      <td>0.984700</td>\n",
       "      <td>0.984693</td>\n",
       "      <td>0.974846</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>0.974780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.984111</td>\n",
       "      <td>0.984083</td>\n",
       "      <td>0.984080</td>\n",
       "      <td>0.974304</td>\n",
       "      <td>0.9743</td>\n",
       "      <td>0.974288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998041</td>\n",
       "      <td>0.998033</td>\n",
       "      <td>0.998034</td>\n",
       "      <td>0.974205</td>\n",
       "      <td>0.9741</td>\n",
       "      <td>0.974099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.996405</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.973931</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.990237</td>\n",
       "      <td>0.990183</td>\n",
       "      <td>0.990175</td>\n",
       "      <td>0.973276</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.973191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.980283</td>\n",
       "      <td>0.980233</td>\n",
       "      <td>0.980216</td>\n",
       "      <td>0.971597</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.971468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.981986</td>\n",
       "      <td>0.981817</td>\n",
       "      <td>0.981786</td>\n",
       "      <td>0.969419</td>\n",
       "      <td>0.9691</td>\n",
       "      <td>0.969042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.975667</td>\n",
       "      <td>0.975567</td>\n",
       "      <td>0.975530</td>\n",
       "      <td>0.968972</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.968748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.980676</td>\n",
       "      <td>0.980600</td>\n",
       "      <td>0.980606</td>\n",
       "      <td>0.968335</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>0.968303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.975075</td>\n",
       "      <td>0.975050</td>\n",
       "      <td>0.975023</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.966768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>0.966567</td>\n",
       "      <td>0.966554</td>\n",
       "      <td>0.960551</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.960364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer  units  epochs  batch_size  train_precision  train_recall   \n",
       "116      1   1024      20         128         1.000000      1.000000  \\\n",
       "118      1   1024      20         256         1.000000      1.000000   \n",
       "110      1   1024      10         256         0.999251      0.999250   \n",
       "92       1    512      20         128         0.999883      0.999883   \n",
       "88       1    512      20          32         0.998551      0.998550   \n",
       "70       1    256      20         256         0.999917      0.999917   \n",
       "100      1   1024       5         128         0.995785      0.995767   \n",
       "94       1    512      20         256         1.000000      1.000000   \n",
       "60       1    256      10         128         0.997774      0.997767   \n",
       "68       1    256      20         128         0.999950      0.999950   \n",
       "66       1    256      20          64         0.999451      0.999450   \n",
       "104      1   1024      10          32         0.996681      0.996667   \n",
       "108      1   1024      10         128         0.998918      0.998917   \n",
       "84       1    512      10         128         0.998386      0.998383   \n",
       "86       1    512      10         256         0.998437      0.998433   \n",
       "74       1    512       5          64         0.995232      0.995217   \n",
       "98       1   1024       5          64         0.994313      0.994267   \n",
       "114      1   1024      20          64         0.997439      0.997433   \n",
       "82       1    512      10          64         0.997383      0.997367   \n",
       "76       1    512       5         128         0.993976      0.993967   \n",
       "64       1    256      20          32         0.998223      0.998217   \n",
       "106      1   1024      10          64         0.997705      0.997700   \n",
       "112      1   1024      20          32         0.997821      0.997817   \n",
       "90       1    512      20          64         0.996949      0.996933   \n",
       "44       1    128      20         128         0.998968      0.998967   \n",
       "58       1    256      10          64         0.997328      0.997317   \n",
       "80       1    512      10          32         0.996723      0.996717   \n",
       "72       1    512       5          32         0.993523      0.993433   \n",
       "102      1   1024       5         256         0.993394      0.993383   \n",
       "36       1    128      10         128         0.994923      0.994917   \n",
       "56       1    256      10          32         0.996774      0.996767   \n",
       "32       1    128      10          32         0.996569      0.996550   \n",
       "40       1    128      20          32         0.998685      0.998683   \n",
       "50       1    256       5          64         0.992190      0.992150   \n",
       "62       1    256      10         256         0.994865      0.994833   \n",
       "42       1    128      20          64         0.998951      0.998950   \n",
       "78       1    512       5         256         0.989379      0.989333   \n",
       "46       1    128      20         256         0.997743      0.997733   \n",
       "26       1    128       5          64         0.989460      0.989433   \n",
       "52       1    256       5         128         0.988694      0.988617   \n",
       "38       1    128      10         256         0.990632      0.990617   \n",
       "24       1    128       5          32         0.990378      0.990333   \n",
       "12       1     64      10         128         0.987397      0.987367   \n",
       "34       1    128      10          64         0.993729      0.993617   \n",
       "96       1   1024       5          32         0.992828      0.992733   \n",
       "18       1     64      20          64         0.997905      0.997900   \n",
       "48       1    256       5          32         0.991063      0.990900   \n",
       "22       1     64      20         256         0.992240      0.992233   \n",
       "8        1     64      10          32         0.993750      0.993733   \n",
       "54       1    256       5         256         0.984728      0.984700   \n",
       "28       1    128       5         128         0.984111      0.984083   \n",
       "16       1     64      20          32         0.998041      0.998033   \n",
       "20       1     64      20         128         0.996405      0.996400   \n",
       "10       1     64      10          64         0.990237      0.990183   \n",
       "2        1     64       5          64         0.980283      0.980233   \n",
       "0        1     64       5          32         0.981986      0.981817   \n",
       "30       1    128       5         256         0.975667      0.975567   \n",
       "14       1     64      10         256         0.980676      0.980600   \n",
       "4        1     64       5         128         0.975075      0.975050   \n",
       "6        1     64       5         256         0.966627      0.966567   \n",
       "\n",
       "     train_f1  test_precision  test_recall   test_f1  \n",
       "116  1.000000        0.986311       0.9863  0.986302  \n",
       "118  1.000000        0.984105       0.9841  0.984100  \n",
       "110  0.999250        0.983741       0.9837  0.983702  \n",
       "92   0.999883        0.983372       0.9833  0.983311  \n",
       "88   0.998550        0.982486       0.9824  0.982412  \n",
       "70   0.999917        0.982324       0.9823  0.982302  \n",
       "100  0.995770        0.982306       0.9822  0.982212  \n",
       "94   1.000000        0.981919       0.9819  0.981896  \n",
       "60   0.997767        0.981836       0.9818  0.981805  \n",
       "68   0.999950        0.981698       0.9817  0.981697  \n",
       "66   0.999450        0.981630       0.9816  0.981598  \n",
       "104  0.996664        0.981600       0.9815  0.981494  \n",
       "108  0.998917        0.981551       0.9815  0.981499  \n",
       "84   0.998383        0.981538       0.9815  0.981493  \n",
       "86   0.998434        0.981266       0.9812  0.981209  \n",
       "74   0.995215        0.981063       0.9810  0.980998  \n",
       "98   0.994272        0.980856       0.9807  0.980716  \n",
       "114  0.997433        0.980737       0.9807  0.980687  \n",
       "82   0.997368        0.980527       0.9803  0.980319  \n",
       "76   0.993967        0.980265       0.9802  0.980203  \n",
       "64   0.998216        0.980162       0.9801  0.980098  \n",
       "106  0.997699        0.980113       0.9800  0.980007  \n",
       "112  0.997816        0.980060       0.9800  0.979995  \n",
       "90   0.996934        0.979746       0.9796  0.979600  \n",
       "44   0.998966        0.979568       0.9795  0.979491  \n",
       "58   0.997317        0.979507       0.9794  0.979402  \n",
       "80   0.996717        0.979352       0.9793  0.979302  \n",
       "72   0.993443        0.979454       0.9792  0.979239  \n",
       "102  0.993375        0.979143       0.9791  0.979075  \n",
       "36   0.994916        0.978928       0.9789  0.978896  \n",
       "56   0.996766        0.978993       0.9789  0.978908  \n",
       "32   0.996550        0.979043       0.9789  0.978903  \n",
       "40   0.998683        0.978574       0.9785  0.978500  \n",
       "50   0.992154        0.978619       0.9785  0.978508  \n",
       "62   0.994837        0.977946       0.9778  0.977805  \n",
       "42   0.998950        0.977681       0.9776  0.977599  \n",
       "78   0.989332        0.977450       0.9774  0.977398  \n",
       "46   0.997733        0.977402       0.9773  0.977295  \n",
       "26   0.989432        0.977221       0.9771  0.977087  \n",
       "52   0.988611        0.977182       0.9771  0.977077  \n",
       "38   0.990617        0.976850       0.9768  0.976790  \n",
       "24   0.990333        0.976579       0.9765  0.976502  \n",
       "12   0.987373        0.976552       0.9765  0.976502  \n",
       "34   0.993631        0.976229       0.9759  0.975939  \n",
       "96   0.992739        0.976160       0.9757  0.975718  \n",
       "18   0.997900        0.975441       0.9753  0.975319  \n",
       "48   0.990917        0.975801       0.9753  0.975342  \n",
       "22   0.992234        0.975222       0.9752  0.975199  \n",
       "8    0.993733        0.975071       0.9750  0.974990  \n",
       "54   0.984693        0.974846       0.9748  0.974780  \n",
       "28   0.984080        0.974304       0.9743  0.974288  \n",
       "16   0.998034        0.974205       0.9741  0.974099  \n",
       "20   0.996400        0.973931       0.9739  0.973900  \n",
       "10   0.990175        0.973276       0.9732  0.973191  \n",
       "2    0.980216        0.971597       0.9715  0.971468  \n",
       "0    0.981786        0.969419       0.9691  0.969042  \n",
       "30   0.975530        0.968972       0.9688  0.968748  \n",
       "14   0.980606        0.968335       0.9683  0.968303  \n",
       "4    0.975023        0.966797       0.9668  0.966768  \n",
       "6    0.966554        0.960551       0.9604  0.960364  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_group_df = sorted_group_df.sort_values(by='test_recall', ascending=False)\n",
    "sorted_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a4d7fd5-c1b1-42cc-97da-cd0e69f29e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986311</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.986302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984105</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.983702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.983372</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.983311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.982486</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>0.982412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.982324</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.982302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.995785</td>\n",
       "      <td>0.995767</td>\n",
       "      <td>0.995770</td>\n",
       "      <td>0.982306</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>0.982212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981919</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>0.981896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.997774</td>\n",
       "      <td>0.997767</td>\n",
       "      <td>0.997767</td>\n",
       "      <td>0.981836</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.981805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.981698</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.981697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.981630</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.981598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998918</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.981551</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996681</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.996664</td>\n",
       "      <td>0.981600</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998386</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.981538</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.998437</td>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.998434</td>\n",
       "      <td>0.981266</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.981209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.995232</td>\n",
       "      <td>0.995217</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.981063</td>\n",
       "      <td>0.9810</td>\n",
       "      <td>0.980998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.994313</td>\n",
       "      <td>0.994267</td>\n",
       "      <td>0.994272</td>\n",
       "      <td>0.980856</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.980716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997439</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.980737</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.980687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997383</td>\n",
       "      <td>0.997367</td>\n",
       "      <td>0.997368</td>\n",
       "      <td>0.980527</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.980319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.980265</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.980203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998223</td>\n",
       "      <td>0.998217</td>\n",
       "      <td>0.998216</td>\n",
       "      <td>0.980162</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.980098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997705</td>\n",
       "      <td>0.997700</td>\n",
       "      <td>0.997699</td>\n",
       "      <td>0.980113</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.980007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.997821</td>\n",
       "      <td>0.997817</td>\n",
       "      <td>0.997816</td>\n",
       "      <td>0.980060</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.979995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.996949</td>\n",
       "      <td>0.996933</td>\n",
       "      <td>0.996934</td>\n",
       "      <td>0.979746</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998968</td>\n",
       "      <td>0.998967</td>\n",
       "      <td>0.998966</td>\n",
       "      <td>0.979568</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>0.979491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997328</td>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.979507</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.979402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996723</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.9793</td>\n",
       "      <td>0.979302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993523</td>\n",
       "      <td>0.993433</td>\n",
       "      <td>0.993443</td>\n",
       "      <td>0.979454</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.979239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.993394</td>\n",
       "      <td>0.993383</td>\n",
       "      <td>0.993375</td>\n",
       "      <td>0.979143</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.979075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.996767</td>\n",
       "      <td>0.996766</td>\n",
       "      <td>0.978993</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996569</td>\n",
       "      <td>0.996550</td>\n",
       "      <td>0.996550</td>\n",
       "      <td>0.979043</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.994923</td>\n",
       "      <td>0.994917</td>\n",
       "      <td>0.994916</td>\n",
       "      <td>0.978928</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.992190</td>\n",
       "      <td>0.992150</td>\n",
       "      <td>0.992154</td>\n",
       "      <td>0.978619</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998685</td>\n",
       "      <td>0.998683</td>\n",
       "      <td>0.998683</td>\n",
       "      <td>0.978574</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.994865</td>\n",
       "      <td>0.994833</td>\n",
       "      <td>0.994837</td>\n",
       "      <td>0.977946</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.977805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.998951</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.977681</td>\n",
       "      <td>0.9776</td>\n",
       "      <td>0.977599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.989379</td>\n",
       "      <td>0.989333</td>\n",
       "      <td>0.989332</td>\n",
       "      <td>0.977450</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>0.977398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.997743</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>0.977402</td>\n",
       "      <td>0.9773</td>\n",
       "      <td>0.977295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.989460</td>\n",
       "      <td>0.989433</td>\n",
       "      <td>0.989432</td>\n",
       "      <td>0.977221</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.977087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.988694</td>\n",
       "      <td>0.988617</td>\n",
       "      <td>0.988611</td>\n",
       "      <td>0.977182</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.977077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.990632</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.976850</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>0.976790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.990378</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.976579</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.976502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.987397</td>\n",
       "      <td>0.987367</td>\n",
       "      <td>0.987373</td>\n",
       "      <td>0.976552</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.976502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.993729</td>\n",
       "      <td>0.993617</td>\n",
       "      <td>0.993631</td>\n",
       "      <td>0.976229</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.975939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.992828</td>\n",
       "      <td>0.992733</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.976160</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.975718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.991063</td>\n",
       "      <td>0.990900</td>\n",
       "      <td>0.990917</td>\n",
       "      <td>0.975801</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.975342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997905</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.975441</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.975319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.992240</td>\n",
       "      <td>0.992233</td>\n",
       "      <td>0.992234</td>\n",
       "      <td>0.975222</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.975199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.975071</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>0.974990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.984728</td>\n",
       "      <td>0.984700</td>\n",
       "      <td>0.984693</td>\n",
       "      <td>0.974846</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>0.974780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.984111</td>\n",
       "      <td>0.984083</td>\n",
       "      <td>0.984080</td>\n",
       "      <td>0.974304</td>\n",
       "      <td>0.9743</td>\n",
       "      <td>0.974288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998041</td>\n",
       "      <td>0.998033</td>\n",
       "      <td>0.998034</td>\n",
       "      <td>0.974205</td>\n",
       "      <td>0.9741</td>\n",
       "      <td>0.974099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.996405</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.973931</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.990237</td>\n",
       "      <td>0.990183</td>\n",
       "      <td>0.990175</td>\n",
       "      <td>0.973276</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.973191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.980283</td>\n",
       "      <td>0.980233</td>\n",
       "      <td>0.980216</td>\n",
       "      <td>0.971597</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.971468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.981986</td>\n",
       "      <td>0.981817</td>\n",
       "      <td>0.981786</td>\n",
       "      <td>0.969419</td>\n",
       "      <td>0.9691</td>\n",
       "      <td>0.969042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.975667</td>\n",
       "      <td>0.975567</td>\n",
       "      <td>0.975530</td>\n",
       "      <td>0.968972</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.968748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.980676</td>\n",
       "      <td>0.980600</td>\n",
       "      <td>0.980606</td>\n",
       "      <td>0.968335</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>0.968303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.975075</td>\n",
       "      <td>0.975050</td>\n",
       "      <td>0.975023</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.966768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>0.966567</td>\n",
       "      <td>0.966554</td>\n",
       "      <td>0.960551</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.960364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer  units  epochs  batch_size  train_precision  train_recall   \n",
       "116      1   1024      20         128         1.000000      1.000000  \\\n",
       "118      1   1024      20         256         1.000000      1.000000   \n",
       "110      1   1024      10         256         0.999251      0.999250   \n",
       "92       1    512      20         128         0.999883      0.999883   \n",
       "88       1    512      20          32         0.998551      0.998550   \n",
       "70       1    256      20         256         0.999917      0.999917   \n",
       "100      1   1024       5         128         0.995785      0.995767   \n",
       "94       1    512      20         256         1.000000      1.000000   \n",
       "60       1    256      10         128         0.997774      0.997767   \n",
       "68       1    256      20         128         0.999950      0.999950   \n",
       "66       1    256      20          64         0.999451      0.999450   \n",
       "108      1   1024      10         128         0.998918      0.998917   \n",
       "104      1   1024      10          32         0.996681      0.996667   \n",
       "84       1    512      10         128         0.998386      0.998383   \n",
       "86       1    512      10         256         0.998437      0.998433   \n",
       "74       1    512       5          64         0.995232      0.995217   \n",
       "98       1   1024       5          64         0.994313      0.994267   \n",
       "114      1   1024      20          64         0.997439      0.997433   \n",
       "82       1    512      10          64         0.997383      0.997367   \n",
       "76       1    512       5         128         0.993976      0.993967   \n",
       "64       1    256      20          32         0.998223      0.998217   \n",
       "106      1   1024      10          64         0.997705      0.997700   \n",
       "112      1   1024      20          32         0.997821      0.997817   \n",
       "90       1    512      20          64         0.996949      0.996933   \n",
       "44       1    128      20         128         0.998968      0.998967   \n",
       "58       1    256      10          64         0.997328      0.997317   \n",
       "80       1    512      10          32         0.996723      0.996717   \n",
       "72       1    512       5          32         0.993523      0.993433   \n",
       "102      1   1024       5         256         0.993394      0.993383   \n",
       "56       1    256      10          32         0.996774      0.996767   \n",
       "32       1    128      10          32         0.996569      0.996550   \n",
       "36       1    128      10         128         0.994923      0.994917   \n",
       "50       1    256       5          64         0.992190      0.992150   \n",
       "40       1    128      20          32         0.998685      0.998683   \n",
       "62       1    256      10         256         0.994865      0.994833   \n",
       "42       1    128      20          64         0.998951      0.998950   \n",
       "78       1    512       5         256         0.989379      0.989333   \n",
       "46       1    128      20         256         0.997743      0.997733   \n",
       "26       1    128       5          64         0.989460      0.989433   \n",
       "52       1    256       5         128         0.988694      0.988617   \n",
       "38       1    128      10         256         0.990632      0.990617   \n",
       "24       1    128       5          32         0.990378      0.990333   \n",
       "12       1     64      10         128         0.987397      0.987367   \n",
       "34       1    128      10          64         0.993729      0.993617   \n",
       "96       1   1024       5          32         0.992828      0.992733   \n",
       "48       1    256       5          32         0.991063      0.990900   \n",
       "18       1     64      20          64         0.997905      0.997900   \n",
       "22       1     64      20         256         0.992240      0.992233   \n",
       "8        1     64      10          32         0.993750      0.993733   \n",
       "54       1    256       5         256         0.984728      0.984700   \n",
       "28       1    128       5         128         0.984111      0.984083   \n",
       "16       1     64      20          32         0.998041      0.998033   \n",
       "20       1     64      20         128         0.996405      0.996400   \n",
       "10       1     64      10          64         0.990237      0.990183   \n",
       "2        1     64       5          64         0.980283      0.980233   \n",
       "0        1     64       5          32         0.981986      0.981817   \n",
       "30       1    128       5         256         0.975667      0.975567   \n",
       "14       1     64      10         256         0.980676      0.980600   \n",
       "4        1     64       5         128         0.975075      0.975050   \n",
       "6        1     64       5         256         0.966627      0.966567   \n",
       "\n",
       "     train_f1  test_precision  test_recall   test_f1  \n",
       "116  1.000000        0.986311       0.9863  0.986302  \n",
       "118  1.000000        0.984105       0.9841  0.984100  \n",
       "110  0.999250        0.983741       0.9837  0.983702  \n",
       "92   0.999883        0.983372       0.9833  0.983311  \n",
       "88   0.998550        0.982486       0.9824  0.982412  \n",
       "70   0.999917        0.982324       0.9823  0.982302  \n",
       "100  0.995770        0.982306       0.9822  0.982212  \n",
       "94   1.000000        0.981919       0.9819  0.981896  \n",
       "60   0.997767        0.981836       0.9818  0.981805  \n",
       "68   0.999950        0.981698       0.9817  0.981697  \n",
       "66   0.999450        0.981630       0.9816  0.981598  \n",
       "108  0.998917        0.981551       0.9815  0.981499  \n",
       "104  0.996664        0.981600       0.9815  0.981494  \n",
       "84   0.998383        0.981538       0.9815  0.981493  \n",
       "86   0.998434        0.981266       0.9812  0.981209  \n",
       "74   0.995215        0.981063       0.9810  0.980998  \n",
       "98   0.994272        0.980856       0.9807  0.980716  \n",
       "114  0.997433        0.980737       0.9807  0.980687  \n",
       "82   0.997368        0.980527       0.9803  0.980319  \n",
       "76   0.993967        0.980265       0.9802  0.980203  \n",
       "64   0.998216        0.980162       0.9801  0.980098  \n",
       "106  0.997699        0.980113       0.9800  0.980007  \n",
       "112  0.997816        0.980060       0.9800  0.979995  \n",
       "90   0.996934        0.979746       0.9796  0.979600  \n",
       "44   0.998966        0.979568       0.9795  0.979491  \n",
       "58   0.997317        0.979507       0.9794  0.979402  \n",
       "80   0.996717        0.979352       0.9793  0.979302  \n",
       "72   0.993443        0.979454       0.9792  0.979239  \n",
       "102  0.993375        0.979143       0.9791  0.979075  \n",
       "56   0.996766        0.978993       0.9789  0.978908  \n",
       "32   0.996550        0.979043       0.9789  0.978903  \n",
       "36   0.994916        0.978928       0.9789  0.978896  \n",
       "50   0.992154        0.978619       0.9785  0.978508  \n",
       "40   0.998683        0.978574       0.9785  0.978500  \n",
       "62   0.994837        0.977946       0.9778  0.977805  \n",
       "42   0.998950        0.977681       0.9776  0.977599  \n",
       "78   0.989332        0.977450       0.9774  0.977398  \n",
       "46   0.997733        0.977402       0.9773  0.977295  \n",
       "26   0.989432        0.977221       0.9771  0.977087  \n",
       "52   0.988611        0.977182       0.9771  0.977077  \n",
       "38   0.990617        0.976850       0.9768  0.976790  \n",
       "24   0.990333        0.976579       0.9765  0.976502  \n",
       "12   0.987373        0.976552       0.9765  0.976502  \n",
       "34   0.993631        0.976229       0.9759  0.975939  \n",
       "96   0.992739        0.976160       0.9757  0.975718  \n",
       "48   0.990917        0.975801       0.9753  0.975342  \n",
       "18   0.997900        0.975441       0.9753  0.975319  \n",
       "22   0.992234        0.975222       0.9752  0.975199  \n",
       "8    0.993733        0.975071       0.9750  0.974990  \n",
       "54   0.984693        0.974846       0.9748  0.974780  \n",
       "28   0.984080        0.974304       0.9743  0.974288  \n",
       "16   0.998034        0.974205       0.9741  0.974099  \n",
       "20   0.996400        0.973931       0.9739  0.973900  \n",
       "10   0.990175        0.973276       0.9732  0.973191  \n",
       "2    0.980216        0.971597       0.9715  0.971468  \n",
       "0    0.981786        0.969419       0.9691  0.969042  \n",
       "30   0.975530        0.968972       0.9688  0.968748  \n",
       "14   0.980606        0.968335       0.9683  0.968303  \n",
       "4    0.975023        0.966797       0.9668  0.966768  \n",
       "6    0.966554        0.960551       0.9604  0.960364  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_group_df = sorted_group_df.sort_values(by='test_f1', ascending=False)\n",
    "sorted_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "77e63004-c1f6-4722-884c-04af0362ed95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>0.966567</td>\n",
       "      <td>0.966554</td>\n",
       "      <td>0.960551</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.960364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.975075</td>\n",
       "      <td>0.975050</td>\n",
       "      <td>0.975023</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.966768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.980676</td>\n",
       "      <td>0.980600</td>\n",
       "      <td>0.980606</td>\n",
       "      <td>0.968335</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>0.968303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.975667</td>\n",
       "      <td>0.975567</td>\n",
       "      <td>0.975530</td>\n",
       "      <td>0.968972</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.968748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.981986</td>\n",
       "      <td>0.981817</td>\n",
       "      <td>0.981786</td>\n",
       "      <td>0.969419</td>\n",
       "      <td>0.9691</td>\n",
       "      <td>0.969042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.980283</td>\n",
       "      <td>0.980233</td>\n",
       "      <td>0.980216</td>\n",
       "      <td>0.971597</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.971468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.990237</td>\n",
       "      <td>0.990183</td>\n",
       "      <td>0.990175</td>\n",
       "      <td>0.973276</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.973191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.996405</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.973931</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998041</td>\n",
       "      <td>0.998033</td>\n",
       "      <td>0.998034</td>\n",
       "      <td>0.974205</td>\n",
       "      <td>0.9741</td>\n",
       "      <td>0.974099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.984111</td>\n",
       "      <td>0.984083</td>\n",
       "      <td>0.984080</td>\n",
       "      <td>0.974304</td>\n",
       "      <td>0.9743</td>\n",
       "      <td>0.974288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.984728</td>\n",
       "      <td>0.984700</td>\n",
       "      <td>0.984693</td>\n",
       "      <td>0.974846</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>0.974780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.975071</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>0.974990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.992240</td>\n",
       "      <td>0.992233</td>\n",
       "      <td>0.992234</td>\n",
       "      <td>0.975222</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.975199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997905</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.975441</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.975319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.991063</td>\n",
       "      <td>0.990900</td>\n",
       "      <td>0.990917</td>\n",
       "      <td>0.975801</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.975342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.992828</td>\n",
       "      <td>0.992733</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.976160</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.975718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.993729</td>\n",
       "      <td>0.993617</td>\n",
       "      <td>0.993631</td>\n",
       "      <td>0.976229</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.975939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.987397</td>\n",
       "      <td>0.987367</td>\n",
       "      <td>0.987373</td>\n",
       "      <td>0.976552</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.976502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.990378</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.976579</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.976502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.990632</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.976850</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>0.976790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.988694</td>\n",
       "      <td>0.988617</td>\n",
       "      <td>0.988611</td>\n",
       "      <td>0.977182</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.977077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.989460</td>\n",
       "      <td>0.989433</td>\n",
       "      <td>0.989432</td>\n",
       "      <td>0.977221</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.977087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.997743</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>0.977402</td>\n",
       "      <td>0.9773</td>\n",
       "      <td>0.977295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.989379</td>\n",
       "      <td>0.989333</td>\n",
       "      <td>0.989332</td>\n",
       "      <td>0.977450</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>0.977398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.998951</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.977681</td>\n",
       "      <td>0.9776</td>\n",
       "      <td>0.977599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.994865</td>\n",
       "      <td>0.994833</td>\n",
       "      <td>0.994837</td>\n",
       "      <td>0.977946</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.977805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998685</td>\n",
       "      <td>0.998683</td>\n",
       "      <td>0.998683</td>\n",
       "      <td>0.978574</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.992190</td>\n",
       "      <td>0.992150</td>\n",
       "      <td>0.992154</td>\n",
       "      <td>0.978619</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.994923</td>\n",
       "      <td>0.994917</td>\n",
       "      <td>0.994916</td>\n",
       "      <td>0.978928</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.996767</td>\n",
       "      <td>0.996766</td>\n",
       "      <td>0.978993</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996569</td>\n",
       "      <td>0.996550</td>\n",
       "      <td>0.996550</td>\n",
       "      <td>0.979043</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.993394</td>\n",
       "      <td>0.993383</td>\n",
       "      <td>0.993375</td>\n",
       "      <td>0.979143</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.979075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996723</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.9793</td>\n",
       "      <td>0.979302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993523</td>\n",
       "      <td>0.993433</td>\n",
       "      <td>0.993443</td>\n",
       "      <td>0.979454</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.979239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997328</td>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.979507</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.979402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998968</td>\n",
       "      <td>0.998967</td>\n",
       "      <td>0.998966</td>\n",
       "      <td>0.979568</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>0.979491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.996949</td>\n",
       "      <td>0.996933</td>\n",
       "      <td>0.996934</td>\n",
       "      <td>0.979746</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.997821</td>\n",
       "      <td>0.997817</td>\n",
       "      <td>0.997816</td>\n",
       "      <td>0.980060</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.979995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997705</td>\n",
       "      <td>0.997700</td>\n",
       "      <td>0.997699</td>\n",
       "      <td>0.980113</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.980007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998223</td>\n",
       "      <td>0.998217</td>\n",
       "      <td>0.998216</td>\n",
       "      <td>0.980162</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.980098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.980265</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.980203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997383</td>\n",
       "      <td>0.997367</td>\n",
       "      <td>0.997368</td>\n",
       "      <td>0.980527</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.980319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997439</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.980737</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.980687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.994313</td>\n",
       "      <td>0.994267</td>\n",
       "      <td>0.994272</td>\n",
       "      <td>0.980856</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.980716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.995232</td>\n",
       "      <td>0.995217</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.981063</td>\n",
       "      <td>0.9810</td>\n",
       "      <td>0.980998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.998437</td>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.998434</td>\n",
       "      <td>0.981266</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.981209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998386</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.981538</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998918</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.981551</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996681</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.996664</td>\n",
       "      <td>0.981600</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.981630</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.981598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.981698</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.981697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.997774</td>\n",
       "      <td>0.997767</td>\n",
       "      <td>0.997767</td>\n",
       "      <td>0.981836</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.981805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981919</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>0.981896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.995785</td>\n",
       "      <td>0.995767</td>\n",
       "      <td>0.995770</td>\n",
       "      <td>0.982306</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>0.982212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.982324</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.982302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.982486</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>0.982412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.983372</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.983311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.983702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984105</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986311</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.986302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer  units  epochs  batch_size  train_precision  train_recall   \n",
       "6        1     64       5         256         0.966627      0.966567  \\\n",
       "4        1     64       5         128         0.975075      0.975050   \n",
       "14       1     64      10         256         0.980676      0.980600   \n",
       "30       1    128       5         256         0.975667      0.975567   \n",
       "0        1     64       5          32         0.981986      0.981817   \n",
       "2        1     64       5          64         0.980283      0.980233   \n",
       "10       1     64      10          64         0.990237      0.990183   \n",
       "20       1     64      20         128         0.996405      0.996400   \n",
       "16       1     64      20          32         0.998041      0.998033   \n",
       "28       1    128       5         128         0.984111      0.984083   \n",
       "54       1    256       5         256         0.984728      0.984700   \n",
       "8        1     64      10          32         0.993750      0.993733   \n",
       "22       1     64      20         256         0.992240      0.992233   \n",
       "18       1     64      20          64         0.997905      0.997900   \n",
       "48       1    256       5          32         0.991063      0.990900   \n",
       "96       1   1024       5          32         0.992828      0.992733   \n",
       "34       1    128      10          64         0.993729      0.993617   \n",
       "12       1     64      10         128         0.987397      0.987367   \n",
       "24       1    128       5          32         0.990378      0.990333   \n",
       "38       1    128      10         256         0.990632      0.990617   \n",
       "52       1    256       5         128         0.988694      0.988617   \n",
       "26       1    128       5          64         0.989460      0.989433   \n",
       "46       1    128      20         256         0.997743      0.997733   \n",
       "78       1    512       5         256         0.989379      0.989333   \n",
       "42       1    128      20          64         0.998951      0.998950   \n",
       "62       1    256      10         256         0.994865      0.994833   \n",
       "40       1    128      20          32         0.998685      0.998683   \n",
       "50       1    256       5          64         0.992190      0.992150   \n",
       "36       1    128      10         128         0.994923      0.994917   \n",
       "56       1    256      10          32         0.996774      0.996767   \n",
       "32       1    128      10          32         0.996569      0.996550   \n",
       "102      1   1024       5         256         0.993394      0.993383   \n",
       "80       1    512      10          32         0.996723      0.996717   \n",
       "72       1    512       5          32         0.993523      0.993433   \n",
       "58       1    256      10          64         0.997328      0.997317   \n",
       "44       1    128      20         128         0.998968      0.998967   \n",
       "90       1    512      20          64         0.996949      0.996933   \n",
       "112      1   1024      20          32         0.997821      0.997817   \n",
       "106      1   1024      10          64         0.997705      0.997700   \n",
       "64       1    256      20          32         0.998223      0.998217   \n",
       "76       1    512       5         128         0.993976      0.993967   \n",
       "82       1    512      10          64         0.997383      0.997367   \n",
       "114      1   1024      20          64         0.997439      0.997433   \n",
       "98       1   1024       5          64         0.994313      0.994267   \n",
       "74       1    512       5          64         0.995232      0.995217   \n",
       "86       1    512      10         256         0.998437      0.998433   \n",
       "84       1    512      10         128         0.998386      0.998383   \n",
       "108      1   1024      10         128         0.998918      0.998917   \n",
       "104      1   1024      10          32         0.996681      0.996667   \n",
       "66       1    256      20          64         0.999451      0.999450   \n",
       "68       1    256      20         128         0.999950      0.999950   \n",
       "60       1    256      10         128         0.997774      0.997767   \n",
       "94       1    512      20         256         1.000000      1.000000   \n",
       "100      1   1024       5         128         0.995785      0.995767   \n",
       "70       1    256      20         256         0.999917      0.999917   \n",
       "88       1    512      20          32         0.998551      0.998550   \n",
       "92       1    512      20         128         0.999883      0.999883   \n",
       "110      1   1024      10         256         0.999251      0.999250   \n",
       "118      1   1024      20         256         1.000000      1.000000   \n",
       "116      1   1024      20         128         1.000000      1.000000   \n",
       "\n",
       "     train_f1  test_precision  test_recall   test_f1  \n",
       "6    0.966554        0.960551       0.9604  0.960364  \n",
       "4    0.975023        0.966797       0.9668  0.966768  \n",
       "14   0.980606        0.968335       0.9683  0.968303  \n",
       "30   0.975530        0.968972       0.9688  0.968748  \n",
       "0    0.981786        0.969419       0.9691  0.969042  \n",
       "2    0.980216        0.971597       0.9715  0.971468  \n",
       "10   0.990175        0.973276       0.9732  0.973191  \n",
       "20   0.996400        0.973931       0.9739  0.973900  \n",
       "16   0.998034        0.974205       0.9741  0.974099  \n",
       "28   0.984080        0.974304       0.9743  0.974288  \n",
       "54   0.984693        0.974846       0.9748  0.974780  \n",
       "8    0.993733        0.975071       0.9750  0.974990  \n",
       "22   0.992234        0.975222       0.9752  0.975199  \n",
       "18   0.997900        0.975441       0.9753  0.975319  \n",
       "48   0.990917        0.975801       0.9753  0.975342  \n",
       "96   0.992739        0.976160       0.9757  0.975718  \n",
       "34   0.993631        0.976229       0.9759  0.975939  \n",
       "12   0.987373        0.976552       0.9765  0.976502  \n",
       "24   0.990333        0.976579       0.9765  0.976502  \n",
       "38   0.990617        0.976850       0.9768  0.976790  \n",
       "52   0.988611        0.977182       0.9771  0.977077  \n",
       "26   0.989432        0.977221       0.9771  0.977087  \n",
       "46   0.997733        0.977402       0.9773  0.977295  \n",
       "78   0.989332        0.977450       0.9774  0.977398  \n",
       "42   0.998950        0.977681       0.9776  0.977599  \n",
       "62   0.994837        0.977946       0.9778  0.977805  \n",
       "40   0.998683        0.978574       0.9785  0.978500  \n",
       "50   0.992154        0.978619       0.9785  0.978508  \n",
       "36   0.994916        0.978928       0.9789  0.978896  \n",
       "56   0.996766        0.978993       0.9789  0.978908  \n",
       "32   0.996550        0.979043       0.9789  0.978903  \n",
       "102  0.993375        0.979143       0.9791  0.979075  \n",
       "80   0.996717        0.979352       0.9793  0.979302  \n",
       "72   0.993443        0.979454       0.9792  0.979239  \n",
       "58   0.997317        0.979507       0.9794  0.979402  \n",
       "44   0.998966        0.979568       0.9795  0.979491  \n",
       "90   0.996934        0.979746       0.9796  0.979600  \n",
       "112  0.997816        0.980060       0.9800  0.979995  \n",
       "106  0.997699        0.980113       0.9800  0.980007  \n",
       "64   0.998216        0.980162       0.9801  0.980098  \n",
       "76   0.993967        0.980265       0.9802  0.980203  \n",
       "82   0.997368        0.980527       0.9803  0.980319  \n",
       "114  0.997433        0.980737       0.9807  0.980687  \n",
       "98   0.994272        0.980856       0.9807  0.980716  \n",
       "74   0.995215        0.981063       0.9810  0.980998  \n",
       "86   0.998434        0.981266       0.9812  0.981209  \n",
       "84   0.998383        0.981538       0.9815  0.981493  \n",
       "108  0.998917        0.981551       0.9815  0.981499  \n",
       "104  0.996664        0.981600       0.9815  0.981494  \n",
       "66   0.999450        0.981630       0.9816  0.981598  \n",
       "68   0.999950        0.981698       0.9817  0.981697  \n",
       "60   0.997767        0.981836       0.9818  0.981805  \n",
       "94   1.000000        0.981919       0.9819  0.981896  \n",
       "100  0.995770        0.982306       0.9822  0.982212  \n",
       "70   0.999917        0.982324       0.9823  0.982302  \n",
       "88   0.998550        0.982486       0.9824  0.982412  \n",
       "92   0.999883        0.983372       0.9833  0.983311  \n",
       "110  0.999250        0.983741       0.9837  0.983702  \n",
       "118  1.000000        0.984105       0.9841  0.984100  \n",
       "116  1.000000        0.986311       0.9863  0.986302  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_name = 'group_layer_1'\n",
    "group_df = next(df for name, df in grouped_dfs if name == group_name)\n",
    "# print(group_df)\n",
    "\n",
    "sorted_group_df = group_df[['layer', 'units', 'epochs', 'batch_size', 'train_precision', 'train_recall', 'train_f1', 'test_precision', 'test_recall', 'test_f1']]\n",
    "sorted_group_df = sorted_group_df.sort_values(by='test_precision', ascending=True)\n",
    "sorted_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce7d14e9-fa81-440d-9c59-1eed0b133c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986311</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.986302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984105</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.999250</td>\n",
       "      <td>0.983741</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.983702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.983372</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.983311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.998550</td>\n",
       "      <td>0.982486</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>0.982412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.982324</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.982302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.995785</td>\n",
       "      <td>0.995767</td>\n",
       "      <td>0.995770</td>\n",
       "      <td>0.982306</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>0.982212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981919</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>0.981896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.997774</td>\n",
       "      <td>0.997767</td>\n",
       "      <td>0.997767</td>\n",
       "      <td>0.981836</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.981805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.981698</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.981697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.981630</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.981598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996681</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.996664</td>\n",
       "      <td>0.981600</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998918</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.981551</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998386</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.981538</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.981493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.998437</td>\n",
       "      <td>0.998433</td>\n",
       "      <td>0.998434</td>\n",
       "      <td>0.981266</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.981209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.995232</td>\n",
       "      <td>0.995217</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.981063</td>\n",
       "      <td>0.9810</td>\n",
       "      <td>0.980998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.994313</td>\n",
       "      <td>0.994267</td>\n",
       "      <td>0.994272</td>\n",
       "      <td>0.980856</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.980716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997439</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.980737</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.980687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997383</td>\n",
       "      <td>0.997367</td>\n",
       "      <td>0.997368</td>\n",
       "      <td>0.980527</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.980319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.993967</td>\n",
       "      <td>0.980265</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>0.980203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998223</td>\n",
       "      <td>0.998217</td>\n",
       "      <td>0.998216</td>\n",
       "      <td>0.980162</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.980098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997705</td>\n",
       "      <td>0.997700</td>\n",
       "      <td>0.997699</td>\n",
       "      <td>0.980113</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.980007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.997821</td>\n",
       "      <td>0.997817</td>\n",
       "      <td>0.997816</td>\n",
       "      <td>0.980060</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.979995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.996949</td>\n",
       "      <td>0.996933</td>\n",
       "      <td>0.996934</td>\n",
       "      <td>0.979746</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998968</td>\n",
       "      <td>0.998967</td>\n",
       "      <td>0.998966</td>\n",
       "      <td>0.979568</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>0.979491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997328</td>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.997317</td>\n",
       "      <td>0.979507</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.979402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993523</td>\n",
       "      <td>0.993433</td>\n",
       "      <td>0.993443</td>\n",
       "      <td>0.979454</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.979239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996723</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.9793</td>\n",
       "      <td>0.979302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.993394</td>\n",
       "      <td>0.993383</td>\n",
       "      <td>0.993375</td>\n",
       "      <td>0.979143</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.979075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996569</td>\n",
       "      <td>0.996550</td>\n",
       "      <td>0.996550</td>\n",
       "      <td>0.979043</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996774</td>\n",
       "      <td>0.996767</td>\n",
       "      <td>0.996766</td>\n",
       "      <td>0.978993</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.994923</td>\n",
       "      <td>0.994917</td>\n",
       "      <td>0.994916</td>\n",
       "      <td>0.978928</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.992190</td>\n",
       "      <td>0.992150</td>\n",
       "      <td>0.992154</td>\n",
       "      <td>0.978619</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998685</td>\n",
       "      <td>0.998683</td>\n",
       "      <td>0.998683</td>\n",
       "      <td>0.978574</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.994865</td>\n",
       "      <td>0.994833</td>\n",
       "      <td>0.994837</td>\n",
       "      <td>0.977946</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.977805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.998951</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.977681</td>\n",
       "      <td>0.9776</td>\n",
       "      <td>0.977599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.989379</td>\n",
       "      <td>0.989333</td>\n",
       "      <td>0.989332</td>\n",
       "      <td>0.977450</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>0.977398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.997743</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>0.997733</td>\n",
       "      <td>0.977402</td>\n",
       "      <td>0.9773</td>\n",
       "      <td>0.977295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.989460</td>\n",
       "      <td>0.989433</td>\n",
       "      <td>0.989432</td>\n",
       "      <td>0.977221</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.977087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.988694</td>\n",
       "      <td>0.988617</td>\n",
       "      <td>0.988611</td>\n",
       "      <td>0.977182</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.977077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.990632</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.976850</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>0.976790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.990378</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.976579</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.976502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.987397</td>\n",
       "      <td>0.987367</td>\n",
       "      <td>0.987373</td>\n",
       "      <td>0.976552</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.976502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.993729</td>\n",
       "      <td>0.993617</td>\n",
       "      <td>0.993631</td>\n",
       "      <td>0.976229</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.975939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.992828</td>\n",
       "      <td>0.992733</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.976160</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.975718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.991063</td>\n",
       "      <td>0.990900</td>\n",
       "      <td>0.990917</td>\n",
       "      <td>0.975801</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.975342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997905</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.975441</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.975319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.992240</td>\n",
       "      <td>0.992233</td>\n",
       "      <td>0.992234</td>\n",
       "      <td>0.975222</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.975199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.975071</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>0.974990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.984728</td>\n",
       "      <td>0.984700</td>\n",
       "      <td>0.984693</td>\n",
       "      <td>0.974846</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>0.974780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.984111</td>\n",
       "      <td>0.984083</td>\n",
       "      <td>0.984080</td>\n",
       "      <td>0.974304</td>\n",
       "      <td>0.9743</td>\n",
       "      <td>0.974288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998041</td>\n",
       "      <td>0.998033</td>\n",
       "      <td>0.998034</td>\n",
       "      <td>0.974205</td>\n",
       "      <td>0.9741</td>\n",
       "      <td>0.974099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.996405</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.973931</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.990237</td>\n",
       "      <td>0.990183</td>\n",
       "      <td>0.990175</td>\n",
       "      <td>0.973276</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.973191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.980283</td>\n",
       "      <td>0.980233</td>\n",
       "      <td>0.980216</td>\n",
       "      <td>0.971597</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.971468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.981986</td>\n",
       "      <td>0.981817</td>\n",
       "      <td>0.981786</td>\n",
       "      <td>0.969419</td>\n",
       "      <td>0.9691</td>\n",
       "      <td>0.969042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.975667</td>\n",
       "      <td>0.975567</td>\n",
       "      <td>0.975530</td>\n",
       "      <td>0.968972</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.968748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.980676</td>\n",
       "      <td>0.980600</td>\n",
       "      <td>0.980606</td>\n",
       "      <td>0.968335</td>\n",
       "      <td>0.9683</td>\n",
       "      <td>0.968303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.975075</td>\n",
       "      <td>0.975050</td>\n",
       "      <td>0.975023</td>\n",
       "      <td>0.966797</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.966768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>0.966567</td>\n",
       "      <td>0.966554</td>\n",
       "      <td>0.960551</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.960364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer  units  epochs  batch_size  train_precision  train_recall   \n",
       "116      1   1024      20         128         1.000000      1.000000  \\\n",
       "118      1   1024      20         256         1.000000      1.000000   \n",
       "110      1   1024      10         256         0.999251      0.999250   \n",
       "92       1    512      20         128         0.999883      0.999883   \n",
       "88       1    512      20          32         0.998551      0.998550   \n",
       "70       1    256      20         256         0.999917      0.999917   \n",
       "100      1   1024       5         128         0.995785      0.995767   \n",
       "94       1    512      20         256         1.000000      1.000000   \n",
       "60       1    256      10         128         0.997774      0.997767   \n",
       "68       1    256      20         128         0.999950      0.999950   \n",
       "66       1    256      20          64         0.999451      0.999450   \n",
       "104      1   1024      10          32         0.996681      0.996667   \n",
       "108      1   1024      10         128         0.998918      0.998917   \n",
       "84       1    512      10         128         0.998386      0.998383   \n",
       "86       1    512      10         256         0.998437      0.998433   \n",
       "74       1    512       5          64         0.995232      0.995217   \n",
       "98       1   1024       5          64         0.994313      0.994267   \n",
       "114      1   1024      20          64         0.997439      0.997433   \n",
       "82       1    512      10          64         0.997383      0.997367   \n",
       "76       1    512       5         128         0.993976      0.993967   \n",
       "64       1    256      20          32         0.998223      0.998217   \n",
       "106      1   1024      10          64         0.997705      0.997700   \n",
       "112      1   1024      20          32         0.997821      0.997817   \n",
       "90       1    512      20          64         0.996949      0.996933   \n",
       "44       1    128      20         128         0.998968      0.998967   \n",
       "58       1    256      10          64         0.997328      0.997317   \n",
       "72       1    512       5          32         0.993523      0.993433   \n",
       "80       1    512      10          32         0.996723      0.996717   \n",
       "102      1   1024       5         256         0.993394      0.993383   \n",
       "32       1    128      10          32         0.996569      0.996550   \n",
       "56       1    256      10          32         0.996774      0.996767   \n",
       "36       1    128      10         128         0.994923      0.994917   \n",
       "50       1    256       5          64         0.992190      0.992150   \n",
       "40       1    128      20          32         0.998685      0.998683   \n",
       "62       1    256      10         256         0.994865      0.994833   \n",
       "42       1    128      20          64         0.998951      0.998950   \n",
       "78       1    512       5         256         0.989379      0.989333   \n",
       "46       1    128      20         256         0.997743      0.997733   \n",
       "26       1    128       5          64         0.989460      0.989433   \n",
       "52       1    256       5         128         0.988694      0.988617   \n",
       "38       1    128      10         256         0.990632      0.990617   \n",
       "24       1    128       5          32         0.990378      0.990333   \n",
       "12       1     64      10         128         0.987397      0.987367   \n",
       "34       1    128      10          64         0.993729      0.993617   \n",
       "96       1   1024       5          32         0.992828      0.992733   \n",
       "48       1    256       5          32         0.991063      0.990900   \n",
       "18       1     64      20          64         0.997905      0.997900   \n",
       "22       1     64      20         256         0.992240      0.992233   \n",
       "8        1     64      10          32         0.993750      0.993733   \n",
       "54       1    256       5         256         0.984728      0.984700   \n",
       "28       1    128       5         128         0.984111      0.984083   \n",
       "16       1     64      20          32         0.998041      0.998033   \n",
       "20       1     64      20         128         0.996405      0.996400   \n",
       "10       1     64      10          64         0.990237      0.990183   \n",
       "2        1     64       5          64         0.980283      0.980233   \n",
       "0        1     64       5          32         0.981986      0.981817   \n",
       "30       1    128       5         256         0.975667      0.975567   \n",
       "14       1     64      10         256         0.980676      0.980600   \n",
       "4        1     64       5         128         0.975075      0.975050   \n",
       "6        1     64       5         256         0.966627      0.966567   \n",
       "\n",
       "     train_f1  test_precision  test_recall   test_f1  \n",
       "116  1.000000        0.986311       0.9863  0.986302  \n",
       "118  1.000000        0.984105       0.9841  0.984100  \n",
       "110  0.999250        0.983741       0.9837  0.983702  \n",
       "92   0.999883        0.983372       0.9833  0.983311  \n",
       "88   0.998550        0.982486       0.9824  0.982412  \n",
       "70   0.999917        0.982324       0.9823  0.982302  \n",
       "100  0.995770        0.982306       0.9822  0.982212  \n",
       "94   1.000000        0.981919       0.9819  0.981896  \n",
       "60   0.997767        0.981836       0.9818  0.981805  \n",
       "68   0.999950        0.981698       0.9817  0.981697  \n",
       "66   0.999450        0.981630       0.9816  0.981598  \n",
       "104  0.996664        0.981600       0.9815  0.981494  \n",
       "108  0.998917        0.981551       0.9815  0.981499  \n",
       "84   0.998383        0.981538       0.9815  0.981493  \n",
       "86   0.998434        0.981266       0.9812  0.981209  \n",
       "74   0.995215        0.981063       0.9810  0.980998  \n",
       "98   0.994272        0.980856       0.9807  0.980716  \n",
       "114  0.997433        0.980737       0.9807  0.980687  \n",
       "82   0.997368        0.980527       0.9803  0.980319  \n",
       "76   0.993967        0.980265       0.9802  0.980203  \n",
       "64   0.998216        0.980162       0.9801  0.980098  \n",
       "106  0.997699        0.980113       0.9800  0.980007  \n",
       "112  0.997816        0.980060       0.9800  0.979995  \n",
       "90   0.996934        0.979746       0.9796  0.979600  \n",
       "44   0.998966        0.979568       0.9795  0.979491  \n",
       "58   0.997317        0.979507       0.9794  0.979402  \n",
       "72   0.993443        0.979454       0.9792  0.979239  \n",
       "80   0.996717        0.979352       0.9793  0.979302  \n",
       "102  0.993375        0.979143       0.9791  0.979075  \n",
       "32   0.996550        0.979043       0.9789  0.978903  \n",
       "56   0.996766        0.978993       0.9789  0.978908  \n",
       "36   0.994916        0.978928       0.9789  0.978896  \n",
       "50   0.992154        0.978619       0.9785  0.978508  \n",
       "40   0.998683        0.978574       0.9785  0.978500  \n",
       "62   0.994837        0.977946       0.9778  0.977805  \n",
       "42   0.998950        0.977681       0.9776  0.977599  \n",
       "78   0.989332        0.977450       0.9774  0.977398  \n",
       "46   0.997733        0.977402       0.9773  0.977295  \n",
       "26   0.989432        0.977221       0.9771  0.977087  \n",
       "52   0.988611        0.977182       0.9771  0.977077  \n",
       "38   0.990617        0.976850       0.9768  0.976790  \n",
       "24   0.990333        0.976579       0.9765  0.976502  \n",
       "12   0.987373        0.976552       0.9765  0.976502  \n",
       "34   0.993631        0.976229       0.9759  0.975939  \n",
       "96   0.992739        0.976160       0.9757  0.975718  \n",
       "48   0.990917        0.975801       0.9753  0.975342  \n",
       "18   0.997900        0.975441       0.9753  0.975319  \n",
       "22   0.992234        0.975222       0.9752  0.975199  \n",
       "8    0.993733        0.975071       0.9750  0.974990  \n",
       "54   0.984693        0.974846       0.9748  0.974780  \n",
       "28   0.984080        0.974304       0.9743  0.974288  \n",
       "16   0.998034        0.974205       0.9741  0.974099  \n",
       "20   0.996400        0.973931       0.9739  0.973900  \n",
       "10   0.990175        0.973276       0.9732  0.973191  \n",
       "2    0.980216        0.971597       0.9715  0.971468  \n",
       "0    0.981786        0.969419       0.9691  0.969042  \n",
       "30   0.975530        0.968972       0.9688  0.968748  \n",
       "14   0.980606        0.968335       0.9683  0.968303  \n",
       "4    0.975023        0.966797       0.9668  0.966768  \n",
       "6    0.966554        0.960551       0.9604  0.960364  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_group_df = sorted_group_df.sort_values(by=['test_precision', 'test_recall', 'test_f1'], ascending=[False, False, False])\n",
    "sorted_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38d33814-902c-479f-85b6-59f5bc7010d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.976707</td>\n",
       "      <td>0.976650</td>\n",
       "      <td>0.976657</td>\n",
       "      <td>0.966189</td>\n",
       "      <td>0.9661</td>\n",
       "      <td>0.966089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.984725</td>\n",
       "      <td>0.984583</td>\n",
       "      <td>0.984576</td>\n",
       "      <td>0.969357</td>\n",
       "      <td>0.9690</td>\n",
       "      <td>0.968984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.980005</td>\n",
       "      <td>0.979900</td>\n",
       "      <td>0.979900</td>\n",
       "      <td>0.970086</td>\n",
       "      <td>0.9697</td>\n",
       "      <td>0.969706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.983590</td>\n",
       "      <td>0.983433</td>\n",
       "      <td>0.983439</td>\n",
       "      <td>0.972585</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>0.972431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.993743</td>\n",
       "      <td>0.993667</td>\n",
       "      <td>0.993676</td>\n",
       "      <td>0.972617</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.972244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.986369</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>0.972981</td>\n",
       "      <td>0.9729</td>\n",
       "      <td>0.972906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.988885</td>\n",
       "      <td>0.988833</td>\n",
       "      <td>0.988830</td>\n",
       "      <td>0.973042</td>\n",
       "      <td>0.9728</td>\n",
       "      <td>0.972775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.994705</td>\n",
       "      <td>0.994650</td>\n",
       "      <td>0.994649</td>\n",
       "      <td>0.973303</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.972991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.990678</td>\n",
       "      <td>0.990633</td>\n",
       "      <td>0.990638</td>\n",
       "      <td>0.973809</td>\n",
       "      <td>0.9737</td>\n",
       "      <td>0.973693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.990401</td>\n",
       "      <td>0.990333</td>\n",
       "      <td>0.990327</td>\n",
       "      <td>0.973921</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.973805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.992353</td>\n",
       "      <td>0.992300</td>\n",
       "      <td>0.992297</td>\n",
       "      <td>0.973942</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.973806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998154</td>\n",
       "      <td>0.998150</td>\n",
       "      <td>0.998150</td>\n",
       "      <td>0.974036</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>0.973997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.991353</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.991162</td>\n",
       "      <td>0.974064</td>\n",
       "      <td>0.9737</td>\n",
       "      <td>0.973692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.987762</td>\n",
       "      <td>0.987533</td>\n",
       "      <td>0.987531</td>\n",
       "      <td>0.975100</td>\n",
       "      <td>0.9746</td>\n",
       "      <td>0.974617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.985351</td>\n",
       "      <td>0.985283</td>\n",
       "      <td>0.985291</td>\n",
       "      <td>0.975260</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.975196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.997138</td>\n",
       "      <td>0.997117</td>\n",
       "      <td>0.997119</td>\n",
       "      <td>0.975302</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.975199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.993953</td>\n",
       "      <td>0.993883</td>\n",
       "      <td>0.993888</td>\n",
       "      <td>0.975599</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>0.975309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.990841</td>\n",
       "      <td>0.990800</td>\n",
       "      <td>0.990804</td>\n",
       "      <td>0.975956</td>\n",
       "      <td>0.9758</td>\n",
       "      <td>0.975794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.996369</td>\n",
       "      <td>0.996350</td>\n",
       "      <td>0.996347</td>\n",
       "      <td>0.976201</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.976109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.989795</td>\n",
       "      <td>0.989717</td>\n",
       "      <td>0.989730</td>\n",
       "      <td>0.976381</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.976226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.992101</td>\n",
       "      <td>0.991850</td>\n",
       "      <td>0.991835</td>\n",
       "      <td>0.976997</td>\n",
       "      <td>0.9763</td>\n",
       "      <td>0.976285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.995414</td>\n",
       "      <td>0.995400</td>\n",
       "      <td>0.995399</td>\n",
       "      <td>0.977025</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.976896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.990970</td>\n",
       "      <td>0.990917</td>\n",
       "      <td>0.990926</td>\n",
       "      <td>0.977477</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>0.977255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.995115</td>\n",
       "      <td>0.995100</td>\n",
       "      <td>0.995101</td>\n",
       "      <td>0.977797</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>0.977702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.994861</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>0.994800</td>\n",
       "      <td>0.977861</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>0.977707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.993964</td>\n",
       "      <td>0.993933</td>\n",
       "      <td>0.993937</td>\n",
       "      <td>0.977913</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.977807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.992385</td>\n",
       "      <td>0.992333</td>\n",
       "      <td>0.992333</td>\n",
       "      <td>0.978001</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.977790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.992655</td>\n",
       "      <td>0.992600</td>\n",
       "      <td>0.992604</td>\n",
       "      <td>0.978162</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.977923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.998404</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.978420</td>\n",
       "      <td>0.9783</td>\n",
       "      <td>0.978296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.991948</td>\n",
       "      <td>0.991867</td>\n",
       "      <td>0.991867</td>\n",
       "      <td>0.978545</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>0.978403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998353</td>\n",
       "      <td>0.998350</td>\n",
       "      <td>0.998350</td>\n",
       "      <td>0.978549</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.997230</td>\n",
       "      <td>0.997217</td>\n",
       "      <td>0.997217</td>\n",
       "      <td>0.978574</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>0.978401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.992065</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.991993</td>\n",
       "      <td>0.978664</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>0.978492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.996045</td>\n",
       "      <td>0.996033</td>\n",
       "      <td>0.996034</td>\n",
       "      <td>0.978692</td>\n",
       "      <td>0.9786</td>\n",
       "      <td>0.978602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.996132</td>\n",
       "      <td>0.996117</td>\n",
       "      <td>0.996116</td>\n",
       "      <td>0.978732</td>\n",
       "      <td>0.9786</td>\n",
       "      <td>0.978597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.991495</td>\n",
       "      <td>0.991483</td>\n",
       "      <td>0.991482</td>\n",
       "      <td>0.979023</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>0.978991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.997870</td>\n",
       "      <td>0.997867</td>\n",
       "      <td>0.997867</td>\n",
       "      <td>0.979030</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>0.978995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.995636</td>\n",
       "      <td>0.995617</td>\n",
       "      <td>0.995614</td>\n",
       "      <td>0.979224</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.979077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.996185</td>\n",
       "      <td>0.996167</td>\n",
       "      <td>0.996166</td>\n",
       "      <td>0.979547</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.979399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.995340</td>\n",
       "      <td>0.995333</td>\n",
       "      <td>0.995332</td>\n",
       "      <td>0.979714</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.979695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997506</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>0.980153</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.980101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.993721</td>\n",
       "      <td>0.993700</td>\n",
       "      <td>0.993696</td>\n",
       "      <td>0.980363</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.980279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996348</td>\n",
       "      <td>0.996333</td>\n",
       "      <td>0.996333</td>\n",
       "      <td>0.980542</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.980401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997157</td>\n",
       "      <td>0.997150</td>\n",
       "      <td>0.997151</td>\n",
       "      <td>0.980694</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.980608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.997443</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.997434</td>\n",
       "      <td>0.980738</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.980617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.998569</td>\n",
       "      <td>0.998567</td>\n",
       "      <td>0.998566</td>\n",
       "      <td>0.980778</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.994518</td>\n",
       "      <td>0.994467</td>\n",
       "      <td>0.994458</td>\n",
       "      <td>0.980996</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.980859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.997720</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.981084</td>\n",
       "      <td>0.9810</td>\n",
       "      <td>0.981006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.995156</td>\n",
       "      <td>0.995133</td>\n",
       "      <td>0.995135</td>\n",
       "      <td>0.981142</td>\n",
       "      <td>0.9810</td>\n",
       "      <td>0.981012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.995071</td>\n",
       "      <td>0.995067</td>\n",
       "      <td>0.995065</td>\n",
       "      <td>0.981277</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.981196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997574</td>\n",
       "      <td>0.997567</td>\n",
       "      <td>0.997567</td>\n",
       "      <td>0.981326</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.981222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.994362</td>\n",
       "      <td>0.994350</td>\n",
       "      <td>0.994350</td>\n",
       "      <td>0.981348</td>\n",
       "      <td>0.9813</td>\n",
       "      <td>0.981304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998718</td>\n",
       "      <td>0.998717</td>\n",
       "      <td>0.998717</td>\n",
       "      <td>0.981659</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.981602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.997856</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.997851</td>\n",
       "      <td>0.981763</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.981714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.999050</td>\n",
       "      <td>0.999050</td>\n",
       "      <td>0.999050</td>\n",
       "      <td>0.981798</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.981793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.998650</td>\n",
       "      <td>0.998651</td>\n",
       "      <td>0.981873</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.981718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.998453</td>\n",
       "      <td>0.998450</td>\n",
       "      <td>0.998450</td>\n",
       "      <td>0.982881</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>0.982806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.997969</td>\n",
       "      <td>0.997967</td>\n",
       "      <td>0.997967</td>\n",
       "      <td>0.983139</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>0.983106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998385</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.983436</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.983399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.998252</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.998250</td>\n",
       "      <td>0.984234</td>\n",
       "      <td>0.9842</td>\n",
       "      <td>0.984201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer  units  epochs  batch_size  train_precision  train_recall   \n",
       "126      2     64       5         256         0.976707      0.976650  \\\n",
       "134      2     64      10         256         0.984725      0.984583   \n",
       "124      2     64       5         128         0.980005      0.979900   \n",
       "122      2     64       5          64         0.983590      0.983433   \n",
       "154      2    128      10          64         0.993743      0.993667   \n",
       "150      2    128       5         256         0.986369      0.986333   \n",
       "148      2    128       5         128         0.988885      0.988833   \n",
       "138      2     64      20          64         0.994705      0.994650   \n",
       "132      2     64      10         128         0.990678      0.990633   \n",
       "130      2     64      10          64         0.990401      0.990333   \n",
       "128      2     64      10          32         0.992353      0.992300   \n",
       "136      2     64      20          32         0.998154      0.998150   \n",
       "200      2    512      10          32         0.991353      0.991150   \n",
       "216      2   1024       5          32         0.987762      0.987533   \n",
       "120      2     64       5          32         0.985351      0.985283   \n",
       "140      2     64      20         128         0.997138      0.997117   \n",
       "180      2    256      10         128         0.993953      0.993883   \n",
       "144      2    128       5          32         0.990841      0.990800   \n",
       "142      2     64      20         256         0.996369      0.996350   \n",
       "146      2    128       5          64         0.989795      0.989717   \n",
       "226      2   1024      10          64         0.992101      0.991850   \n",
       "152      2    128      10          32         0.995414      0.995400   \n",
       "192      2    512       5          32         0.990970      0.990917   \n",
       "158      2    128      10         256         0.995115      0.995100   \n",
       "162      2    128      20          64         0.994861      0.994800   \n",
       "172      2    256       5         128         0.993964      0.993933   \n",
       "218      2   1024       5          64         0.992385      0.992333   \n",
       "168      2    256       5          32         0.992655      0.992600   \n",
       "166      2    128      20         256         0.998404      0.998400   \n",
       "170      2    256       5          64         0.991948      0.991867   \n",
       "164      2    128      20         128         0.998353      0.998350   \n",
       "206      2    512      10         256         0.997230      0.997217   \n",
       "198      2    512       5         256         0.992065      0.992000   \n",
       "202      2    512      10          64         0.996045      0.996033   \n",
       "156      2    128      10         128         0.996132      0.996117   \n",
       "174      2    256       5         256         0.991495      0.991483   \n",
       "190      2    256      20         256         0.997870      0.997867   \n",
       "224      2   1024      10          32         0.995636      0.995617   \n",
       "178      2    256      10          64         0.996185      0.996167   \n",
       "176      2    256      10          32         0.995340      0.995333   \n",
       "186      2    256      20          64         0.997506      0.997500   \n",
       "196      2    512       5         128         0.993721      0.993700   \n",
       "160      2    128      20          32         0.996348      0.996333   \n",
       "234      2   1024      20          64         0.997157      0.997150   \n",
       "184      2    256      20          32         0.997443      0.997433   \n",
       "214      2    512      20         256         0.998569      0.998567   \n",
       "228      2   1024      10         128         0.994518      0.994467   \n",
       "208      2    512      20          32         0.997720      0.997717   \n",
       "222      2   1024       5         256         0.995156      0.995133   \n",
       "194      2    512       5          64         0.995071      0.995067   \n",
       "210      2    512      20          64         0.997574      0.997567   \n",
       "220      2   1024       5         128         0.994362      0.994350   \n",
       "188      2    256      20         128         0.998718      0.998717   \n",
       "236      2   1024      20         128         0.997856      0.997850   \n",
       "182      2    256      10         256         0.999050      0.999050   \n",
       "212      2    512      20         128         0.998656      0.998650   \n",
       "238      2   1024      20         256         0.998453      0.998450   \n",
       "204      2    512      10         128         0.997969      0.997967   \n",
       "232      2   1024      20          32         0.998385      0.998383   \n",
       "230      2   1024      10         256         0.998252      0.998250   \n",
       "\n",
       "     train_f1  test_precision  test_recall   test_f1  \n",
       "126  0.976657        0.966189       0.9661  0.966089  \n",
       "134  0.984576        0.969357       0.9690  0.968984  \n",
       "124  0.979900        0.970086       0.9697  0.969706  \n",
       "122  0.983439        0.972585       0.9724  0.972431  \n",
       "154  0.993676        0.972617       0.9722  0.972244  \n",
       "150  0.986333        0.972981       0.9729  0.972906  \n",
       "148  0.988830        0.973042       0.9728  0.972775  \n",
       "138  0.994649        0.973303       0.9730  0.972991  \n",
       "132  0.990638        0.973809       0.9737  0.973693  \n",
       "130  0.990327        0.973921       0.9738  0.973805  \n",
       "128  0.992297        0.973942       0.9738  0.973806  \n",
       "136  0.998150        0.974036       0.9740  0.973997  \n",
       "200  0.991162        0.974064       0.9737  0.973692  \n",
       "216  0.987531        0.975100       0.9746  0.974617  \n",
       "120  0.985291        0.975260       0.9752  0.975196  \n",
       "140  0.997119        0.975302       0.9752  0.975199  \n",
       "180  0.993888        0.975599       0.9753  0.975309  \n",
       "144  0.990804        0.975956       0.9758  0.975794  \n",
       "142  0.996347        0.976201       0.9761  0.976109  \n",
       "146  0.989730        0.976381       0.9762  0.976226  \n",
       "226  0.991835        0.976997       0.9763  0.976285  \n",
       "152  0.995399        0.977025       0.9769  0.976896  \n",
       "192  0.990926        0.977477       0.9772  0.977255  \n",
       "158  0.995101        0.977797       0.9777  0.977702  \n",
       "162  0.994800        0.977861       0.9777  0.977707  \n",
       "172  0.993937        0.977913       0.9778  0.977807  \n",
       "218  0.992333        0.978001       0.9778  0.977790  \n",
       "168  0.992604        0.978162       0.9779  0.977923  \n",
       "166  0.998400        0.978420       0.9783  0.978296  \n",
       "170  0.991867        0.978545       0.9784  0.978403  \n",
       "164  0.998350        0.978549       0.9785  0.978499  \n",
       "206  0.997217        0.978574       0.9784  0.978401  \n",
       "198  0.991993        0.978664       0.9785  0.978492  \n",
       "202  0.996034        0.978692       0.9786  0.978602  \n",
       "156  0.996116        0.978732       0.9786  0.978597  \n",
       "174  0.991482        0.979023       0.9790  0.978991  \n",
       "190  0.997867        0.979030       0.9790  0.978995  \n",
       "224  0.995614        0.979224       0.9791  0.979077  \n",
       "178  0.996166        0.979547       0.9794  0.979399  \n",
       "176  0.995332        0.979714       0.9797  0.979695  \n",
       "186  0.997500        0.980153       0.9801  0.980101  \n",
       "196  0.993696        0.980363       0.9803  0.980279  \n",
       "160  0.996333        0.980542       0.9804  0.980401  \n",
       "234  0.997151        0.980694       0.9806  0.980608  \n",
       "184  0.997434        0.980738       0.9806  0.980617  \n",
       "214  0.998566        0.980778       0.9807  0.980700  \n",
       "228  0.994458        0.980996       0.9809  0.980859  \n",
       "208  0.997717        0.981084       0.9810  0.981006  \n",
       "222  0.995135        0.981142       0.9810  0.981012  \n",
       "194  0.995065        0.981277       0.9812  0.981196  \n",
       "210  0.997567        0.981326       0.9812  0.981222  \n",
       "220  0.994350        0.981348       0.9813  0.981304  \n",
       "188  0.998717        0.981659       0.9816  0.981602  \n",
       "236  0.997851        0.981763       0.9817  0.981714  \n",
       "182  0.999050        0.981798       0.9818  0.981793  \n",
       "212  0.998651        0.981873       0.9817  0.981718  \n",
       "238  0.998450        0.982881       0.9828  0.982806  \n",
       "204  0.997967        0.983139       0.9831  0.983106  \n",
       "232  0.998383        0.983436       0.9834  0.983399  \n",
       "230  0.998250        0.984234       0.9842  0.984201  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_name = 'group_layer_2'\n",
    "group_df = next(df for name, df in grouped_dfs if name == group_name)\n",
    "# print(group_df)\n",
    "\n",
    "sorted_group_df = group_df[['layer', 'units', 'epochs', 'batch_size', 'train_precision', 'train_recall', 'train_f1', 'test_precision', 'test_recall', 'test_f1']]\n",
    "sorted_group_df = sorted_group_df.sort_values(by=['test_precision', 'test_recall', 'test_f1'], ascending=[True, True, True])\n",
    "sorted_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dfd9f338-4af0-46f4-8bd4-4fecd888dac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>units</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.975845</td>\n",
       "      <td>0.975733</td>\n",
       "      <td>0.975736</td>\n",
       "      <td>0.968857</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.968795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.982197</td>\n",
       "      <td>0.982083</td>\n",
       "      <td>0.982062</td>\n",
       "      <td>0.969649</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>0.969559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.984460</td>\n",
       "      <td>0.984033</td>\n",
       "      <td>0.983967</td>\n",
       "      <td>0.970105</td>\n",
       "      <td>0.9693</td>\n",
       "      <td>0.969175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.984612</td>\n",
       "      <td>0.984533</td>\n",
       "      <td>0.984531</td>\n",
       "      <td>0.970173</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.969995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.983383</td>\n",
       "      <td>0.983017</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.970756</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.970133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.991874</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.991786</td>\n",
       "      <td>0.971151</td>\n",
       "      <td>0.9708</td>\n",
       "      <td>0.970778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.989634</td>\n",
       "      <td>0.989517</td>\n",
       "      <td>0.989513</td>\n",
       "      <td>0.972017</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>0.971755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.986105</td>\n",
       "      <td>0.986067</td>\n",
       "      <td>0.986056</td>\n",
       "      <td>0.972700</td>\n",
       "      <td>0.9725</td>\n",
       "      <td>0.972479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.986932</td>\n",
       "      <td>0.986867</td>\n",
       "      <td>0.986854</td>\n",
       "      <td>0.972807</td>\n",
       "      <td>0.9727</td>\n",
       "      <td>0.972690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.992082</td>\n",
       "      <td>0.992067</td>\n",
       "      <td>0.992065</td>\n",
       "      <td>0.973765</td>\n",
       "      <td>0.9737</td>\n",
       "      <td>0.973687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.996805</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>0.996799</td>\n",
       "      <td>0.973872</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.973797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.994906</td>\n",
       "      <td>0.994883</td>\n",
       "      <td>0.994881</td>\n",
       "      <td>0.973937</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.973800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.990186</td>\n",
       "      <td>0.990050</td>\n",
       "      <td>0.990067</td>\n",
       "      <td>0.974188</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.973850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.992471</td>\n",
       "      <td>0.992417</td>\n",
       "      <td>0.992412</td>\n",
       "      <td>0.974614</td>\n",
       "      <td>0.9745</td>\n",
       "      <td>0.974472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.990137</td>\n",
       "      <td>0.990083</td>\n",
       "      <td>0.990091</td>\n",
       "      <td>0.975351</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>0.975204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.990141</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.990094</td>\n",
       "      <td>0.975473</td>\n",
       "      <td>0.9754</td>\n",
       "      <td>0.975390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993582</td>\n",
       "      <td>0.993550</td>\n",
       "      <td>0.993540</td>\n",
       "      <td>0.975758</td>\n",
       "      <td>0.9756</td>\n",
       "      <td>0.975599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.991527</td>\n",
       "      <td>0.991483</td>\n",
       "      <td>0.991475</td>\n",
       "      <td>0.975838</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.975688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.996313</td>\n",
       "      <td>0.996300</td>\n",
       "      <td>0.996299</td>\n",
       "      <td>0.976251</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.975999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.990638</td>\n",
       "      <td>0.990467</td>\n",
       "      <td>0.990488</td>\n",
       "      <td>0.976606</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>0.976229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.989628</td>\n",
       "      <td>0.989550</td>\n",
       "      <td>0.989548</td>\n",
       "      <td>0.976815</td>\n",
       "      <td>0.9766</td>\n",
       "      <td>0.976602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.993881</td>\n",
       "      <td>0.993867</td>\n",
       "      <td>0.993863</td>\n",
       "      <td>0.976872</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>0.976786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.990148</td>\n",
       "      <td>0.989983</td>\n",
       "      <td>0.990002</td>\n",
       "      <td>0.976978</td>\n",
       "      <td>0.9767</td>\n",
       "      <td>0.976718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.991868</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.991805</td>\n",
       "      <td>0.976987</td>\n",
       "      <td>0.9766</td>\n",
       "      <td>0.976644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.990005</td>\n",
       "      <td>0.989767</td>\n",
       "      <td>0.989782</td>\n",
       "      <td>0.977452</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>0.977118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.989657</td>\n",
       "      <td>0.989633</td>\n",
       "      <td>0.989627</td>\n",
       "      <td>0.977549</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>0.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.990304</td>\n",
       "      <td>0.990267</td>\n",
       "      <td>0.990266</td>\n",
       "      <td>0.977669</td>\n",
       "      <td>0.9776</td>\n",
       "      <td>0.977597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996648</td>\n",
       "      <td>0.996617</td>\n",
       "      <td>0.996619</td>\n",
       "      <td>0.978499</td>\n",
       "      <td>0.9783</td>\n",
       "      <td>0.978302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.997012</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.997001</td>\n",
       "      <td>0.978558</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>0.978406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.993940</td>\n",
       "      <td>0.993817</td>\n",
       "      <td>0.993829</td>\n",
       "      <td>0.978565</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.978248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.994155</td>\n",
       "      <td>0.994117</td>\n",
       "      <td>0.994115</td>\n",
       "      <td>0.978731</td>\n",
       "      <td>0.9786</td>\n",
       "      <td>0.978608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.993648</td>\n",
       "      <td>0.993617</td>\n",
       "      <td>0.993619</td>\n",
       "      <td>0.978779</td>\n",
       "      <td>0.9786</td>\n",
       "      <td>0.978610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993384</td>\n",
       "      <td>0.993350</td>\n",
       "      <td>0.993341</td>\n",
       "      <td>0.978838</td>\n",
       "      <td>0.9787</td>\n",
       "      <td>0.978678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.995404</td>\n",
       "      <td>0.995383</td>\n",
       "      <td>0.995383</td>\n",
       "      <td>0.978861</td>\n",
       "      <td>0.9787</td>\n",
       "      <td>0.978711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.995683</td>\n",
       "      <td>0.995667</td>\n",
       "      <td>0.995668</td>\n",
       "      <td>0.978988</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.978894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.994639</td>\n",
       "      <td>0.994617</td>\n",
       "      <td>0.994616</td>\n",
       "      <td>0.979127</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>0.978996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997207</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.979276</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.979202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.991455</td>\n",
       "      <td>0.991417</td>\n",
       "      <td>0.991409</td>\n",
       "      <td>0.979475</td>\n",
       "      <td>0.9793</td>\n",
       "      <td>0.979286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.992442</td>\n",
       "      <td>0.992400</td>\n",
       "      <td>0.992396</td>\n",
       "      <td>0.979655</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.979577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.992775</td>\n",
       "      <td>0.992700</td>\n",
       "      <td>0.992699</td>\n",
       "      <td>0.979689</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.979584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.994613</td>\n",
       "      <td>0.994533</td>\n",
       "      <td>0.994543</td>\n",
       "      <td>0.979724</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>0.979510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.996776</td>\n",
       "      <td>0.996767</td>\n",
       "      <td>0.996767</td>\n",
       "      <td>0.979778</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.979717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.995133</td>\n",
       "      <td>0.995117</td>\n",
       "      <td>0.995117</td>\n",
       "      <td>0.979811</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>0.979694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.992088</td>\n",
       "      <td>0.992050</td>\n",
       "      <td>0.992053</td>\n",
       "      <td>0.980014</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.979916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993634</td>\n",
       "      <td>0.993583</td>\n",
       "      <td>0.993578</td>\n",
       "      <td>0.980205</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.980004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993806</td>\n",
       "      <td>0.993767</td>\n",
       "      <td>0.993764</td>\n",
       "      <td>0.980957</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997108</td>\n",
       "      <td>0.997083</td>\n",
       "      <td>0.997085</td>\n",
       "      <td>0.981044</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>0.980915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.997002</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.996999</td>\n",
       "      <td>0.981215</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.981199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.995102</td>\n",
       "      <td>0.995067</td>\n",
       "      <td>0.995062</td>\n",
       "      <td>0.981324</td>\n",
       "      <td>0.9812</td>\n",
       "      <td>0.981188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998503</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.981363</td>\n",
       "      <td>0.9813</td>\n",
       "      <td>0.981309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997295</td>\n",
       "      <td>0.997283</td>\n",
       "      <td>0.997284</td>\n",
       "      <td>0.981767</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.981708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998117</td>\n",
       "      <td>0.998117</td>\n",
       "      <td>0.998117</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.981790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.998069</td>\n",
       "      <td>0.998067</td>\n",
       "      <td>0.998067</td>\n",
       "      <td>0.981824</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.981798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.996946</td>\n",
       "      <td>0.996933</td>\n",
       "      <td>0.996934</td>\n",
       "      <td>0.981967</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.981804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.998369</td>\n",
       "      <td>0.998367</td>\n",
       "      <td>0.998366</td>\n",
       "      <td>0.982069</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.982002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.997972</td>\n",
       "      <td>0.997967</td>\n",
       "      <td>0.997967</td>\n",
       "      <td>0.982093</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.981999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.997723</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.982128</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.982011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.993809</td>\n",
       "      <td>0.993783</td>\n",
       "      <td>0.993786</td>\n",
       "      <td>0.982156</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>0.982101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>3</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.997462</td>\n",
       "      <td>0.997450</td>\n",
       "      <td>0.997451</td>\n",
       "      <td>0.982624</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.982508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>0.998983</td>\n",
       "      <td>0.998983</td>\n",
       "      <td>0.982703</td>\n",
       "      <td>0.9827</td>\n",
       "      <td>0.982694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer  units  epochs  batch_size  train_precision  train_recall   \n",
       "246      3     64       5         256         0.975845      0.975733  \\\n",
       "244      3     64       5         128         0.982197      0.982083   \n",
       "342      3   1024       5         256         0.984460      0.984033   \n",
       "240      3     64       5          32         0.984612      0.984533   \n",
       "270      3    128       5         256         0.983383      0.983017   \n",
       "260      3     64      20         128         0.991874      0.991800   \n",
       "248      3     64      10          32         0.989634      0.989517   \n",
       "242      3     64       5          64         0.986105      0.986067   \n",
       "254      3     64      10         256         0.986932      0.986867   \n",
       "252      3     64      10         128         0.992082      0.992067   \n",
       "262      3     64      20         256         0.996805      0.996800   \n",
       "258      3     64      20          64         0.994906      0.994883   \n",
       "316      3    512       5         128         0.990186      0.990050   \n",
       "250      3     64      10          64         0.992471      0.992417   \n",
       "336      3   1024       5          32         0.990137      0.990083   \n",
       "264      3    128       5          32         0.990141      0.990100   \n",
       "296      3    256      10          32         0.993582      0.993550   \n",
       "318      3    512       5         256         0.991527      0.991483   \n",
       "286      3    128      20         256         0.996313      0.996300   \n",
       "338      3   1024       5          64         0.990638      0.990467   \n",
       "266      3    128       5          64         0.989628      0.989550   \n",
       "274      3    128      10          64         0.993881      0.993867   \n",
       "340      3   1024       5         128         0.990148      0.989983   \n",
       "292      3    256       5         128         0.991868      0.991800   \n",
       "290      3    256       5          64         0.990005      0.989767   \n",
       "288      3    256       5          32         0.989657      0.989633   \n",
       "268      3    128       5         128         0.990304      0.990267   \n",
       "280      3    128      20          32         0.996648      0.996617   \n",
       "310      3    256      20         256         0.997012      0.997000   \n",
       "302      3    256      10         256         0.993940      0.993817   \n",
       "276      3    128      10         128         0.994155      0.994117   \n",
       "314      3    512       5          64         0.993648      0.993617   \n",
       "320      3    512      10          32         0.993384      0.993350   \n",
       "324      3    512      10         128         0.995404      0.995383   \n",
       "300      3    256      10         128         0.995683      0.995667   \n",
       "278      3    128      10         256         0.994639      0.994617   \n",
       "282      3    128      20          64         0.997207      0.997200   \n",
       "312      3    512       5          32         0.991455      0.991417   \n",
       "348      3   1024      10         128         0.992442      0.992400   \n",
       "298      3    256      10          64         0.992775      0.992700   \n",
       "350      3   1024      10         256         0.994613      0.994533   \n",
       "256      3     64      20          32         0.996776      0.996767   \n",
       "322      3    512      10          64         0.995133      0.995117   \n",
       "294      3    256       5         256         0.992088      0.992050   \n",
       "304      3    256      20          32         0.993634      0.993583   \n",
       "272      3    128      10          32         0.993806      0.993767   \n",
       "306      3    256      20          64         0.997108      0.997083   \n",
       "328      3    512      20          32         0.997002      0.997000   \n",
       "346      3   1024      10          64         0.995102      0.995067   \n",
       "284      3    128      20         128         0.998503      0.998500   \n",
       "330      3    512      20          64         0.997295      0.997283   \n",
       "352      3   1024      20          32         0.998117      0.998117   \n",
       "326      3    512      10         256         0.998069      0.998067   \n",
       "332      3    512      20         128         0.996946      0.996933   \n",
       "334      3    512      20         256         0.998369      0.998367   \n",
       "354      3   1024      20          64         0.997972      0.997967   \n",
       "358      3   1024      20         256         0.997723      0.997717   \n",
       "344      3   1024      10          32         0.993809      0.993783   \n",
       "356      3   1024      20         128         0.997462      0.997450   \n",
       "308      3    256      20         128         0.998984      0.998983   \n",
       "\n",
       "     train_f1  test_precision  test_recall   test_f1  \n",
       "246  0.975736        0.968857       0.9688  0.968795  \n",
       "244  0.982062        0.969649       0.9696  0.969559  \n",
       "342  0.983967        0.970105       0.9693  0.969175  \n",
       "240  0.984531        0.970173       0.9700  0.969995  \n",
       "270  0.983033        0.970756       0.9701  0.970133  \n",
       "260  0.991786        0.971151       0.9708  0.970778  \n",
       "248  0.989513        0.972017       0.9718  0.971755  \n",
       "242  0.986056        0.972700       0.9725  0.972479  \n",
       "254  0.986854        0.972807       0.9727  0.972690  \n",
       "252  0.992065        0.973765       0.9737  0.973687  \n",
       "262  0.996799        0.973872       0.9738  0.973797  \n",
       "258  0.994881        0.973937       0.9738  0.973800  \n",
       "316  0.990067        0.974188       0.9738  0.973850  \n",
       "250  0.992412        0.974614       0.9745  0.974472  \n",
       "336  0.990091        0.975351       0.9752  0.975204  \n",
       "264  0.990094        0.975473       0.9754  0.975390  \n",
       "296  0.993540        0.975758       0.9756  0.975599  \n",
       "318  0.991475        0.975838       0.9757  0.975688  \n",
       "286  0.996299        0.976251       0.9760  0.975999  \n",
       "338  0.990488        0.976606       0.9762  0.976229  \n",
       "266  0.989548        0.976815       0.9766  0.976602  \n",
       "274  0.993863        0.976872       0.9768  0.976786  \n",
       "340  0.990002        0.976978       0.9767  0.976718  \n",
       "292  0.991805        0.976987       0.9766  0.976644  \n",
       "290  0.989782        0.977452       0.9771  0.977118  \n",
       "288  0.989627        0.977549       0.9774  0.977400  \n",
       "268  0.990266        0.977669       0.9776  0.977597  \n",
       "280  0.996619        0.978499       0.9783  0.978302  \n",
       "310  0.997001        0.978558       0.9784  0.978406  \n",
       "302  0.993829        0.978565       0.9782  0.978248  \n",
       "276  0.994115        0.978731       0.9786  0.978608  \n",
       "314  0.993619        0.978779       0.9786  0.978610  \n",
       "320  0.993341        0.978838       0.9787  0.978678  \n",
       "324  0.995383        0.978861       0.9787  0.978711  \n",
       "300  0.995668        0.978988       0.9789  0.978894  \n",
       "278  0.994616        0.979127       0.9790  0.978996  \n",
       "282  0.997200        0.979276       0.9792  0.979202  \n",
       "312  0.991409        0.979475       0.9793  0.979286  \n",
       "348  0.992396        0.979655       0.9796  0.979577  \n",
       "298  0.992699        0.979689       0.9796  0.979584  \n",
       "350  0.994543        0.979724       0.9795  0.979510  \n",
       "256  0.996767        0.979778       0.9797  0.979717  \n",
       "322  0.995117        0.979811       0.9797  0.979694  \n",
       "294  0.992053        0.980014       0.9799  0.979916  \n",
       "304  0.993578        0.980205       0.9800  0.980004  \n",
       "272  0.993764        0.980957       0.9809  0.980900  \n",
       "306  0.997085        0.981044       0.9809  0.980915  \n",
       "328  0.996999        0.981215       0.9812  0.981199  \n",
       "346  0.995062        0.981324       0.9812  0.981188  \n",
       "284  0.998500        0.981363       0.9813  0.981309  \n",
       "330  0.997284        0.981767       0.9817  0.981708  \n",
       "352  0.998117        0.981818       0.9818  0.981790  \n",
       "326  0.998067        0.981824       0.9818  0.981798  \n",
       "332  0.996934        0.981967       0.9818  0.981804  \n",
       "334  0.998366        0.982069       0.9820  0.982002  \n",
       "354  0.997967        0.982093       0.9820  0.981999  \n",
       "358  0.997717        0.982128       0.9820  0.982011  \n",
       "344  0.993786        0.982156       0.9821  0.982101  \n",
       "356  0.997451        0.982624       0.9825  0.982508  \n",
       "308  0.998983        0.982703       0.9827  0.982694  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_name = 'group_layer_3'\n",
    "group_df = next(df for name, df in grouped_dfs if name == group_name)\n",
    "# print(group_df)\n",
    "\n",
    "sorted_group_df = group_df[['layer', 'units', 'epochs', 'batch_size', 'train_precision', 'train_recall', 'train_f1', 'test_precision', 'test_recall', 'test_f1']]\n",
    "sorted_group_df = sorted_group_df.sort_values(by=['test_precision', 'test_recall', 'test_f1'], ascending=[True, True, True])\n",
    "sorted_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0f958339-6e4a-413b-bd6e-57b2f69afb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'layer': 2, 'units': 64, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9981542052497664, 'train_recall': 0.99815, 'train_f1': 0.9981504539078294, 'test_precision': 0.9740360699099502, 'test_recall': 0.974, 'test_f1': 0.9739972118472795}, {'layer': 2, 'units': 64, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9972407730101432, 'train_recall': 0.9972333333333333, 'train_f1': 0.9972340568406748, 'test_precision': 0.9775476152584839, 'test_recall': 0.9775, 'test_f1': 0.9775027867742472}]\n",
      "[{'layer': 2, 'units': 128, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9963484817370523, 'train_recall': 0.9963333333333333, 'train_f1': 0.9963334839638083, 'test_precision': 0.9805417362953536, 'test_recall': 0.9804, 'test_f1': 0.980401448484472}, {'layer': 2, 'units': 128, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9988185309508619, 'train_recall': 0.9988166666666667, 'train_f1': 0.9988165943632138, 'test_precision': 0.9804715426505787, 'test_recall': 0.9804, 'test_f1': 0.9804050953715412}]\n",
      "[{'layer': 2, 'units': 256, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9974430553453125, 'train_recall': 0.9974333333333333, 'train_f1': 0.9974338193991888, 'test_precision': 0.9807381233076521, 'test_recall': 0.9806, 'test_f1': 0.9806169872268612}, {'layer': 2, 'units': 256, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9993671814321037, 'train_recall': 0.9993666666666666, 'train_f1': 0.9993665724666987, 'test_precision': 0.9814687230786039, 'test_recall': 0.9814, 'test_f1': 0.981397346798378}]\n",
      "[{'layer': 2, 'units': 512, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9977196761370051, 'train_recall': 0.9977166666666667, 'train_f1': 0.997716767169853, 'test_precision': 0.9810835087224854, 'test_recall': 0.981, 'test_f1': 0.9810064639098993}, {'layer': 2, 'units': 512, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 0.9997335302690555, 'train_recall': 0.9997333333333334, 'train_f1': 0.9997333287211976, 'test_precision': 0.9859057354041056, 'test_recall': 0.9859, 'test_f1': 0.9858952997825092}]\n",
      "[{'layer': 2, 'units': 1024, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'adam', 'train_precision': 0.9983847595941063, 'train_recall': 0.9983833333333333, 'train_f1': 0.9983833072626787, 'test_precision': 0.9834363984412731, 'test_recall': 0.9834, 'test_f1': 0.98339855268655}, {'layer': 2, 'units': 1024, 'epochs': 20, 'batch_size': 32, 'activation': 'relu', 'optimizer': 'rmsprop', 'train_precision': 1.0, 'train_recall': 1.0, 'train_f1': 1.0, 'test_precision': 0.9861019445649656, 'test_recall': 0.9861, 'test_f1': 0.9860944082624646}]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layer_to_test = 2\n",
    "epoch_to_test = 20\n",
    "batch_size_to_test = 32\n",
    "\n",
    "# 固定的層數、epoch、batch_size\n",
    "unit_numbers = [64, 128, 256, 512, 1024]  # 不同的 units\n",
    "precision_scores = []\n",
    "\n",
    "for unit_num in unit_numbers:\n",
    "    # 過濾 找出藥用的\n",
    "    filtered_records = [record for record in all_record if \n",
    "                        record[\"batch_size\"] == batch_size_to_test and\n",
    "                        record[\"epochs\"] == epoch_to_test and\n",
    "                        record[\"units\"] == unit_num and\n",
    "                        record[\"layer\"] == layer_to_test]\n",
    "    print(filtered_records)\n",
    "    precision_scores.append([record[\"test_precision\"] for record in filtered_records][0])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(unit_numbers, precision_scores, marker='o')\n",
    "\n",
    "plt.xlabel('Units')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('The change of units influence units.')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
